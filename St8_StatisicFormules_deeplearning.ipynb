{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pylab import plot,show,hist\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "from scipy.stats import norm, chi2_contingency\n",
    "import statsmodels.api as sm\n",
    "from numpy import linspace,hstack\n",
    "from pylab import plot,show,hist\n",
    "import pydot\n",
    "#%config InlineBackend.figure_format = 'svg' –¥–ª—è –±–æ–ª—å—à–µ–π —á–µ—Ç–∫–æ—Å—Ç–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "#–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#–î–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º —Ä–∞—Å—Å–µ–∏–≤–∞–Ω–∏—è\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#–ì—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "\n",
    "#–†–∞—Å—â–µ–ø–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#XGBoost\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "#–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏\n",
    "from keras.models import Sequential #—Ç–∏–ø —Å–µ—Ç–∏\n",
    "from keras.layers import Dense #–º–µ—Ç–æ–¥ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å–ª–æ–µ–≤\n",
    "from keras.utils import np_utils #–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –ö–µ—Ä–∞—Å\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "#Deep Learning\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "os.chdir(r'C:\\Users\\Mr Alex\\Documents\\GitHub\\FlightPreparence')\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "           'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "#df = pd.read_csv('AmesHousing.txt', sep=\"\\t\", header = 0, index_col=False)\n",
    "#df = pd.read_csv('town_1959_2.csv', header = 0,)\n",
    "#df = pd.read_csv('swiss_bank_notes.csv', index_col=0)\n",
    "#df = pd.read_csv('beverage_r.csv', sep=\";\", index_col='numb.obs')\n",
    "#df = pd.read_csv('Protein Consumption in Europe.csv', sep=';', decimal=',', index_col='Country')\n",
    "#df = pd.read_csv('assess.dat', sep='\\t', index_col='NAME')\n",
    "#df = pd.read_csv('Albuquerque Home Prices_data.txt', sep='\\t')\n",
    "#df = pd.read_csv('agedeath.dat.txt', sep='\\s+', header=None, names=['group', 'age', 'index'])\n",
    "#df = pd.read_csv('interference.csv')\n",
    "#df = pd.read_csv('diamond.dat', header=None, sep='\\s+', names=['weight', 'price'])\n",
    "#df = pd.read_csv('Credit.csv', sep=';', encoding='cp1251')\n",
    "#df = pd.read_csv('adult.test', header=None, names=columns, na_values=' ?', skiprows=1)\n",
    "#df = pd.read_csv('Wine.txt', sep='\\t', header=0)\n",
    "#df = pd.read_csv('monthly-car-sales-in-quebec-1960.csv', sep=';', header=0, parse_dates=[0])\n",
    "#df = pd.read_csv('stickleback.csv', sep=';', decimal=',')\n",
    "df = pd.read_csv('Swiss Fertility.csv', sep=';', decimal=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df):\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ\n",
    "    \"\"\"\n",
    "    scaled = preprocessing.StandardScaler().fit_transform(df)\n",
    "    scaled = pd.DataFrame(scaled, columns=df.columns)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–†–∞—Å—â–µ–ø–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –≤—ã–±–æ—Ä–∫–∏\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
    "                                                    # –¥–æ–ª—è –æ–±—ä—ë–º–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞\n",
    "                                                    test_size=0.2)\n",
    "#–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#–°—Ç—Ä–æ–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: –¥–æ–ª—è —Å–æ–≤–ø–∞–≤—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤ y_pred –∏ y_test, –∏–ª–∏ —Å—á–∏—Ç–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç—É\n",
    "#–ï—Å–ª–∏ –¥–æ–ª—è –≤ –æ–±—É—á–∞—é—â–µ–º –≤—ã—à–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ, –æ–∑–Ω–∞—á–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ù—É–∂–Ω–æ —É–ø—Ä–æ—â–∞—Ç—å –º–æ–¥–µ–ª—å\n",
    "#–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫  ùê∂=(ùëêùëñ,ùëó) , –≥–¥–µ  ùëêùëñ,ùëó –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ ùëñ , –∫–æ—Ç–æ—Ä—ã–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–∏—Å–≤–æ–∏–ª –∫–ª–∞—Å—Å ùëó \n",
    "#–¢–æ—á–Ω–æ—Å—Ç—å(precision) - –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º. \n",
    "#–ü–æ–ª–Ω–æ—Ç–∞(recall) - –¥–æ–ª—è —ç—Ç–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ù–ê –°–ê–ú–û–ú –î–ï–õ–ï\n",
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "#–ì–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ F1 = 2*—Ç–æ—á–Ω–æ—Å—Ç—å*–ø–æ–ª–Ω–æ—Ç–∞/(—Ç–æ—á–Ω–æ—Å—Ç—å+–ø–æ–ª–Ω–æ—Ç–∞). –°—á–∏—Ç–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é classification_report\n",
    "print(metrics.classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ú–µ—Ç–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞\n",
    "#–ú–µ—Ç–æ–¥ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è E = —Å—É–º–º–∞(Yi-Vi)**2 –ø–æ–∑–≤–æ–ª—è–µ—Ç —á–µ—Ä–µ–∑ MSE –Ω–∞—Ö–æ–¥–∏—Ç—å –æ—à–∏–±–∫—É –∏ –Ω–∞ –µ–µ –æ—Å–Ω–æ–≤–µ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –≤–µ—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏. Deep Learning\n",
    "#–ê–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è = —Å—É–º–º–∞(Wi*Xi) –æ—Ç —á–∏—Å–ª–∞ –≤—Ö–æ–¥–æ–≤ –Ω–µ–π—Ä–æ–Ω–∞. \n",
    "#–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è f(x)=e**x/(1+e**x) –∏–ª–∏ –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏–π —Ç–∞–Ω–≥–µ–Ω—Å\n",
    "#ReLU —Ñ—É–Ω–∫—Ü–∏—è f(x) = max(0, X) –ø—Ä–æ—â–µ, –Ω–æ —á—É—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω–∞—è –∏ —Å–ª–æ–∂–Ω–µ–µ –≤ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "#–ü–æ–¥–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ù–° –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —á–∏—Å–ª–æ –Ω–µ–π—Ä–æ–Ω–æ–≤, –Ω–∞—Å—Ç—Ä–æ–∏–≤ –≤—Ö–æ–¥–Ω–æ–π, —Å–∫—Ä—ã—Ç—ã–µ, –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–∏. \n",
    "#–°–µ—Ç–∏ –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è: –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Å–ª–æ—è –Ω–µ–π—Ä–æ–Ω—ã –Ω–µ —Å–≤—è–∑–∞–Ω—ã, –ø–µ—Ä–µ–¥–∞—é—Ç —Ç–æ–ª—å–∫–æ –≤ —Å–ª–µ–¥. —Å–ª–æ–π, –ø–µ—Ä–µ–ø—Ä—ã–≥–∏–≤–∞—Ç—å –Ω–µ–ª—å–∑—è\n",
    "#–û–±—É—á–µ–Ω–∏–µ –ù–° = –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –í–ï–°–û–í (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)  –∫–∞–∂–¥–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è. –û—Å—Ç–∞–ª—å–Ω–æ–µ –∑–∞–¥–∞–µ—Ç—Å—è –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–º –∑–∞—Ä–∞–Ω–µ–µ\n",
    "#Keras –º–æ–¥—É–ª–∏: –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –≤—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —É—Å–ª–æ–≤–∏—è –æ–±—É—á–µ–Ω–∏—è, –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "\n",
    "wine['Desired1(3)'].value_counts(normalize=True)\n",
    "\n",
    "#–ò–º–µ–Ω—É–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã –∏ –æ—Ç–∫–ª–∏–∫\n",
    "y = wine['Desired1(3)']\n",
    "X = wine.drop('Desired1(3)', axis=1)\n",
    "\n",
    "#—Ä–∞—Å—â–µ–ø–ª–µ–Ω–∏–µ –Ω–∞ –≤—ã–±–æ—Ä–∫–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –æ–±—ä—ë–º–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12345, test_size=0.33)\n",
    "\n",
    "#–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ np.array –¥–ª—è Keras\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "#–ü–æ—Å–∫–æ–ª—å–∫—É –±–æ–ª—å—à–µ –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤ –∏ –æ–Ω–∏ –Ω–µ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω—ã, —Ç–æ —Ä–∞–∑–±–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É \"y\" –Ω–∞ —Ç—Ä–∏, —Å –±–∏–Ω–∞—Ä–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "y_train_bin = np_utils.to_categorical(y_train)\n",
    "y_test_bin = np_utils.to_categorical(y_test)\n",
    "y_train_bin[0:5]\n",
    "\n",
    "#–ú–µ—Ç–æ–¥ —Å–∫–æ—Ä–µ–π—à–µ–≥–æ(–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ) —Å–ø—É—Å–∫–∞ SGD. –£–ª—É—á—à–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞: Momentum, Nesterov momentum, Adam –∏ –¥—Ä.\n",
    "#Argmin, –ø—Ä–∞–≤–∏–ª–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: —á–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –∏–ª–∏ –º–∞–ª–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "#–ù–∞—á–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞(–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è). –ù–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥.–±. –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º, –±–ª–∏–∂–µ –∫ –Ω—É–ª—é (–∫—Ä–æ–º–µ —Å–≤–æ–±–æ–¥–Ω—ã—Ö —Å–ª–∞–≥–∞–µ–º—ã—Ö)\n",
    "#–ì—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è –∫–∞–∞—á-–≤–∞ Q –æ—Ç –Ω–æ–º–µ—Ä–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏. –ú–∞–ª–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è(0.001 –∏ —Ç.–¥.) –¥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –∏–¥–µ—Ç –¥–æ–ª—å—à–µ\n",
    "#–í—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞—Ç—å(—Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫ –Ω–∞—Å—ã—â–µ–Ω–∏—è) (Xi-Xmin)/(Xmax-Xmin)\n",
    "#Batch - –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –≤–µ—Å–æ–≤ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏. –ù–∞—Å—ã—â–µ–Ω–∏–µ - –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–π. Gradient clipping - –±–ª–æ–∫ –æ—Ç –±–æ–ª—å—à–∏—Ö –ø–æ–ø—Ä–∞–≤–æ–∫\n",
    "\n",
    "#–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏. –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–Ω–æ –≤—Å–µ–≥–æ –ø–∞—Ä—É —Å–ª–æ–µ–≤ –≤ 5-7 –Ω–µ–π—Ä–æ–Ω–æ–≤\n",
    "#–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è. –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö –≤–µ—Å–æ–≤\n",
    "init = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None) #–ó–µ—Ä–Ω–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –æ–±—É—á–µ–Ω–∏–µ–º –º–µ–Ω—å—à–µ\n",
    "init_2 = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=12345) #–£—Å–µ—á–µ–Ω–Ω–æ–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ. –ò–Ω–∏—Ü–∏–∞—Ü–∏—è –≤–µ—Å–æ–≤\n",
    "init_3 = initializers.Constant(value = 1e-3) #–ò–Ω–∏—Ü–∏–∞—Ü–∏—è —Å–≤–æ–±–æ–¥–Ω—ã—Ö —á–ª–µ–Ω–æ–≤\n",
    "\n",
    "model = Sequential() #–£–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ —Ç–∏–ø –º–æ–¥–µ–ª–∏ (—Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è)\n",
    "model.add(Dense(9, input_dim=13, activation='relu')) #–ü–µ—Ä–≤—ã–π —Å–ª–æ–π, 9 –Ω–µ–π—Ä–æ–Ω–æ–≤, –≤—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (13 –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤)\n",
    "model.add(Dense(10, activation='relu', )) #–í—Ç–æ–æ–π —Å–ª–æ–π, 10 –Ω–µ–π—Ä–æ–Ω–æ–≤\n",
    "model.add(Dense(3, activation='softmax')) #–¢—Ä–µ—Ç–∏–π —Å–ª–æ–π, —Ä–∞–Ω–∂–∏—Ä–æ–≤–∫–∞ —Å–æ—Ñ—Ç–º–∞–∫—Å–æ–º –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –¢—Ä–∏ –≤—ã—Ö–æ–¥–∞\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(9, input_dim=13, activation='relu'))\n",
    "model2.add(Dense(10, activation='relu' ))\n",
    "model2.add(Dense(3, activation='softmax')) #–°–æ—Ñ—Ç–º–∞–∫—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è n-–∫–ª–∞—Å—Å–æ–≤\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(9, input_dim=13, activation='relu', kernel_initializer=init_2, bias_initializer=init_3))\n",
    "model3.add(Dense(10, activation='relu', kernel_initializer=init_2, bias_initializer=init_3 ))\n",
    "model3.add(Dense(3, activation='softmax', kernel_initializer=init_2, bias_initializer=init_3))\n",
    "\n",
    "#Categorical crossentropy (CC) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–∞ –∫ –∫–ª–∞—Å—Å—É (—É–ø–æ—Ä—è–¥–æ—á–Ω–æ–º—É)\n",
    "\n",
    "#–ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º: optimizer(rmsprop –∏–ª–∏ adam), loss function(categorical_crossentropy(–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è) –∏–ª–∏ mse(—Ä–µ–≥—Ä–µ—Å—Å–∏—è)). –¢–æ—á–Ω–æ—Å—Ç—å\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "sgd2 = optimizers.SGD(lr=0.02, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=sgd2, metrics=['accuracy'])\n",
    "\n",
    "sgd3 = optimizers.SGD(lr=0.02, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=sgd3, metrics=['accuracy'])\n",
    "\n",
    "#–û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å: 300 —ç–ø–æ—Ö, –ø—Ä–æ–ø—É—Å–∫ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ —Å–º–µ–Ω—ã –≤–µ—Å–æ–≤\n",
    "model.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "model2.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "model3.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "\n",
    "#–ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ\n",
    "scores = model.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "scores2 = model2.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy2: %.2f%%\" % (scores2[1]*100))\n",
    "\n",
    "scores3 = model3.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy3: %.2f%%\" % (scores3[1]*100))\n",
    "\n",
    "\n",
    "#–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã\n",
    "predictions = model.predict(X_test)\n",
    "predictions2 = model2.predict(X_test)\n",
    "predictions3 = model3.predict(X_test)\n",
    "#round predictions\n",
    "#rounded = [round(x[0]) for x in predictions]\n",
    "#print(rounded)\n",
    "print(predictions[0:5])\n",
    "print(predictions2[0:5])\n",
    "print(predictions3[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (–≤ –Ω–∞–¥–µ–∂–¥–µ, —á—Ç–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Ä—è–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è). –ê–¥–¥–∏—Ç–∏–≤–Ω–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å, –ª–æ–≥–∞—Ä–∏—Ñ–º –Ω–µ –Ω—É–∂–µ–Ω\n",
    "#–£–º–µ–Ω—å—à–µ–Ω–∏–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏ –º.–±. —Ç–æ–ª—å–∫–æ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–µ—Ç–µ–π (–∏ —Å–æ —Å–Ω–∏–∂–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –≤ —Ü–µ–ª–æ–º)\n",
    "#–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã —Å–∞–º—ã–µ –Ω–µ–¥–∞–≤–Ω–∏–µ (–∏–ª–∏ —Å–∞–º—ã–µ –≤—ã—Å–æ–∫–æ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ) –Ω–∞–±–ª—é–¥–µ–Ω–∏—è. –ò—Ö –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ —Å—Ç–∞–≤–∏—Ç—å –≤ —Ç–µ—Å—Ç–æ–≤—É—é\n",
    "#MSE, MAE –∏–ª–∏ Mean absolute percentage error (MAPE) - –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –æ—à–∏–±–∫—É –≤ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "#MAPE = 1/n *—Å—É–º–º–∞(Yi-Y–ø—Ä–æ–≥–Ω–æ–∑)/Yi*100%\n",
    "\n",
    "sales = pd.read_csv('monthly-car-sales-in-quebec-1960.csv', sep=';', header=0, parse_dates=[0])\n",
    "\n",
    "#–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "sales_2 = pd.DataFrame()\n",
    "\n",
    "for i in range(12,0,-1): #–£–±—Ä–∞–Ω–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫, –ø–æ—Å–∫–æ–ª—å–∫—É –Ω–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    sales_2['t-'+str(i)] = sales.iloc[:,1].shift(i) #–ù–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –≥–¥–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–¥–≤–∏–Ω—É—Ç—ã —Å –æ–±—Ä–∞—Ç–Ω—ã–º –≤—Ä–µ–º–µ–Ω–Ω—ã–º —à–∞–≥–æ–º(–ø–æ–º–µ—Å—è—á–Ω–æ)\n",
    "\n",
    "sales_2['t'] = sales.iloc[:,1].values #–î—É–±–ª–∏—Ä—É–µ–º –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—É—é –∫–æ–ª–æ–Ω–∫—É\n",
    "sales_4 = sales_2[12:] #–û—Ç—Ä–µ–∑–∞–µ–º –ø–µ—Ä–≤—ã–µ 12 —Å—Ç—Ä–æ–∫\n",
    "\n",
    "#–ó–∞–¥–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã –∏ –æ—Ç–∫–ª–∏–∫\n",
    "y = sales_4['t']\n",
    "X = sales_4.drop('t', axis=1)\n",
    "\n",
    "#–†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é. –¢–µ—Å—Ç–æ–≤–∞—è - –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è\n",
    "X_train = X[:91]\n",
    "y_train = y[:91]\n",
    "X_test  = X[91:]\n",
    "y_test  = y[91:]\n",
    "\n",
    "#–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ np.array\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "#–°–æ–∑–¥–∞–µ–º, –∫–æ–º–ø–∏–ª–∏—Ä—É–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(Dense(1, activation='linear')) #–õ–∏–Ω–µ–π–Ω–∞—è –≤—ã—Ö–æ–¥–Ω–∞—è —Ñ-—Ü–∏—è, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–∏–Ω–µ–π–Ω—É—é –∫–æ–º–±–∏–Ω–∞—Ü–∏—é  \n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=None)\n",
    "\n",
    "#–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nMAPE: %.2f%%\" % (scores[1]))\n",
    "\n",
    "#–í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "#–í—ã—á–∏—Å–ª—è–µ–º –ø–æ–¥–≥–æ–Ω–∫—É\n",
    "predictions_train = model.predict(X_train)\n",
    "\n",
    "#–ì—Ä–∞—Ñ–∏–∫ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ numpy.arange(start, stop, step, dtype=None)\n",
    "x2 = np.arange(0, 91, 1)\n",
    "x3 = np.arange(91, 96, 1)\n",
    "\n",
    "plt.plot(x2, y_train, color='blue')\n",
    "plt.plot(x2, predictions_train, color='green')\n",
    "plt.plot(x3, y_test, color='blue')\n",
    "plt.plot(x3, predictions, color='red') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–æ–≤\n",
    "ser_g = pd.read_csv('series_g.csv', sep=';', header=0)\n",
    "\n",
    "#–ú—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å. –ü–æ—Ç–æ–º—É –¥–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥–∞—Ä–∏—Ñ–º, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤ —Å—É–º–º—É\n",
    "ser_g['log_y'] = np.log10(ser_g['series_g'])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ser_g['series_g'].plot(ax=ax1)\n",
    "ax1.set_title(u'–û–±—ä—ë–º –ø–∞—Å—Å–∞–∂–∏—Ä–æ–ø–µ—Ä–µ–≤–æ–∑–æ–∫')\n",
    "ax1.set_ylabel(u'–¢—ã—Å—è—á —á–µ–ª–æ–≤–µ–∫')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "pd.Series(ser_g['log_y']).plot(ax=ax2)\n",
    "ax2.set_title(u'log10 –æ—Ç –æ–±—ä—ë–º–∞ –ø–∞—Å—Å–∞–∂–∏—Ä–æ–ø–µ—Ä–µ–≤–æ–∑–æ–∫')\n",
    "ax2.set_ylabel(u'log10 –æ—Ç —Ç—ã—Å—è—á —á–µ–ª–æ–≤–µ–∫')\n",
    "\n",
    "#–î–∞–Ω–Ω—ã–µ –ª—É—á—à–µ —Ä–∞–∑–±–∏–≤–∞—Ç—å –Ω–∞ –¥–≤–∞ —Ä—è–¥–∞: —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –∏ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ (—Å–≥–ª–∞–∂–µ–Ω–Ω—ã–π —Ä—è–¥ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ)\n",
    "#–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ: 12 –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤(–ø–æ–º–µ—Å—è—Ü–∞–º), 1 –æ—Ç–∫–ª–∏–∫\n",
    "ser_g_2 = pd.DataFrame()\n",
    "for i in range(12,0,-1):\n",
    "    ser_g_2['t-'+str(i)] = ser_g.iloc[:,2].shift(i)\n",
    "ser_g_2['t'] = ser_g.iloc[:,2].values\n",
    "\n",
    "#–û—Ç—Ä–µ–∑–∞–µ–º –ø–µ—Ä–≤—ã–µ 12 —Å—Ç—Ä–æ–∫\n",
    "ser_g_4 = ser_g_2[12:]\n",
    "\n",
    "#–ó–∞–¥–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã –∏ –æ—Ç–∫–ª–∏–∫ \n",
    "y = ser_g_4['t']\n",
    "X = ser_g_4.drop('t', axis=1)\n",
    "\n",
    "X_train = X[:120]\n",
    "y_train = y[:120]\n",
    "X_test  = X[120:]\n",
    "y_test  = y[120:]\n",
    "\n",
    "#numpy array\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "#–û–±—É—á–∞–µ–º\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=None)\n",
    "\n",
    "#–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nMAPE: %.2f%%\" % (scores[1]))\n",
    "\n",
    "#–ø—Ä–æ–≥–Ω–æ–∑\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "#–ø–æ–¥–≥–æ–Ω–∫–∞\n",
    "predictions_train = model.predict(X_train)\n",
    "\n",
    "#–†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "x2 = np.arange(0, 120, 1)\n",
    "x3 = np.arange(120, 132, 1)\n",
    "\n",
    "#–ì—Ä–∞—Ñ–∏–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–≤–∏–¥–µ—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å —É–ª–æ–≤–∏–ª–∞ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ \n",
    "plt.plot(x2, y_train, color='blue')\n",
    "plt.plot(x2, predictions_train, color='green')\n",
    "plt.plot(x3, y_test, color='blue')\n",
    "plt.plot(x3, predictions, color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning. –ù–∞–±–æ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è MNIST(28x28 px, 256 grey). \n",
    "#–ö–∞—Å–∫–∞–¥ –•–∞–∞—Ä–∞(15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—â—É—Ç—Å—è –Ω–∞ —á–∞—Å—Ç—è—Ö –∫–∞—Ä—Ç–∏–Ω–∫–∏)\n",
    "#–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–µ—Ç–∏ —Å–æ–∑–¥–∞—é—Ç –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á. –§–∏—à–∏ –ø–æ—à–∞–≥–æ–≤–æ —É—Å–ª–æ–∂–Ω—è—é—Ç—Å—è\n",
    "#Stride - –ø–∞—Ä–∞–º–µ—Ç—Ä —Å–¥–≤–∏–≥–∞ –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–µ\n",
    "#Pooling - —É–º–µ–Ω—å—à–µ–Ω–∏–µ —á–∏—Å–ª–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤ –∑–∞ —Å—á–µ—Ç –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏ –∏ —Å—É–∂–µ–Ω–∏—è. –í–µ—Å–∞ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç—Å—è\n",
    "#FC - –ø—Ä–µ–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –≤ –≤–µ–∫—Ç–æ—Ä. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–∂–µ –≥–æ—Ç–æ–≤—É—é —Å–µ—Ç—å –∏ –¥–æ–æ–±—É—á–∏—Ç—å –µ–µ –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "#Dropout - –±–æ—Ä—å–±–∞ —Å –ø–µ—Ä–µ–ø–æ–¥–≥–æ–Ω–∫–æ–π –∏ –¥–µ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π\n",
    "\n",
    "#–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏\n",
    "batch_size = 128 #–û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏\n",
    "nr_classes = 10 #–ß–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤ (10 —Ü–∏—Ñ—Ä)\n",
    "nr_iterations = 20 #–ß–∏—Å–ª–æ —ç–ø–æ—Ö\n",
    "\n",
    "#–ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–∫–∞—á–∞–µ–º MNIST)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#–î–ª—è —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏ –∫–∞—Ä—Ç–∏–Ω–∫—É –≤—ã—Ç—è–≥–∏–≤–∞–µ–º –≤ —Å—Ç–æ–ª–±–µ—Ü\n",
    "X_train = X_train.reshape(60000, 784) #784 - —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è –≤ –∫–∞—Ä—Ç–∏–Ω–∫–µ 28—Ö28\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "#–£—Ç–æ—á–Ω—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#–ù–æ—Ä–º–∏—Ä—É–µ–º (—Å—Ç–∞–Ω–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º) –≤—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (256 –æ—Ç—Ç–µ–Ω–∫–æ–≤)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "#–î–µ–ª–∞–µ–º 10 –±–∏–Ω–∞—Ä–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ (—Ç–∞–∫ –∫–∞–∫ 10 —Ü–∏—Ñ—Ä)\n",
    "Y_train = np_utils.to_categorical(y_train, nr_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nr_classes)\n",
    "\n",
    "#  –û–ø–∏—Å—ã–≤–∞–µ–º —Å–µ—Ç—å. –û–¥–∏–Ω –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Å–ª–æ–π\n",
    "model = Sequential()\n",
    "model.add(Dense(196, input_shape=(784,))) #–ß–∏—Å–ª–æ –Ω–µ–π—Ä–æ–Ω–æ–≤, —á–∏—Å–ª–æ –≤—Ö–æ–¥–æ–≤\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5)) #–ö–∞–∫–∞—è –¥–æ–ª—è –±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è (–ø–æ–ª–æ–≤–∏–Ω–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤ –±—É–¥–µ—Ç —Å–ø–∞—Ç—å)\n",
    "model.add(Dense(10)) #–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
    "model.add(Activation('softmax')) #–°—É–º–º–∞ –≤—Å–µ—Ö —á–∏—Å–µ–ª –≤ –∏—Ç–æ–≥–µ –±—É–¥–µ—Ç —Ä–∞–≤–Ω–∞ 1\n",
    "\n",
    "#–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ–±—è\n",
    "model.summary()\n",
    "\n",
    "#–û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "np.random.seed(1337)  #–¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Å–µ—Ç–∏. –ó–µ—Ä–Ω–æ –¥–∞—Ç—á–∏–∫–∞ —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç\n",
    "\n",
    "#–û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "net_res_1 = model.fit(X_train, Y_train,\n",
    "                    batch_size = batch_size, epochs = nr_iterations,\n",
    "                    verbose = 1, validation_data = (X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose = 0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Deep Learning - 2. –ù–∞–±–æ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è MNIST(28x28 px, 256 grey)\n",
    "#–ß—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å–ª–æ–µ–≤ –∏ –Ω–µ–π—Ä–æ–Ω–æ–≤ –Ω—É–∂–Ω–æ —ç–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å\n",
    "#–¢–µ–æ—Ä–µ–º–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è - –≤—Å–µ–≥–¥–∞ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–æ–≥ —Ä–∞–±–æ—á–µ–π –ù–° —Å –æ–¥–Ω–∏–º —Å–ª–æ–µ–º (–Ω–æ –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –Ω–µ–π—Ä–æ–Ω–æ–≤)\n",
    "#Augmentation - —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ(–≤—Ä–∞—â–µ–Ω–∏–µ, –∑–µ—Ä–∫–∞–ª—å–Ω–æ—Å—Ç—å –∏ —Ç.–¥.) –∫–∞—Ä—Ç–∏–Ω–æ–∫ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ \n",
    "#–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø–æ –¢–∏—Ö–æ–Ω–æ–≤—É - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–æ–¥–≥–æ–Ω–∫—É, –Ω–∞–π—Ç–∏ –∫–æ–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å, –∑–∞–ø—Ä–µ—Ç–∏—Ç—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "\n",
    "train = pd.read_csv(r\"C:\\Users\\Mr Alex\\Files\\FlightPreparens\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\Mr Alex\\Files\\FlightPreparens\\test.csv\") \n",
    "\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª—è–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã –∏ –æ—Ç–∫–ª–∏–∫\n",
    "Y = train['label'] #–°—Ç–æ–ª–±–µ—Ü —Å –∫–æ–¥–∞–º–∏(10 —Ü–∏—Ñ—Ä)\n",
    "X = train.drop(['label'], axis=1) #–¶–≤–µ—Ç–æ–≤–∞—è –≥–∞–º–º–∞\n",
    "\n",
    "#–†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –∏ –≤—ã–±–æ—Ä–∫—É –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—á—Ç–æ–±—ã –Ω–µ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X.values, Y.values, test_size=0.10, random_state=42)\n",
    "\n",
    "#–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏, —á—Ç–æ–±—ã –∏—Ö –±—ã–ª–æ —É–¥–æ–±–Ω–æ –º–µ–Ω—è—Ç—å. Batch Normalization\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5 \n",
    "\n",
    "#—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "#–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "#–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
    "x_val = x_val.astype('float32')\n",
    "x_val /= 255\n",
    "\n",
    "#–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "Xtest = test.values\n",
    "Xtest = Xtest.reshape(Xtest.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª–∏–∫–∞ –≤ 10 –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø–µ—Ä–º–µ–Ω–Ω—ã—Ö \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "#–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (4 —Å–ª–æ—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞)\n",
    "model = Sequential()\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) #–ü–µ—Ä–≤—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π\n",
    "model.add(Conv2D(64, (3, 3), activation='relu')) #–í—Ç–æ—Ä–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #Pooling. –°–æ–∫—Ä–∞—â–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç\n",
    "model.add(Dropout(0.25)) #Dropout. –ß–µ—Ç–≤–µ—Ä—å –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–∑ –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "#–ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è\n",
    "model.add(Flatten()) #–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤ –≤–µ–∫—Ç–æ—Ä\n",
    "model.add(Dense(128, activation='relu')) #–ü–µ—Ä–≤—ã–π —Å–ª–æ–π –∞–Ω–∞–ª–∏–∑–∞ \n",
    "model.add(Dropout(0.5)) #Dropout\n",
    "model.add(Dense(num_classes, activation='softmax')) #–í—Ç–æ—Ä–æ–π —Å–ª–æ–π –∞–Ω–∞–ª–∏–∑–∞ \n",
    "\n",
    "#–û–ø—Ä–µ–¥–µ–ª—è–µ–º—Å—è —Å –æ–±—É—á–µ–Ω–∏–µ–º\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,  optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test accuracy:', accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤. –ù—É–∂–Ω–∞ –¥–ª—è —Ä–∞–±–æ—á–∏—Ö –º–æ–º–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–∞—à–∏–Ω—É \n",
    "#–£—Ç–æ—á–Ω–µ–Ω–∏–µ –≤—ã–¥–∞–Ω–Ω—ã—Ö –º–∞—à–∏–Ω–æ–π –∑–Ω–∞—á–µ–Ω–∏–π, —Å —É—á–µ—Ç–æ–º \"–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\" –ø–æ–ø–∞—Å—Ç—å –≤ –∫–ª–∞—Å—Å\n",
    "#–ú–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –ø–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ä–µ–¥–µ–ª—è—Ç –∫–ª–∞—Å—Å. –í –ø—Ä–æ–º–µ–∂—É—Ç–∫–µ –º–µ–∂–¥—É –Ω–∏–º–∏ –º–∞—à–∏–Ω–∞ —Å–∫–∞–∂–µ—Ç \"–Ω–µ –∑–Ω–∞—é\"\n",
    "#–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ - —ç—Ç–æ –ø–µ—Ä–µ—Å—á–µ—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, —á—Ç–æ–±—ã –ø—Ä–æ –Ω–∏—Ö –º.–±. —Å–∫–∞–∑–∞—Ç—å - —ç—Ç–æ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å\n",
    "#–ò–∑–æ—Ç–æ–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è - —ç—Ç–æ –ª–∏–Ω–∏—è, —É –∫–æ—Ç–æ—Ä–æ–π –≤–µ–∫—Ç–æ—Ä –Ω–µ —É–±—ã–≤–∞–µ—Ç\n",
    "#–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∫—Ä–∏–≤–∞—è, –º–µ—Ç–æ–¥ –ü–ª–∞—Ç—Ç–∞ - –Ω–µ—É–±—ã–≤–∞—é—â–∞—è –ª–∏–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–±–ª–∏–∑–∏—Ç Pi –∫ P\n",
    "\n",
    "\n",
    "# –°—Ç—Ä–æ–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç—å –∫ –∫–ª–∞—Å—Å—É\n",
    "y_pred_train2 = model.predict_proba(X_train)\n",
    "y_pred_test2 = model.predict_proba(X_test)\n",
    "\n",
    "#–ó–∞–≤–µ—Ä—à–∞–µ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–∞—à–∏–Ω—É –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π\n",
    "model_sigmoid = CalibratedClassifierCV(model, \n",
    "                                       cv=2, \n",
    "                                       method='sigmoid' #–ò–ª–∏ \"isotonic\"\n",
    "                                      )\n",
    "\n",
    "\n",
    "#–û–±—É—á–∞–µ–º –∫–∞–ª–∏–±—Ä–æ–≤–∫—É –Ω–∞ –≤—ã–±–æ—Ä–∫–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "model_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "#–°–º–æ—Ç—Ä–∏–º –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏\n",
    "model_sigmoid.predict_proba(X_test)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –¥–ª—è –Ω–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
    "new_item = [1, 1, 1, 1]\n",
    "model.predict([new_item])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
