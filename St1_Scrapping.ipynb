{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://coderlessons.com/tutorials/python-technologies/izuchite-python-web-scraping/python-web-scraping-kratkoe-rukovodstvo\n",
    "https://www.tutorialspoint.com/automata_theory/regular_expressions.htm\n",
    "https://www.tutorialspoint.com/python/python_reg_expressions.htm\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "https://lxml.de/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html \n",
    "import urllib3\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import boto3\n",
    "from urllib.request import urlopen\n",
    "import datetime\n",
    "import random\n",
    "import sqlite3\n",
    "import pymysql\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "Server\n",
      "Expires\n",
      "Cache-Control\n",
      "Pragma\n",
      "Link\n",
      "Set-Cookie\n",
      "Keep-Alive\n",
      "Connection\n",
      "Transfer-Encoding\n",
      "Content-Type\n",
      "text/html; charset=UTF-8\n",
      "Apache\n"
     ]
    }
   ],
   "source": [
    "#Объект ответа в HTTP. Получаем инфу о контенте \n",
    "url = \"https://authoraditiagarwal.com/wpcontent/uploads/2018/05/MetaSlider_ThinkBig-1080x180.jpg\"\n",
    "\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "for headers in r.headers: \n",
    "    print(headers)\n",
    "    \n",
    "#загружаем контент\n",
    "\n",
    "r = requests.get(url) \n",
    "with open(\"ThinkBig.png\",'wb') as f:\n",
    "   f.write(r.content)\n",
    "\n",
    "print (r.headers.get('content-type'))\n",
    "print (r.headers.get('Server'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#запрос для выполнения HTTP-запросов GET для URL\n",
    "r = requests.get(' https://authoraditiagarwal.com/ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем объект Soup \n",
    "soup = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#запишем захваченные данные в файл CSV с именем dataprocessing.csv в этой папке\n",
    "f = csv.writer(open(' dataprocessing.csv ','w'))\n",
    "f.writerow(['Title'])\n",
    "f.writerow([soup.title.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping Wikipedia to find out all the countries in Asia.\n",
    "\n",
    "\n",
    "website_url = requests.get('https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area').text\n",
    "soup = BeautifulSoup(website_url, 'lxml')\n",
    "My_table = soup.find('table',{'class': 'wikitable sortable'})\n",
    "links = My_table.find_all(\"a\")\n",
    "Countries = []\n",
    "for link in links:\n",
    "    Countries.append(link.get('title'))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"Country\"] = Countries\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aws.amazon.com/ru/getting-started/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#соскрести данные с URL\n",
    "data = requests.get(\"Enter the URL\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для хранения данных в корзину S3 нам нужно создать клиент S3 \n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = \"our-content\"\n",
    "#Созбдаем сегмент S3\n",
    "s3.create_bucket(Bucket = bucket_name, ACL = 'public-read')\n",
    "s3.put_object(Bucket = bucket_name, Key = '', Body = data, ACL = \"public-read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подключаемся к SQL серверу\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1',user='root', passwd = None, db = 'mysql',\n",
    "charset = 'utf8', port=None)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE scrap\")\n",
    "random.seed(datetime.datetime.now())\n",
    "def store(title, content):\n",
    "   cur.execute('INSERT INTO scrap_pages (title, content) VALUES ''(\"%s\",\"%s\")', (title, content))\n",
    "   cur.connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gолучаем данные из вики.\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "   html = urlopen('http://en.wikipedia.org'+articleUrl)\n",
    "   bs = BeautifulSoup(html, 'html.parser')\n",
    "   title = bs.find('h1').get_text()\n",
    "   content = bs.find('div', {'id':'mw-content-text'}).find('p').get_text()\n",
    "   store(title, content)\n",
    "   return bs.find('div', {'id':'bodyContent'}).findAll('a',href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "try:\n",
    "   while len(links) > 0:\n",
    "      newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "      print(newArticle)\n",
    "      links = getLinks(newArticle)\n",
    "        \n",
    "finally:\n",
    "   cur.close()\n",
    "   conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Запускаем ХромДрайвер\n",
    "driver = webdriver.Chrome()  # Optional argument, if not specified will search path.\n",
    "driver.get('http://www.google.com/');\n",
    "search_box = driver.find_element_by_name('q')\n",
    "search_box.send_keys('ChromeDriver')\n",
    "search_box.submit()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#robots.txt — это файл, используемый для идентификации частей сайта, которые сканерам разрешено просматривать \n",
    "#Sitemap - карты сайта, которые помогают сканерам находить контент без необходимости сканировать каждую страницу\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
