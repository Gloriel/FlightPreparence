{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полезные ссылки:<br>\n",
    "* [Формулы на LaTeX](http://www.machinelearning.ru/wiki/images/e/e4/latex_examples.pdf)<br>\n",
    "* [Теория и практика на Хабр](https://habr.com/ru/company/ods/blog/322534/)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T07:55:19.664337Z",
     "start_time": "2021-02-08T07:55:12.164773Z"
    },
    "code_folding": [
     91
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pylab import plot,show,hist\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "from scipy.stats import norm, chi2_contingency\n",
    "import statsmodels.api as sm\n",
    "from numpy import linspace,hstack\n",
    "import pydot\n",
    "#%config InlineBackend.figure_format = 'svg' для большей четкости графиков\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "#Стандартизация данных\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Для построения диаграмм рассеивания\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Графика для интерпретации моделей\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "\n",
    "#Иерархический кластерный анализ\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "#Кластерный анализ методом К-средних\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Линейная регрессия\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Полиномиальная регрессия\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Логистическая регрессия\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Расщепление на обучающую и тестовую выборки\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Деревья решений для задачи классификации\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Деревья решений для задач регрессии \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Калибровка деревьев решений\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "#XGBoost\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "#Нейронные сети\n",
    "from keras.models import Sequential #тип сети\n",
    "from keras.layers import Dense #метод соединения слоев\n",
    "from keras.utils import np_utils #обработка данных под Керас\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "#Deep Learning\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Факторный анализ\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#SVD - Singular Value decomposition\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "\n",
    "# os.chdir(r'C:\\Users\\Mr Alex\\Documents\\GitHub\\FlightPreparence')\n",
    "\n",
    "# columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "#            'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "#            'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "#df = pd.read_csv('AmesHousing.txt', sep=\"\\t\", header = 0, encoding='cp1251', index_col=False)\n",
    "#df = pd.read_csv('beverage_r.csv', sep=\";\", decimal=',', parse_dates=[0], index_col='numb.obs')\n",
    "#df = pd.read_csv('diamond.dat', header=None, sep='\\s+', names=['weight', 'price'])\n",
    "#df = pd.read_csv('adult.test', header=None, names=columns, na_values=' ?', skiprows=1)\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Классификация: приписание объекта к классу на основе ключевой(группирующей) переменной или совокупности его характеристик\n",
    "# Типы переменных. Количественные(непрерывные, дискретные). Номинальные (несравниваемые). Ранговые (порядковые)\n",
    "# Гистограмма частот - форма распределения количественного признака\n",
    "# Описательная статистика. Меры центральной тенденции. Меры изменчивости (Размах - Xmax-Xmin)\n",
    "# МЦТ. Мода - самый частый признак. Медиана - делит упорядоченное множество пополам. Среднее (Математическое ожидание, EX)\n",
    "# Дисперсия D - средний квадрат отклонений индивидуальных значений от средней величины. С ростом n, дисперсия сокращается\n",
    "# D = сумма(Xинд - Xсред)**2/n-1. Хсред генеральной совокупности обозначается как мю, М\n",
    "# Стандартное отклонение, \"сигма\", sd = D**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормальное распределение. Унимодально и симметрично\n",
    "# Центральная предельная теорема. Для выборок стандартная ошибка среднего se=SDинд/n**0.5, где n - число элементов выборки\n",
    "# Если n выборка репрезентативная и число элементов > 30, то se=0.5\n",
    "# Интервал для поиска М генеральной совокупности(доверительный интервал): для 95% выборок Хсред ± 1.96*se включат в себя М"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ненормальные распределения\n",
    "# Сгладить распределение, уменьшив шкалу на основании полезности данных, удалив аномалии\n",
    "# Логарифмировать переменные (не забываем про ноль в исходной переменной). Схлопывает экстремальные значения\n",
    "# Логарифмирование отлично работает с ассиметричными распределениями\n",
    "# Если логарифмы переменных зависимы линейно, то значит сами переменные зависят нелинейно\n",
    "# Преобразование Бокса-Кокса подбирает оптимальную степень для возведения в нее mathworks.com/help/finance/boxcox.html\n",
    "# Bootstrap и метод Монте-Карло. Сравнивать медиану, мин, макс, 13-процентиль, среднее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Число наблюдений(N1), попавших в столбец. H = C*N1\n",
    "# H = N1/(N*длина интервала) - в таком случае гистограмма будет вероятностной, то есть в пределах единицы\n",
    "# Плотность распределения f(x) позволяет рассчитать вероятность P(A) попаданий в определенный интервал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ядерная оценка плотности Скотта-Сильвермана - обобщение гистограммы F(t) = (1/n*h)*сумма всех наблюдений K(t-Xi/h)\n",
    "# Распределение Японечникова определяет плотность К - симметричная, неотрицательная, с интегралом=1\n",
    "# Метод определяет меру сглаживания\n",
    "my_density = gaussian_kde(df['y'], bw_method=1)\n",
    "x = linspace(min(df['y']), max(df['y']), 1000)\n",
    "plot(x, my_density(x), 'g')  # распределение функции\n",
    "hist(df['y'], density=True, alpha=.3)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для определения \"типичного\" объекта совокупности можно использовать среднее(если нет выбросов) или медиану(если есть)\n",
    "# При неравномерном распределении можно убрать выбросы\n",
    "df_new = df.iloc[2:1004]\n",
    "# Или логарифмировать переменную (для лог-нормального распределения)\n",
    "x = np.log10(df[u'y'])\n",
    "pd.Series(x).hist(bins=45)\n",
    "# Усеченное среднее. Выбрасывается 2,5% самых малых и 2,5% наибольших значений переменной. Для новой БД считается среднее\n",
    "exclude = int(len(df)/100*2.5)\n",
    "redacted_town = df[exclude:len(df)-exclude]\n",
    "# Если рассевание нельзя разделить линейно, то меняем точку начала координат и выбираем новые параметры для разделения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Для получения комплексного сравнения объектов по нескольким переменным\n",
    "# Диагональ показывает ядерную оценку плотности\n",
    "# Матрица состоит из диаграмм рассеивания\n",
    "colors = {'genuine': 'green', 'counterfeit': 'red'}\n",
    "scatter_matrix(df,\n",
    "               figsize=(6, 6),\n",
    "               diagonal='kde',  # плотность вместо гистограммы на диагонали\n",
    "               c=df['data'].replace(colors),\n",
    "               alpha=0.2,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Иерархический кластерный анализ разделяет объекты на группы (стратификация). Число групп заранее неизвестно\n",
    "# Кластерный анализ позволяет сократить число наблюдений и проинтерпретировать их\n",
    "# Схожесть внутри кластера отображается как расстоянием между близкими объектами на диаграмме кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расстояние можно рассчитать методами: Евклида(или квадрата Евклида), Блок(Манхеттен), Хэмминга(для слов) и тд.\n",
    "# Манхеттен предпочтительнее, когда нет больших различий в рандомных переменных, потому что вес аномалий тогда меньше\n",
    "# Расстояние между кластерами рассчитывается:\n",
    "# Метод Варда (WARD) - позволяет работать с шаровыми скоплениями\n",
    "# Метод ближайших соседей (позволяет определять ленточные кластеры)\n",
    "# Средневзвешенное расстояние: среднее для суммы всех расстояний (также для ленточных)\n",
    "# Центроид: расстояние между кластерами равно расстоянию между их центрами тяжести\n",
    "# Методы дальнего и ближайшего соседа: расстояние между самыми дальними\\близкими объектами есть межкластер\n",
    "# Метод расстояния Sorencen-Dice Q = 2*|A^B|/|A|+|B|. Не работает если множества слабо пересекаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN основана на гипотезе компактности, если метрика удачна, то схожие примеры чаще лежат в одном классе, а не в разных\n",
    "# kNN часто используется для построения мета-признаков (прогноз kNN подается на вход прочим моделям) и рекомендаций\n",
    "# Качество классификации/регрессии зависит от числа соседей и метрики расстояния между объектами\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "# weights: \"uniform\" (все веса равны), \"distance\" (вес обратно пропорционален расстоянию)\n",
    "# algorithm: \"brute\", \"ball_tree\", \"KD_tree\", или \"auto\" \n",
    "# leaf_size: порог переключения на полный перебор в случае выбора BallTree или KDTree для нахождения соседей\n",
    "# metric: \"minkowski\", \"manhattan\", \"euclidean\", \"chebyshev\"\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "knn_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, knn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Алгоритмы кластерного анализа. Объекты группируются с ближайшими, пока нет скачка в расстояниях для следующего слияния\n",
    "# Момент для прекращения слияния определяется дендрограммой (для умеренного числа объектов)\n",
    "# Каменистая осыпь/локоть показывают скачок (резкий взлет графика) шагов объединений, когда кластеризуются тысячи объектов\n",
    "# Задача аналитика: отобрать переменные, выбрать метод стандартизации, установить расстояние между кластерами и объектами\n",
    "# обратная матрица существует только для несингулярных матриц, у которых нет линейной зависимости колонок или строк.\n",
    "\n",
    "# Объект, в котором будет хранится информация о последовательном слиянии кластеров\n",
    "# Для функции нужен фрейм, метод межкластера и метод межобъектов\n",
    "link = linkage(df, 'ward', 'euclidean')\n",
    "\n",
    "# link - матрица (n-1) x 4, где n - число наблюдений.\n",
    "# Каждая строка - результат слияния очередной пары кластеров с номерами link[i, 0] и link[i, 1].\n",
    "# Новому кластеру присваивается номер n + i\n",
    "# link[i, 2] означает расстояние между слитыми кластерами, а link[i, 3] - размер нового кластера.\n",
    "\n",
    "# Построение дендрограммы\n",
    "dn = dendrogram(link, orientation='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ключевые характеристики кластеров\n",
    "# В колонку cluster запишем номер кластера объекта с помощью функции fcluster.\n",
    "# Аргументы: linkage, пороговое значение для межкластера (либо число кластеров), criterion: distance для остановки разбиения\n",
    "# Останавливаем объединение, если расстояние между кластерами превышает 3\n",
    "df['cluster'] = fcluster(link, 3, criterion='distance')\n",
    "# Доля объектов в кластере, которые имеют соответствующие характеристики\n",
    "df.groupby(\"cluster\").mean()\n",
    "\n",
    "# Кластерный анализ методом К-средних\n",
    "\n",
    "# Создание модели\n",
    "model = KMeans(n_clusters=2,  # Число кластеров\n",
    "               # random_state - зерно датчика случайных чисел. Для воспроизводимости результата\n",
    "               random_state=42\n",
    "               )\n",
    "\n",
    "# подгонка модели по данным из БД\n",
    "model.fit(df)\n",
    "\n",
    "# Результат кластеризации на данных из БД\n",
    "model.labels_\n",
    "\n",
    "# координаты центров кластеров\n",
    "model.cluster_centers_\n",
    "\n",
    "# Добавление в кластер данных. Предсказание для новых наблюдений. Метод predict\n",
    "new_items = [\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "model.predict(new_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Число кластеров можно определить через график локтя для для разного числа кластеров\n",
    "# Метод inertia_ вернёт сумму расстояний от каждой точки данных до центра ближайшего у ней кластера\n",
    "# Кластеризацию можно считать условно хорошей, когда инерция перестаёт сильно уменьшаться при увеличении числа кластеров\n",
    "K = range(1, 11)\n",
    "models = [KMeans(n_clusters=k, random_state=42).fit(df) for k in K]\n",
    "dist = [model.inertia_ for model in models]\n",
    "\n",
    "# График локтя\n",
    "plt.plot(K, dist, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of distances')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n",
    "\n",
    "# В колонке NR находится номер объекта, его нужно исключить из данных для кластеризации\n",
    "del df['NR']\n",
    "\n",
    "# Оптимизируем модель, меняя число задаваемых кластеров на основании графика локтя\n",
    "model = KMeans(n_clusters=4, random_state=42)\n",
    "model.fit(df)\n",
    "df['cluster'] = model.labels_\n",
    "df.groupby('cluster').mean()\n",
    "\n",
    "# Смотрим к какому кластеру какие объекты относятся\n",
    "df['cluster'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка статистических гипотез:\n",
    "# Гипотеза согласия. Совпадает рандомная функция распределения с нормальным распределением? Самый дешевый и простой вариант\n",
    "# Гипотеза согласия2. Гипотеза об экспоненциальности распределения. Нужна, когда есть переменная времени ожидания\n",
    "# Гипотеза однородности. Совпадают две рандомные функции распредления? Например, чтобы сравнить данные до и после события\n",
    "# Гипотеза независимости. Нулевая гипотеза для рандомных объектов. Проверяется через коэффициент корреляции (скаляры)\n",
    "# Гипотеза о параметре распределения. Определение ключевых параметров. Например одинаковые средние или медианы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Альфа-это уровень значимости(0.05, 0.01. 0.005). Определеяет ошибки первого рода. На второго рода влияет размер выборки\n",
    "# Т- это статистика критерия. Если T<Cальфа, то верна нулевая гипотеза\n",
    "# Cальфа- это критическое значение. Вероятность отвергнуть правильную гипотезу(T>C) не должна превышать А(альфа)\n",
    "# p-value показывает насколько часто статистика критерия в верной гипотезе будет превышать реальные значения p=P{T>Tэксп}\n",
    "# Если p<A, гипотезу отвергаем. Если p>A, гипотезу не отвергаем. Проверяются все условия, при которых критерий будет работать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тесты Колмогорова-Смирнова и Shapiro-Wilk позволяют проверить выборку на принадлежность к ГС и нормальность распредеелния\n",
    "\n",
    "# Применяем критерий Шапиро-Вилка после логарифмирования.\n",
    "df = df.set_index(u'y')\n",
    "plt.hist(np.log10(df[u'data']), bins=50)\n",
    "res = stats.shapiro(np.log10(df[u'data']))\n",
    "print('p-value: ', res[1])\n",
    "# P очень маленькое, поэтому гипотезу о нормальности отвергаем.\n",
    "# Отклонения от нормальности будут несущественны, если убрать выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тест на гипотезу однородности\n",
    "# За один вариант дизайна выказалось 28 из 100 опрошенных, за второй 20 из 100 опрошенных.\n",
    "# Проверяем, является ли эта разница статистически значимой с помощью критерия хи-квадрат.\n",
    "\n",
    "# Cтроим таблицу сопряжённости.\n",
    "contingency_table = pd.DataFrame([[28, 72], [20, 80]],\n",
    "                                 index=['first', 'second'],\n",
    "                                 columns=['for', 'against']\n",
    "                                 )\n",
    "\n",
    "# AB-тест. Проверка разных вариантах на схожих выборках\n",
    "res = stats.chi2_contingency(contingency_table)\n",
    "print('p-value: {0}'.format(res[1]))\n",
    "\n",
    "# p-value получился достаточно большим, поэтому оснований отвергнуть гипотезу о равенстве долей нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cлучайная величина  имеет распределение Бернулли, если она принимает всего два значения (0 и 1 с вероятностями $\\theta$ и $\\ 1-\\theta$ соответственно) и имеет следующую функцию распределения вероятности:\n",
    "\\begin{equation}\n",
    "\\ p\\left(\\theta, x\\right) = \\theta^{x} \\left(1 - \\theta\\right)^\\left(1 - x\\right), x \\in \\left\\{0, 1\\right\\}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T07:55:31.964796Z",
     "start_time": "2021-02-08T07:55:31.956795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5.288388269523', '0.000000123399']\n",
      "['5.288388269523', '0.000000123399']\n"
     ]
    }
   ],
   "source": [
    "# Z-метка (организация выборок так, чтобы они мало отличались от нормального распределения)\n",
    "\n",
    "s1 = 135       # успех в выборке А\n",
    "n1 = 1781      # выборка А\n",
    "s2 = 47        # успех в выборке Б\n",
    "n2 = 1443      # выборка Б\n",
    "p1 = s1/n1  # оценка вероятности успеха выборка А\n",
    "p2 = s2/n2  # оценка вероятности успеха выборка Б\n",
    "p = (s1 + s2)/(n1+n2)  # оценка вероятности успеха выборки А+Б\n",
    "z = (p2-p1) / ((p*(1-p)*((1/n1)+(1/n2)))**0.5)  # Z-метка\n",
    "\n",
    "p_value = norm.cdf(z)  # Функция распределения нормального распределения\n",
    "\n",
    "#  z-метка и p-значение\n",
    "print(['{:.12f}'.format(a) for a in (abs(z), p_value * 2)])\n",
    "# Нулевая гипотеза отвергнута, статистические доли отличаются\n",
    "\n",
    "# То же самое, но со встроенным методом библиотеки statsmodels\n",
    "z1, p_value1 = sm.stats.proportions_ztest([s1, s2], [n1, n2])\n",
    "print(['{:.12f}'.format(b) for b in (z1, p_value1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тест Стьюдента на независимость переменных\n",
    "x = df[df['data'] == '1']['y']\n",
    "y = df[df['data'] == '0']['y']\n",
    "x.name, y.name = '1', '0'\n",
    "two_histograms(x, y)  # Данные условно нормальны.\n",
    "\n",
    "# Проверим c помощью критерия Флигнера-Килина, равны ли дисперсии.\n",
    "res = stats.fligner(x, y)\n",
    "# p-value низкое, гипотезу о равенстве дисперсий отвергаем, наблюдаемые объекты несвязные\n",
    "print('p-value: ', res[1])\n",
    "\n",
    "# Гипотезу о равенстве средних значений будем проверять с помощью теста Стьюдента при неравных дисперсиях\n",
    "# Опция equal_var=False говорит, что равенство дисперсии не предполагать\n",
    "res = stats.ttest_ind(x, y, equal_var=False)\n",
    "# P-значение значительно меньше альфы, гипотеза о равенстве отвергается\n",
    "print('p-value: ', res[1])\n",
    "\n",
    "# Ищем зависимость отклика от предиктора. Чтобы применить Стьюдента, проверим нормальность данных и равенство дисперсий\n",
    "# Заменяем -9999 (здесь=пустое) на корректное пустое значение.\n",
    "df = df.replace(-9999, np.nan)\n",
    "# Сохраним в отдельные переменные выборки, которые собираемся сравнивать.\n",
    "x = df[df['data'] == '1']['y']\n",
    "y = df[df['data'] == '0']['y']\n",
    "x.name, y.name = '1', '0'\n",
    "\n",
    "# Видно, что выбросы не дают применить Стюдента и нужно пробовать Манна-Витни\n",
    "two_histograms(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распределение Стьюдента (t-distribution) для n<30 - более высокие хвосты распределений.Число степеней свободы df=n-1\n",
    "# t заменяет Z в распределении Стьюдента. t=(Xинд-M)/(sd/n**0.5)\n",
    "# Помимо средних, нужно сравнить дисперсии D (тест Флигнера-Килина) и медианы (много n - тест Муда, мало n - Манн-Витни)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Парный t-критерий Стьюдента.  X1сред - Х2сред = А , se=((sd1**2/n1)+(sd2**2/n2))**0.5 , df=n1+n2-2\n",
    "# При t = A/se и df можно рассчитать p при котором M1-M2=0. То есть разницы между выборками почти нет\n",
    "# Q-Q Plot показывает насколько выборочные значения соответствуют предсказанным(из нормального распределеня)\n",
    "x = df['data']\n",
    "y = df['data1']\n",
    "x.name, y.name = 'data', 'data1'\n",
    "two_histograms(x, y)\n",
    "\n",
    "# Распределения условно нормальны. Поскольку в наблюдениях содержатся одни и те же люди, выборки связные (парные)\n",
    "res = stats.ttest_rel(x, y)  # Метод для парных выборок\n",
    "print('p-value: ', res[1])\n",
    "p-value: 0.0162416779538\n",
    "# p-value низкий, гипотеза на уровне значимости 0.05 будет отвергнута, но на уровне 0.01 уже нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-критерий Манна-Витни переводит значения в ранговую шкалу и проверяет НЕ равенство медиан. P{X>Y}=P{X<Y}\n",
    "\n",
    "res = stats.mannwhitneyu(x, y)\n",
    "print('p-value:', res[1])\n",
    "# p-value большое, поэтому у нас нет оснований отвергнуть нулевую гипотезу. Разница медиан в выборках случайна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Корреляция. Scatter-plot или диаграмма рассеивания\n",
    "# Сила и направление взаимосвязи определяется ковариацией. cov=Сумма((Xi-Xсред)*(Yi-Yсред))/N-1\n",
    "# Коэффициент корреляции Пирсона находится в промежутке [-1; 1] и считается как Rxy=cov/SDx*SDy\n",
    "# Коэффициент детерминации r**2 показывает влияние дисперсии одной переменной на другую в промежутке [0; 1]\n",
    "# Коэффициент Спирмена позволяет блокировать выбросы через ранги. d=X-Y. Rs=1-6*сумма d**2/N(N**2-1)\n",
    "# Часто корреляция обусловлена скрытой переменной\n",
    "\n",
    "# Корреляция двух предикторов\n",
    "plt.scatter(df['data'], df['data1'])\n",
    "\n",
    "# Допускаем что коэфффициент корреляции=0, но гипотеза отвергнута\n",
    "res = stats.pearsonr(df['data'], df['data1'])\n",
    "\n",
    "print('Pearson rho: ', res[0])\n",
    "print('p-value: ', res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация позволяет сделать вес важных переменных соизмеримым. Min=0(-1), max=1. ИЛИ Z\n",
    "# Z-Стандартизация: преобразование в тип, где М=0, sd = 1. Правило одной, двух и трех \"сигм\"\n",
    "# Z=(Xинд-М)/sd Пример: по таблице Z, где Хсред=150, sd=8, превышать Xинд будет 0.5z или 30%\n",
    "# Z=(Xсред-M)/se =(18,5-20)/0.5 = -3. Вероятность получить такой результат p = 0.0027\n",
    "\n",
    "# Если в БД нет единой метрики, то стандартизируем данные\n",
    "norm = preprocessing.StandardScaler()\n",
    "norm.fit(df)\n",
    "X = norm.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дисперсионный Анализ. Если межгрупповой показатель изменчивости сильно превышает внутригрупповой, то средние разнятся\n",
    "# SST - общая сумма квадратов показывает общую изменчивость данных. Сумма(Xинд-Xсред)**2  SST = SSW+SSB\n",
    "# SSW - сумма квадратов внутригрупповая. Сумма(X1инд-Х1сред)**2 + ...(XNинд-ХNсред)**2\n",
    "# SSB - сумма квадратов межгрупповая. SSB= n(X1сред - Хсред)**2 + ...n(XNсред-Хсред)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распределение Фишера, F-значение. F=(ssb/n-1)/(ssw/N-n). При верности нулевой гипотезы значения F очень маленькие\n",
    "# Поправка Бонферрони на множественную проверку гипотез. a = ai/n  НО: мешает получить значимые уровни различия\n",
    "# FDR или критерий Тьюки считает p-уровень для сравниваемых пар Xтэ=Xa-Xб\n",
    "# Двухакторный дисперсионный анализ SStotal=SSW+SSBa +SSBb + SSBa*SSBb\n",
    "# Взаимодействие факторов в ANOVA\n",
    "# Дисперсионный анализ требует нормальности распределения зависимой переменной и гомогенности дисперсии(тест Левена)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Регрессионнный Анализ позволяет исследовать взаимосвязи переменных и делать линию тренда\n",
    "# Простая Линейная Регрессия. Взаимосвязь 2-х переменных. Y-зависимая(отклик) Х-независимая(предиктор)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим модель линейной регрессии следующим образом:\n",
    "\\begin{equation}\n",
    "\\ \\vec y = X \\vec w + \\epsilon,\n",
    "\\end{equation}\n",
    " * $\\ \\vec y \\in \\mathbb{R}^n $ объясняемая переменная (отклик);\n",
    " * $\\ w $ - вектор параметров модели (вес);\n",
    " * $\\ X $ - матрица наблюдений и признаков размерности  строк на столбцах (включая фиктивную единичную колонку слева) с полным рангом по столбцам: $\\ \\text{rank}\\left(X\\right) = m + 1 $;\n",
    " * $\\epsilon$ - случайная переменная, соответствующая непрогнозируемой ошибке модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выражение для каждого конкретного наблюдения [линейной регрессии](https://habr.com/ru/company/ods/blog/322076/):\n",
    "\\begin{equation}\n",
    "\\ y_i = \\sum_{j=0}^m w_j X_{ij} + \\epsilon_i\n",
    "\\end{equation}\n",
    "При ограничениях:\n",
    "* матожидание случайных ошибок равно нулю: $\\ \\forall i: \\mathbb{E}\\left[\\epsilon_i\\right] = 0$;\n",
    "* дисперсия случайных ошибок одинакова и конечна (гомоскедастична): $\\forall i: \\text{Var}\\left(\\epsilon_i\\right) = \\sigma^2 < \\infty$\n",
    "* случайные ошибки не скоррелированы: $\\forall i \\neq j: \\text{Cov}\\left(\\epsilon_i, \\epsilon_j\\right) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка распределена нормально с центром в нуле и некоторым разбросом: $\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка $\\hat{w}_i$ весов $\\ w_i$ называется линейной, если\n",
    "$ \\ \\hat{w}_i = \\omega_{1i}y_1 + \\omega_{2i}y_2 + \\cdots + \\omega_{ni}y_n,$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка $\\hat{w}_i$ называется несмещенной, когда матожидание оценки равно реальному, но неизвестному значению оцениваемого параметра: $\\ \\mathbb{E}\\left[\\hat{w}_i\\right] = w_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из способов вычислить значения параметров модели является метод наименьших квадратов (МНК), который минимизирует среднеквадратичную ошибку между реальным значением отклика и прогнозом, выданным моделью:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ \\begin{array}{rcl}\\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) &=& \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\\\ &=& \\frac{1}{2n} \\left\\| \\vec{y} - X \\vec{w} \\right\\|_2^2 \\\\ &=& \\frac{1}{2n} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right) \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "Для решения данной оптимизационной задачи необходимо вычислить производные по параметрам модели, приравнять их к нулю и решить полученные уравнения относительно $ \\vec w$. <br>\n",
    "Оценка МНК является лучшей оценкой параметров модели, среди всех линейных и несмещенных оценок, то есть обладающей наименьшей дисперсией. И она же является максимизацией правдоподобия данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y=B0(intercept)+B1(slope). Зачение Y, где линия пересекает ось, угол наклона линии к оси X\n",
    "# Метод наименьших квадратов(МНК) находит оптимальные параметры B0 и B1, чтобы сумма квадратов (SE) была минимальна MSE\n",
    "# Уравнение регрессии Y=B0+B1*X1\n",
    "# B1 = SDy/SDx*Rxy, B0 = (Yсред-B1*Xсред), t = B1/se, df=N-2 Если B1 близка к нулю, то взаимосвязи почти нет\n",
    "# Коэффтцтент Детерминации (выборочная дисперсия) R указывает какой процент вариации отклика определяется влиянием предиктора\n",
    "# R**2 = 1-(SSres/SStotal) доля дисперсии Y, объясняемая регрессионной моделью. Чем больше R , тем лучше\n",
    "# Требования: линейная вхаимосвязь X Y, нормальное распределение остатков, гомоскедатичность(изменчивость) остатков\n",
    "# Избежать ошибок спецификации при линейной регрессии помогает Анализ Остатков. Выявлять колинеарность\n",
    "# Число обусловленности Y по отношению к X измеряет изменения Y при колебании Х, т.е. чувствительность функции\n",
    "# Регуляризация — способ уменьшить сложность модели чтобы предотвратить переобучение или исправить некорректную задачу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Алгоритм обучения - выбор из большого семейства гипотез по критерию меры качества\n",
    "# Модель обладает обобщающей способностью, когда ошибка на тестовом наборе данных мала или предсказуема \n",
    "# Эмпирический риск (функция стоимости) принимает форму среднеквадратичной ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расщепление на обучающую и тестовые выборки\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Линейная регрессия\n",
    "\n",
    "# Строим модель\n",
    "model = LinearRegression()\n",
    "\n",
    "# обучаем модель\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Строим предсказание модели на тестовом множестве\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Промежуточные Регрессионные Коэффициенты от метода model.coef_ и свободный член от метода model.intercept_\n",
    "coef = pd.DataFrame(zip(['intercept'] + X.columns.tolist(), [model.intercept_] + model.coef_.tolist()),\n",
    "                    columns=['predictor', 'coef'])\n",
    "\n",
    "# Матрица показывает базовую цену и вес коэффициентов: 83.17 + 0.29*площадь SQFT + 12.17*удобства и т.д.\n",
    "# Логика показывает, что что-то не то. Проверяем на колинеарность\n",
    "df.corr()\n",
    "\n",
    "# Видим, что колинеарен TAX. Убираем и снова считаем, в этот раз с p-значением\n",
    "regression_coef(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка прогноза любой модели вида $\\ y = f\\left(\\vec{x}\\right) + \\epsilon$ складывается из:\n",
    "* квадрата смещения: $\\ \\text{Bias}\\left(\\hat{f}\\right)$ – средняя ошибка по всевозможным наборам данных;\n",
    "* дисперсии: $\\ \\text{Var}\\left(\\hat{f}\\right)$  – вариативность ошибки, то, на сколько ошибка будет отличаться, если обучать модель на разных наборах данных;\n",
    "* неустранимой ошибки: $\\ \\sigma^2$ . <br>\n",
    "\n",
    "При увеличении сложности модели (например, при увеличении количества свободных параметров) увеличивается дисперсия (разброс) оценки, но уменьшается смещение. Теорема Маркова-Гаусса как раз утверждает, что МНК-оценка параметров является лучшей в классе несмещенных линейных оценок, с наименьшей дисперсией. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В линейной регрессии пространство гипотез ограничено только линейными функциями от признаков. В полиномиальной регрессии пространство гипотез расширено до всех полиномов степени . Тогда в нашем случае, когда количество признаков равно одному , пространство гипотез будет выглядеть следующим образом:\n",
    "\n",
    "$\\ \\begin{array}{rcl} \\forall h \\in \\mathcal{H}, h\\left(x\\right) &=& w_0 + w_1 x + w_1 x^2 + \\cdots + w_n x^p \\\\ &=& \\sum_{i=0}^p w_i x^i \\end{array} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Полиномиальная регрессия\n",
    "# Множественная регрессия  Y= B0+B1*X1 + ... + BN*XN   Многомерный scatter-plot\n",
    "# Дополнительно требует: мультиколлинеарность(без сильной корреляции или идентичности), нормальное распределение переменных.\n",
    "# t-критерий показывает оказываемое влияние каждого предиктора. Если 0, то влияния нет\n",
    "# Для множественной регрессии используется \"Исправленный\" R**2\n",
    "# Предсказать результат не только с помощью переменной (1я модель), но и её квадрата(2я модель) и их обеих (3я модель)\n",
    "# Класс PolynomialFeatures, метод fit_transform сгенерирует из множества фич множество одночленов заданной степени\n",
    "# Например, для степени 2 и фич a, b будут сгенерированы фичи [a, b, a**2, b**2, ab]\n",
    "# при указанном параметре include_bias=True ещё и вектор-свободный член из единиц.\n",
    "\n",
    "import statsmodels.api as sm\n",
    "poly = PolynomialFeatures(\n",
    "    # Максимальная степень\n",
    "    degree=2,\n",
    "    # Не генерировать свободный член\n",
    "    include_bias=False)\n",
    "y = df['price']\n",
    "X0 = poly.fit_transform(df[['weight']])\n",
    "X0 = pd.DataFrame(X0, columns=['weight', 'weight**2'])\n",
    "\n",
    "X0 = [\n",
    "    # Одна оригинальная переменная weight\n",
    "    X0[['weight']],\n",
    "    # Одна переменная weight**2\n",
    "    X0[['weight**2']],\n",
    "    # Две переменных weight и weight**2\n",
    "    X0.copy()]\n",
    "models = [LinearRegression() for _ in X0]\n",
    "\n",
    "for X, model in zip(X0, models):\n",
    "    model.fit(X, y)\n",
    "    print(model.score(X, y))\n",
    "\n",
    "# 𝑅**2  во всех моделях очень большой и примерно одинаков. Но на самом деле модели различны. Проверим их более тщательно\n",
    "\n",
    "regression_coef(models[0], X0[0], y)\n",
    "regression_coef(models[1], X0[1], y)\n",
    "regression_coef(models[2], X0[2], y)\n",
    "\n",
    "# Коэффициенты показывают спорные моменты в 1 и 3 моделях. 3-я ошибается из-за колинеарности (ложной)\n",
    "\n",
    "X2 = sm.add_constant(X0[2])\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основе байесовой статистики находится формула Байеса, нацеленная на поиски апостериорного распределениея:\n",
    "\n",
    "$ \\color{green}{p\\left(y \\mid x\\right)} = \\dfrac{\\color{orange}{p\\left(x \\mid y\\right)} \\color{blue}{p\\left(y\\right)}}{\\color{red}{p\\left(x\\right)}} $\n",
    "\n",
    " * $\\color{blue}{p\\left(y\\right)}$ априорные ожидания (prior): насколько правдоподобна гипотеза перед наблюдением данных;\n",
    " * $\\color{orange}{p\\left(x \\mid y\\right)}$ правдоподобие (likelihood): насколько правдоподобны данные при условии того, что гипотеза верна;\n",
    " * $\\color{red}{p\\left(x\\right) = \\sum_{z} p\\left(x \\mid z\\right) p\\left(z\\right)}$ маргинальная вероятность (marginal probability): вероятность данных, усредненная по всевозможным гипотезам;\n",
    " * $\\color{green}{p\\left(y \\mid x\\right)}$ апостериорное распределение (posterior): насколько правдоподобна гипотеза при наблюдаемых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Прогнозирование временных рядов\n",
    "# Линейная регрессия плохой метод, но безалтернативен при коротких временных рядах или двух или более факторах сезонности\n",
    "\n",
    "# Преобразуем строчки с датами в объект datetime\n",
    "# format показывает что читаем: '%b %Y' трехбуквенный месяц, затем год\n",
    "df['date'] = pd.to_datetime(df['date'], format='%b %Y')\n",
    "\n",
    "# Построим график проверить тип тренда (линейный или нет), тип сезонности (аддитивный или мультипликативный), его длину, выбросы\n",
    "# Видим линейный тренд и мультипликативную сезонность. Это подтверждается после логирафмирование цикла\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "df['series_g'].plot(ax=ax1)\n",
    "ax1.set_title(u'Объём пассажироперевозок')\n",
    "ax1.set_ylabel(u'Тысяч человек')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "pd.Series(np.log10(df['series_g'])).plot(ax=ax2)\n",
    "ax2.set_title(u'log10 от объёма пассажироперевозок')\n",
    "ax2.set_ylabel(u'log10 от тысяч человек')\n",
    "\n",
    "# Вывод: будем строить модель линейной регрессии для приближения логарифма от объёма перевозок.\n",
    "# log𝑦𝑖=𝛽𝑥𝑖+𝑐(𝑥𝑖)+𝜀𝑖, где  𝑦𝑖 объём перевозок,  𝑥𝑖 порядковый номер месяца,  𝑐(𝑥𝑖) сезонная составляющая,  𝜀𝑖  случайный шум\n",
    "# Создадим новый объект класса DateTimeIndex для 12 новых дат (месяцев) с помощью функции pd.date_range.\n",
    "# Создаём последовательсть месяцев. freq='MS' означает первое число каждого месяца из указанного диапазона\n",
    "new_dates = pd.date_range('1961-01-01', '1961-12-01', freq='MS')\n",
    "\n",
    "# Приводим df['date'] к типу Index, объединяем с 12 месяцами, полученными на предыдущем шаге\n",
    "new_dates = pd.Index(df['date']) | new_dates\n",
    "\n",
    "# Создаём датафрейм из одной колонки с расширенным набором дат\n",
    "df2 = pd.DataFrame({'date': new_dates})\n",
    "# Объединяем два датафрейма по колонке 'date'.\n",
    "# Склеиваем по указанной колонке (on) и правилу склейки (how)\n",
    "df = pd.merge(df, df2, on='date', how='right')\n",
    "\n",
    "# Регрессионная переменная month_num - порядковый номер пары (месяц, год). Логарифмируем таргет\n",
    "df['month_num'] = range(1, len(df) + 1)\n",
    "df['log_y'] = np.log10(df['series_g'])\n",
    "\n",
    "# Создадем 12 колонок season_1.., season_12, в которые поместим индикаторы соответствующего месяца\n",
    "# Чтобы избежать колинеарности, исключаем один из месяцев(январь) и делаем его эталоном, с которым сравниваем все остальные\n",
    "# Внутри цикла проверяем, равен ли очередной месяц текущему значению из цикла\n",
    "for x in range(1, 13):\n",
    "    df['season_' + str(x)] = df['date'].dt.month == x\n",
    "\n",
    "# xrange(2, 13) соответствует всем месяцам с февраля по декабрь\n",
    "season_columns = ['season_' + str(x) for x in range(2, 13)]\n",
    "\n",
    "# Создадим матрицу X и вектор y для обучения модели\n",
    "X = df[['month_num'] + season_columns]\n",
    "y = df['log_y']\n",
    "\n",
    "# Оставим только те строчки, у которых известны значения y (с номером < 144)\n",
    "X1 = X[X.index < 144]\n",
    "y1 = y[y.index < 144]\n",
    "\n",
    "# Настроим регрессионную модель. \"Подгонка\" через .fit\n",
    "model = LinearRegression()\n",
    "model.fit(X1, y1)\n",
    "\n",
    "pred = pd.DataFrame({\n",
    "    'pred': model.predict(X1),\n",
    "    'real': y1})\n",
    "pred.plot()\n",
    "\n",
    "# строим предсказание для всей матрицы X, включая неизвестные 12 месяцев\n",
    "pred = pd.DataFrame({\n",
    "    'pred': model.predict(X),\n",
    "    'real': y})\n",
    "pred.plot()\n",
    "\n",
    "# Экспонируем прогноз, чтобы получить реальные числа\n",
    "pred['number'] = 10**pred['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Логистическая регрессия позволяет исседовать взаимосвязи для зависимой переменной с двумя значениями (0,1)\n",
    "# Линейный классификатор делит признаковое пространство гиперплоскостью на 2 части, в которых прогнозируется бинарный отклик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Результирующая переменная в порядковой шкале\n",
    "# При невозможности распознать классы их следует отбросить (укрупняя группировку)\n",
    "# Критерии качества: #Accuracy (доля правильных ответов). Precision (точночть). Recall (полнота) ИЛИ\n",
    "# AUC area under curve - общая площадь под кривой(один классификатор и множество пороговых значений)\n",
    "# ROC reciever operator curve. FPR=FP/(FP+TP). Чем ближе точка в РОК кривой в классификаторе к TPR, тем лучше\n",
    "# Пороговое значение управляет качеством классификатора. Например, можно увеличивать охват, но снижать долю\n",
    "\n",
    "# Если несколько классов, но хочется сделать классификацию строго бинарной, то разбиваем на группы ДА и НЕТ\n",
    "#df['Desired1(3)'] = df['Desired1(3)'].replace(0, 1)\n",
    "\n",
    "x = np.array(np.arange(-10, 10, 0.5))\n",
    "y = 1. / (1 + np.exp(-x))\n",
    "plt.plot(x, y)\n",
    "plt.title(u'Логистическая функция')\n",
    "\n",
    "df = pd.read_csv('bank-full.csv', sep=';')\n",
    "df = df.dropna()\n",
    "\n",
    "# как распределены пропущенные значения 'unknown' по колонкам.\n",
    "df.apply(lambda x: sum(x == 'unknown'), axis=0)\n",
    "\n",
    "# Удалим самые пустые колонки полностью и выбросим строчки с пропущенными значениями в job и education\n",
    "df = df.drop(['contact', 'poutcome'], axis=1)\n",
    "df = df[(df['job'] != 'unknown') & (df['education'] != 'unknown')]\n",
    "df.shape\n",
    "\n",
    "print(df['education'].value_counts())\n",
    "print(df['default'].value_counts())\n",
    "print(df['housing'].value_counts())\n",
    "print(df['loan'].value_counts())\n",
    "print(df['y'].value_counts())\n",
    "\n",
    "# Преобразуем бинарные столбцы в численные. Колонку y тоже\n",
    "df['education'] = df['education'].map(\n",
    "    {'primary': 0, 'secondary': 1, 'tertiary': 2})\n",
    "df['default'] = df['default'].map({'no': 0, 'yes': 1})\n",
    "df['housing'] = df['housing'].map({'no': 0, 'yes': 1})\n",
    "df['loan'] = df['loan'].map({'no': 0, 'yes': 1})\n",
    "df['y'] = df['y'].map({'no': 0, 'yes': 1})\n",
    "\n",
    "# Категориальные колонки не имеют естественного порядка, поэтому преобразуем их с помощью one-hot encoding\n",
    "cat_features = ['job', 'marital', 'month']\n",
    "df = pd.get_dummies(df, columns=cat_features)\n",
    "\n",
    "# Разделяем на предикторы и отклик\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "\n",
    "# Создаем модель\n",
    "model = LogisticRegression(\n",
    "    # метод для поиска решения. Для больших - sag и saga. Варианты: newton-cg, lbfgs\n",
    "    solver='liblinear',\n",
    "    penalty='l2',  # норма для регуляризации. Варианты: l2, l1\n",
    "    C=1,  # параметр регуляризации. Чем меньше, тем сильнее регуляризация. Можно искать greedsearch\n",
    "    tol=1e-4,  # параметр для остановки поиска решения.\n",
    "    multi_class='ovr'  # Уточняем, что всего 2 класса\n",
    ")\n",
    "\n",
    "# Обучаем модель\n",
    "model.fit(X, y)\n",
    "\n",
    "# Для анализа предсказания результатов строим матрицу ошибок\n",
    "preds = model.predict(X)\n",
    "conf_mat = metrics.confusion_matrix(y, preds)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "# Смотрим результат рассчета вероятности\n",
    "pred_prob = model.predict_proba(X)\n",
    "\n",
    "# Рассчитываем РОК-кривую, указывая FPR, TPR\n",
    "preds = pred_prob[:, 1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Рисуем РОК-кривую\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задача распознавания наименований или порядков через деревья классификации. И чисел через регрессию\n",
    "# Помимо внутренних параметров (заданных изначально), есть еще внешние (задаваемые аналитиком)\n",
    "# Выбор модели с помощью обучающей/тестовой выборок через наименьшую среднюю квадратичную ошибку\n",
    "# Критерий качества Q - сумма модулей ошибок или сумма квадратов ошибок или процент ошибок и т.д.\n",
    "# Валидация - метод проверки выбранной модели на ее адекватность\n",
    "# Регуляризация - инструмент проверки моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART - Classification and regression trees\n",
    "# деление матрицы прямыми\\гиперплоскостями, чтобы в ограниченных областях доминировали схожие объекты\n",
    "# Узел(node) - множество, которое расщепляется. Родительский, потомок, конечный.\n",
    "# Пороговое значение - эталон для сравнения\n",
    "# Ограничения задаются оператором. На кол-во слоев, на свойство потомков, на родителя, на правила остановки\n",
    "# Чистота - порядок разделения выборки на части, в каждой из которых \"загрязнение\" данных меньше\n",
    "# Критерий загразненности(вероятность принадлежать к классу P) измеряется Энтропией, Индексом Джини или Ошибкой Классификации\n",
    "# Энтропия H1 = -СуммаP*log2P. Индекс Джини H2 = 1-СуммаP**2 = СуммаP*(1-P). Ошибка Классификации H3 = 1-maxP\n",
    "# Дельта H - вклад переменной в очищение. Считаем суммы для каждой и получаем информативность переменной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энтропия Шеннона соответствуете степени хаоса в системе. Чем выше энтропия, тем менее упорядочена система.\n",
    "\\begin{equation}\n",
    "\\ S = -\\sum_{i=1}^{N}p_i \\log_2{p_i},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В деревьях классификации прирост информации IG при разбиении выборки по признаку Q определяется как\n",
    "\n",
    "\\begin{equation}\n",
    "\\ IG(Q) = S_O - \\sum_{i=1}^{q}\\frac{N_i}{N}S_i,\n",
    "\\end{equation}\n",
    "\n",
    "где q число групп после разбиения, Ni число элементов выборки, у которых признак Q имеет i-ое значение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неопределенность Джини: максимизация числа пар объектов одного класса, оказавшихся в одном поддереве.\n",
    "\\begin{equation}\n",
    "\\ G = 1 - \\sum\\limits_k (p_k)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка классификации: \n",
    "\\begin{equation}\n",
    "\\ E = 1 - \\max\\limits_k p_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# В основе алгоритмов построения деревmев решений лежит принцип максимизации прироста информации\n",
    "import pydot\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Инициализируем и обучаем модель\n",
    "model = DecisionTreeClassifier(random_state=42,\n",
    "                               # функция для impurity ('gini' или 'entropy')\n",
    "                               criterion='gini',\n",
    "                               max_depth=5,                               \n",
    "                               min_samples_split=5, # минимальное число элементов в узле для разбиения (может быть долей)\n",
    "                               min_samples_leaf=5, # минимальное число элементов в листе (может быть долей)\n",
    "                               # min_impurity_decrease=0, # минимальное значение дельты impurity\n",
    "                               class_weight=None  # веса для классов\n",
    "                               )\n",
    "\n",
    "# обучаем модель\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Строим предсказание модели на тестовом множестве\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Для интерпретации получившейся модели изображаем её в виде дерева предикатов (решающих правил)\n",
    "export_graphviz(clf_tree, feature_names=['x1', 'x2'],\n",
    "                out_file='small_tree.dot', filled=True)\n",
    "\n",
    "# Расширенные настройки\n",
    "export_graphviz(model,\n",
    "                out_file='tree.dot',\n",
    "                feature_names=X.columns,  # задать названия фич\n",
    "                class_names=None,\n",
    "                label='all',  # показывать названия полей у численных значений внутри узла\n",
    "                filled=True,  # раскрашивать узлы в цвет преобладающего класса\n",
    "                impurity=True,  # показывать значение impurity для каждого узла\n",
    "                node_ids=True,  # показывать номера узлов                \n",
    "                proportion=True, # Показывать доли каждого класса в узлах (а не количество)                \n",
    "                rotate=True, # Повернуть дерево на 90 градусов (вертикальная ориентация)\n",
    "                precision=3  # Число точек после запятой для отображаемых дробей\n",
    "                )\n",
    "\n",
    "# Преобразуем файл .dot в .png\n",
    "# node - номер узла, X[1]<=1.5 правило расщепления, gini, samples-доля наблюдений попавших в узел, p-value (p0, pX)\n",
    "!dot - Tpng 'small_tree.dot' -o 'small_tree.png'\n",
    "# ИЛИ\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree.png')\n",
    "Image(\"tree.png\")\n",
    "\n",
    "# Модель позволяет оценить ценность (importance) и эффективность каждой фичи, считая для каждой из сумму дельты H\n",
    "pd.DataFrame({'feature': X.columns,\n",
    "              'importance': model.feature_importances_}).sort_values('importance',\n",
    "                                                                     ascending=False\n",
    "                                                                     )\n",
    "\n",
    "# Метод predict позволяет получить предсказания классов для входного списка элементов (подаём на вход матрицу)\n",
    "# Предсказание класса для новых элементов\n",
    "new_item = [1, 1, 1, 1]\n",
    "model.predict([new_item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При прогнозировании количественного признака критерий качества меняется на дисперсию вокруг среднего:\n",
    "\\begin{equation}\n",
    "\\ D = \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} (y_i - \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} y_i)^2,\n",
    "\\end{equation}\n",
    "$\\ell$ - число объектов в листе,  $\\ y_i$ – значения целевого признака. Минимизируя дисперсию, значения отклика в каждом листе будут примерно равны.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Самый простой вариант обучения дерева и просмотра результатов\n",
    "model = DecisionTreeClassifier(random_state=17)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "export_graphviz(age_tree, feature_names=['data', 'data1'],\n",
    "                out_file='tree.dot', filled=True)\n",
    "\n",
    "!dot - Tpng 'tree.dot' - o 'tree.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества классификатора: доля совпавших ответов в y_pred и y_test, или считаем точность и полноту\n",
    "# Если доля в обучающем выше тестового, означает переобученность модели. Нужно упрощать модель\n",
    "# Матрица ошибок  𝐶=(𝑐𝑖,𝑗) , где  𝑐𝑖,𝑗 количество элементов класса 𝑖 , которым классификатор присвоил класс 𝑗\n",
    "# Точность(precision) - доля правильно классифицированных объектов в найденных классификатором.\n",
    "# Полнота(recall) - доля этих объектов НА САМОМ ДЕЛЕ\n",
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "# Гармоническое среднее F1 = 2*точность*полнота/(точность+полнота). Считается с помощью classification_report\n",
    "print(metrics.classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Деревья решений для задач регрессии (отклик не дискретный, а непрерывный). Методы схожы с деревом классификации\n",
    "# Предпочтительнии линейной регрессии, когда зависимость не линейная :)\n",
    "# В этом случа Дельта H = сумма квадратов ошибок\n",
    "# Prune (обрезание) - очистка от узлов, которые не нужны, через добавление третьей выборки (валидации)\n",
    "\n",
    "# Случайный лес. Ключевые параметры:\n",
    "# ntree - число деревьев(в начале макс, потом сокращать), mtry - число переменных в выборке (M**0.5)\n",
    "# sampsize - число наблюдений в подвыборке(0.632*N для декорреляции), nodesize - мин. число наблюдений в узле (10)\n",
    "# replace - подвыборка с  возвращением или без\n",
    "# out-of-bag - неиспользуемая при обучении часть выборки, используется в качестве предварительного теста модели\n",
    "\n",
    "# Если несколько классов, но хочется сделать классификацию строго бинарной, то разбиваем на группы ДА и НЕТ\n",
    "df['Desired1(3)'] = df['Desired1(3)'].replace(0, 1)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(random_state=42,  # зерно датчика случайных чисел\n",
    "                               # число деревьев в лесу\n",
    "                               n_estimators=30,\n",
    "                               # функция для дельта H, impurity ('gini' или 'entropy')\n",
    "                               criterion='gini',\n",
    "                               # Макс число слоев\n",
    "                               max_depth=5,\n",
    "                               # Вычислять out-of-bag ошибку\n",
    "                               oob_score=True,\n",
    "                               # использовать результаты предыдущего вызова и нарастить предыдущий лес\n",
    "                               warm_start=False,\n",
    "                               # веса классов для балансировки выборки для обучения\n",
    "                               class_weight=None\n",
    "\n",
    "                               )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_pred, y_test))\n",
    "\n",
    "print('Out-of-bag score: {0}'.format(model.oob_score_))\n",
    "\n",
    "pd.DataFrame({'feature': X.columns,\n",
    "              'importance': model.feature_importances_}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приемы улучшения классификаторов: stacking, bagging, boosting\n",
    "# Stacking(предсказание на базе предсказаний)\n",
    "# Bagging(усредненное мнение всех моделей), он же случайный лес. Чтобы избежать колинеарности, выборки собираются рандомно\n",
    "# Boosting - обучение на основе ошибок предыдущего классификатора (улучшением слабого классификатора)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GBM - Gradient Boosting Machine. Остановка бустинга, когда очередные циклы перстают улучшать модель\n",
    "# Сумма квадратов ошибок Zi = -2*(Yi - f(Xi))\n",
    "# Метод максимального правдоподобия. Предполагаем наиболее вероятное событие. Критерий качества = P**A*(1-P)**(n-A)\n",
    "# Критерии качества Гаусса и Лапласа универсальны. Для двух классов - биномиальное, для большего - мультиноминальное\n",
    "# При временном промежутке - распределение Пуассона\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn import metrics\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "           'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "df = pd.read_csv('adult.data', header=None, names=columns, na_values=' ?')\n",
    "\n",
    "# Удаляем колонку education (потому что есть уже закодированная колонка education-num)\n",
    "df = df.drop('education', axis=1)\n",
    "\n",
    "# Кодируем отклик в бинарные значения\n",
    "df['income'] = df['income'].map({' <=50K': 0, ' >50K': 1})\n",
    "\n",
    "# удаляем строки с NA значениями\n",
    "df = df.dropna()\n",
    "\n",
    "# Создаем фрейм специально для теста\n",
    "test = pd.read_csv('adult.test', header=None, names=columns,\n",
    "                   na_values=' ?', skiprows=1)\n",
    "test = test.drop('education', axis=1)\n",
    "test['income'] = test['income'].map({' <=50K.': 0, ' >50K.': 1})\n",
    "test = test.dropna()\n",
    "\n",
    "# Распределение классов в отклике\n",
    "df['income'].value_counts(normalize=True)\n",
    "\n",
    "# Разбиваем дату. Бинаризуем категориальные признаки (one-hot encoding).\n",
    "X_train = pd.get_dummies(df).drop('income', axis=1)\n",
    "y_train = df['income']\n",
    "\n",
    "X_test = pd.get_dummies(test).drop('income', axis=1)\n",
    "y_test = test['income']\n",
    "\n",
    "# В тестовой выборке не хватает одной колонки\n",
    "print(len(X_train.columns))\n",
    "print(len(X_test.columns))\n",
    "\n",
    "# Приводим множество названий колонок к типу set, находим разность двух множеств: Голландии нет в колонке native-county\n",
    "print(set(X_train.columns) - set(X_test.columns))\n",
    "print(set(X_test.columns) - set(X_train.columns))\n",
    "\n",
    "# Добавляем недостающую колонку. Смотрим, стоит ли склеивать отдельные переменные в более крупные классы\n",
    "columns = set(X_train.columns) | set(X_test.columns)\n",
    "X_train = X_train.reindex(columns=columns).fillna(0)\n",
    "X_test = X_test.reindex(columns=columns).fillna(0)\n",
    "\n",
    "# Проверяем совпадение колонок (если да, то True)\n",
    "all(X_train.columns == X_test.columns)\n",
    "\n",
    "# Создаем и обучаем модель\n",
    "model = GradientBoostingClassifier(random_state=42,\n",
    "                                   # Число деревьев\n",
    "                                   n_estimators=500,\n",
    "                                   # загрязнение измеряем “mse”, “mae” или “friedman_mse” (mse с улучшениями)\n",
    "                                   criterion='friedman_mse',\n",
    "                                   # Максимальная глубина каждого дерева\n",
    "                                   # критерий качества ‘deviance’ (кросс-энтропия) или ‘exponential’\n",
    "                                   # ‘deviance’ для классификации с вероятностями на выходе\n",
    "                                   loss='deviance',\n",
    "                                   # минимальное уменьшение загрязнения\n",
    "                                   min_impurity_decrease=0.0,\n",
    "                                   # Устарело\n",
    "                                   min_impurity_split=None,\n",
    "                                   # число узлов в дереве\n",
    "                                   max_depth=5,\n",
    "                                   # минимальное число наблюдений в потомке\n",
    "                                   min_samples_leaf=5,\n",
    "                                   # минимальное число наблюдений в родителе\n",
    "                                   min_samples_split=10,\n",
    "                                   # Параметр, уменьшающий переобучение, являющемся весом отдельного дерева (меньше лучше)\n",
    "                                   learning_rate=0.01\n",
    "                                   )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Строим предсказание (обучающая, тестовая)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "# Смотрим точность(ошибки 1 и 2го рода)\n",
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "# Cмотрим важность признаков\n",
    "fi = pd.DataFrame({'features': X_train.columns,\n",
    "                   'importance': model.feature_importances_})\n",
    "fi.sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "# Калибровка (интерпретация вероятности)\n",
    "model_sigmoid = CalibratedClassifierCV(model, cv=2, method='sigmoid')\n",
    "# method : ‘sigmoid’ or ‘isotonic’\n",
    "\n",
    "# Calibrate probabilities\n",
    "model_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "# View calibrated probabilities\n",
    "model_sigmoid.predict_proba(X_test)[0:11, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost - множество деревьев регрессии. q - структура дерева.\n",
    "# W - вес узла, набор правил попадения наблюдений в конечный узел.\n",
    "# T - количество конечных узлов, для избежание переобучения. Автоматом определяется гаммой\n",
    "# L - критерий качества XGBoost состоит из традиционного(Q)+регуляризация(1/2*гамма*T). Влияет на изменения критерия чистоты\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "           'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "df = pd.read_csv('adult.data', header=None, names=columns, na_values=' ?')\n",
    "# Удаляем колонку education (потому что есть уже закодированная колонка education-num)\n",
    "df = df.drop('education', axis=1)\n",
    "# Кодируем отклик в бинарные значения\n",
    "df['income'] = df['income'].map({' <=50K': 0, ' >50K': 1})\n",
    "# удаляем строки с NA значениями\n",
    "df = df.dropna()\n",
    "\n",
    "test = pd.read_csv('adult.test', header=None, names=columns,\n",
    "                   na_values=' ?', skiprows=1)\n",
    "test = test.drop('education', axis=1)\n",
    "test['income'] = test['income'].map({' <=50K.': 0, ' >50K.': 1})\n",
    "test = test.dropna()\n",
    "\n",
    "X_train = pd.get_dummies(df).drop('income', axis=1)\n",
    "y_train = df['income']\n",
    "X_test = pd.get_dummies(test).drop('income', axis=1)\n",
    "y_test = test['income']\n",
    "\n",
    "columns = set(X_train.columns) | set(X_test.columns)\n",
    "X_train = X_train.reindex(columns=columns).fillna(0)\n",
    "X_test = X_test.reindex(columns=columns).fillna(0)\n",
    "\n",
    "\n",
    "# Создаем модель. В выборе параметров помогают grid_search и валидация\n",
    "model = XGBClassifier(seed=42,\n",
    "                      booster=gbtree,  # выбор деревьев gbtree или линейных gblinear\n",
    "                      silent=True,  # НЕ вывод промежуточных результатов\n",
    "                      n_estimators=100,  # Предельное число деревьев\n",
    "                      learning_rate=0.02,  # скорость обучения\n",
    "                      min_child_weight=5,  # минимальные веса в узле-потомке\n",
    "                      max_leaf_nodes=6,  # Макс. значение конечных узлов в дереве\n",
    "                      max_depth=5,  # Максимальное число слоев дерева\n",
    "                      gamma=0.05,  # Запрещает расщепление, если загрязнение уменьшилось меньше чем на гамму\n",
    "                      subsample=0.7,  # Доля наблюдений, попадающих в случайную подвыборку\n",
    "                      reg_lambda=0,  # Критерий качества, сумма квадратов, mse\n",
    "                      reg_alpha=1  # Критерий качества, сумма модулей, mae\n",
    "                      )\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "xgb.plot_importance(model)\n",
    "xgb.plot_importance(model, max_num_features=30)\n",
    "\n",
    "# В выборе параметров помогают grid_search и валидация\n",
    "# Оптимально перебирать параметры последовательно по одному (но наилучшее решение можно и не найти)\n",
    "\n",
    "smart_xgboost = GridSearchCV(cv=5,  # Параметр разделения для кросс-валидации (фолды)\n",
    "                             error_score='raise',\n",
    "                             estimator=XGBClassifier(  # Задаем оценку = XGBClassifier\n",
    "                                 base_score=0.5,  # Задаем параметры, которые не собираемся оптимизировать\n",
    "                                 colsample_bylevel=1,\n",
    "                                 colsample_bytree=0.8,\n",
    "                                 gamma=0,\n",
    "                                 max_delta_step=0,\n",
    "                                 missing=None,\n",
    "                                 nthread=-1,\n",
    "                                 # Критерий кач-ва при обучении. здесь 2, больше-softmax, sofprob\n",
    "                                 objective='binary:logistic',\n",
    "                                 num_class=3,  # Дополнительно задачем число классов в задаче\n",
    "                                 reg_alpha=0,\n",
    "                                 reg_lambda=1,\n",
    "                                 scale_pos_weight=1,\n",
    "                                 seed=1234,\n",
    "                                 silent=True,\n",
    "                                 subsample=0.8\n",
    "                             ),\n",
    "                             fit_params={},\n",
    "                             iid=True,\n",
    "                             n_jobs=-1,\n",
    "                             param_grid={  # Изменяемые параметры\n",
    "                                 'min_child_weight': [1, 3, 5],\n",
    "                                 'max_depth': [3, 5, 7]\n",
    "                                 'n_estimators': [100, 300, 500, 800, 1000],\n",
    "                                 'learning_rate': [0.05, 0.1, 0.3]\n",
    "                             },\n",
    "                             pre_dispatch='2*n_jobs',\n",
    "                             refit=True,\n",
    "                             scoring='accuracy',\n",
    "                             verbose=0\n",
    "                             )\n",
    "\n",
    "smart_xgboost.fit(X_train, y_train)\n",
    "\n",
    "sorted(smart_xgboost.cv_results_.keys())\n",
    "\n",
    "smart_xgboost.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi.sort_values('importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод градиентного спуска\n",
    "# Метод обратного распределения E = сумма(Yi-Vi)**2 позволяет через MSE находить ошибку и на ее основе исправлять веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нейронные сети. Deep Learning\n",
    "# Активационная функция = сумма(Wi*Xi) от числа входов нейрона.\n",
    "# Логистическая функция f(x)=e**x/(1+e**x) или гиперболический тангенс\n",
    "# ReLU функция f(x) = max(0, X) проще, но чуть менее точная и сложнее в добавлении параметров\n",
    "# Подбор архитектуры НС позволяет оптимизировать число нейронов, настроив входной, скрытые, выходной слои.\n",
    "# Сети прямого распространения: в пределах слоя нейроны не связаны, передают только в след. слой, перепрыгивать нельзя\n",
    "# Обучение НС = определение значений ВЕСОВ (внутренних параметров)  каждого соединения. Остальное задается аналитиком заранее\n",
    "# Keras модули: архитектура, входные значения, условия обучения, оценка качества\n",
    "\n",
    "wine['Desired1(3)'].value_counts(normalize=True)\n",
    "\n",
    "# Именуем предикторы и отклик\n",
    "y = wine['Desired1(3)']\n",
    "X = wine.drop('Desired1(3)', axis=1)\n",
    "\n",
    "# расщепление на выборки с указанием объёма тестового множества\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=12345, test_size=0.33)\n",
    "\n",
    "# Преобразование в np.array для Keras\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Поскольку больше двух классов и они не упорядочены, то разбиваем колонку \"y\" на три, с бинарными значениями\n",
    "y_train_bin = np_utils.to_categorical(y_train)\n",
    "y_test_bin = np_utils.to_categorical(y_test)\n",
    "y_train_bin[0:5]\n",
    "\n",
    "# Метод скорейшего(градиентного) спуска SGD. Улучшение метода: Momentum, Nesterov momentum, Adam и др.\n",
    "# Argmin, правило остановки: число итераций или малое уменьшение функции\n",
    "# Начальная точка(инициализация). Начальные значение д.б. минимальным, ближе к нулю (кроме свободных слагаемых)\n",
    "# График зависимости критерия каач-ва Q от номера итерации. Малая скорость обучения(0.001 и т.д.) дает качество, но идет дольше\n",
    "# Входные значения рекомендуется стандартизовать(снижает риск насыщения) (Xi-Xmin)/(Xmax-Xmin)\n",
    "# Batch - коррекция весов после каждой эпохи. Насыщение - отсутствие коррекций. Gradient clipping - блок от больших поправок\n",
    "\n",
    "# Создаем модели. Для небольших данных можно всего пару слоев в 5-7 нейронов\n",
    "# Инициализация. Присвоение стартовых весов\n",
    "# Зерно не указано, контроля над обучением меньше\n",
    "init = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "# Усеченное нормальное распределение. Инициация весов\n",
    "init_2 = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=12345)\n",
    "init_3 = initializers.Constant(value=1e-3)  # Инициация свободных членов\n",
    "\n",
    "model = Sequential()  # Указываем на тип модели (сеть прямого распространения)\n",
    "# Первый слой, 9 нейронов, входные значения (13 предикторов)\n",
    "model.add(Dense(9, input_dim=13, activation='relu'))\n",
    "model.add(Dense(10, activation='relu', ))  # Втоой слой, 10 нейронов\n",
    "# Третий слой, ранжировка софтмаксом для стандартизации и активации. Три выхода\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(9, input_dim=13, activation='relu'))\n",
    "model2.add(Dense(10, activation='relu'))\n",
    "# Софтмакс используется для распознавания n-классов\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(9, input_dim=13, activation='relu',\n",
    "                 kernel_initializer=init_2, bias_initializer=init_3))\n",
    "model3.add(Dense(10, activation='relu',\n",
    "                 kernel_initializer=init_2, bias_initializer=init_3))\n",
    "model3.add(Dense(3, activation='softmax',\n",
    "                 kernel_initializer=init_2, bias_initializer=init_3))\n",
    "\n",
    "# Categorical crossentropy (CC) используется для определения вероятности принадлежности объекта к классу (упорядочному)\n",
    "\n",
    "# Компилируем: optimizer(rmsprop или adam), loss function(categorical_crossentropy(классификация) или mse(регрессия)). Точность\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "sgd2 = optimizers.SGD(lr=0.02, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "               optimizer=sgd2, metrics=['accuracy'])\n",
    "\n",
    "sgd3 = optimizers.SGD(lr=0.02, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "               optimizer=sgd3, metrics=['accuracy'])\n",
    "\n",
    "# Обучаем модель: 300 эпох, пропуск 10 элементов до смены весов\n",
    "model.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "model2.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "model3.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "\n",
    "# Проверяем на тестовом множестве\n",
    "scores = model.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "scores2 = model2.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy2: %.2f%%\" % (scores2[1]*100))\n",
    "\n",
    "scores3 = model3.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy3: %.2f%%\" % (scores3[1]*100))\n",
    "\n",
    "\n",
    "# Рассчитываем предикторы\n",
    "predictions = model.predict(X_test)\n",
    "predictions2 = model2.predict(X_test)\n",
    "predictions3 = model3.predict(X_test)\n",
    "# round predictions\n",
    "#rounded = [round(x[0]) for x in predictions]\n",
    "# print(rounded)\n",
    "print(predictions[0:5])\n",
    "print(predictions2[0:5])\n",
    "print(predictions3[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нейронные сети: Прогнозирование (в надежде, что поведение ряда сохраняется). Аддитивная сезонность, логарифм не нужен\n",
    "# Уменьшение накапливаемости ошибки м.б. только с параллельным обучением нескольких сетей (и со снижением качества в целом)\n",
    "# Наиболее важны самые недавние (или самые высокоранжированные) наблюдения. Их оптимально ставить в тестовую\n",
    "# MSE, MAE или Mean absolute percentage error (MAPE) - показывают относительную ошибку в регрессии\n",
    "# MAPE = 1/n *сумма(Yi-Yпрогноз)/Yi*100%\n",
    "\n",
    "sales = pd.read_csv('monthly-car-sales-in-quebec-1960.csv',\n",
    "                    sep=';', header=0, parse_dates=[0])\n",
    "\n",
    "# Преобразуем данные\n",
    "sales_2 = pd.DataFrame()\n",
    "\n",
    "for i in range(12, 0, -1):  # Убрано накопление ошибок, поскольку нет предсказаний на основе предыдущих предсказаний\n",
    "    # Новые колонки, где значения сдвинуты с обратным временным шагом(помесячно)\n",
    "    sales_2['t-'+str(i)] = sales.iloc[:, 1].shift(i)\n",
    "\n",
    "sales_2['t'] = sales.iloc[:, 1].values  # Дублируем первоначальную колонку\n",
    "sales_4 = sales_2[12:]  # Отрезаем первые 12 строк\n",
    "\n",
    "# Задаем предикторы и отклик\n",
    "y = sales_4['t']\n",
    "X = sales_4.drop('t', axis=1)\n",
    "\n",
    "# Разделяем на обучающую и тестовую. Тестовая - последние наблюдения\n",
    "X_train = X[:91]\n",
    "y_train = y[:91]\n",
    "X_test = X[91:]\n",
    "y_test = y[91:]\n",
    "\n",
    "# Преобразование в np.array\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Создаем, компилируем и обучаем модель\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "# Линейная выходная ф-ция, чтобы сохранить линейную комбинацию\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',\n",
    "              metrics=['mean_absolute_percentage_error'])\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=None)\n",
    "\n",
    "# оценка качества модели на тестовом множестве\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nMAPE: %.2f%%\" % (scores[1]))\n",
    "\n",
    "# Вычисляем прогноз\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Вычисляем подгонку\n",
    "predictions_train = model.predict(X_train)\n",
    "\n",
    "# График с результатами numpy.arange(start, stop, step, dtype=None)\n",
    "x2 = np.arange(0, 91, 1)\n",
    "x3 = np.arange(91, 96, 1)\n",
    "\n",
    "plt.plot(x2, y_train, color='blue')\n",
    "plt.plot(x2, predictions_train, color='green')\n",
    "plt.plot(x3, y_test, color='blue')\n",
    "plt.plot(x3, predictions, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нейронные сети: прогнозирование логарифмов\n",
    "ser_g = pd.read_csv('series_g.csv', sep=';', header=0)\n",
    "\n",
    "# Мультипликативная сезонность. Потому добавляем логарифм, который превращает произведение в сумму\n",
    "ser_g['log_y'] = np.log10(ser_g['series_g'])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ser_g['series_g'].plot(ax=ax1)\n",
    "ax1.set_title(u'Объём пассажироперевозок')\n",
    "ax1.set_ylabel(u'Тысяч человек')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "pd.Series(ser_g['log_y']).plot(ax=ax2)\n",
    "ax2.set_title(u'log10 от объёма пассажироперевозок')\n",
    "ax2.set_ylabel(u'log10 от тысяч человек')\n",
    "\n",
    "# Данные лучше разбивать на два ряда: сезонность и все остальное (сглаженный ряд скользящего среднего)\n",
    "# Преобразуем данные: 12 предикторов(помесяцам), 1 отклик\n",
    "ser_g_2 = pd.DataFrame()\n",
    "for i in range(12, 0, -1):\n",
    "    ser_g_2['t-'+str(i)] = ser_g.iloc[:, 2].shift(i)\n",
    "ser_g_2['t'] = ser_g.iloc[:, 2].values\n",
    "\n",
    "# Отрезаем первые 12 строк\n",
    "ser_g_4 = ser_g_2[12:]\n",
    "\n",
    "# Задаем предикторы и отклик\n",
    "y = ser_g_4['t']\n",
    "X = ser_g_4.drop('t', axis=1)\n",
    "\n",
    "X_train = X[:120]\n",
    "y_train = y[:120]\n",
    "X_test = X[120:]\n",
    "y_test = y[120:]\n",
    "\n",
    "# numpy array\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Обучаем\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',\n",
    "              metrics=['mean_absolute_percentage_error'])\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=None)\n",
    "\n",
    "# оценка качества модели на тестовом\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nMAPE: %.2f%%\" % (scores[1]))\n",
    "\n",
    "# прогноз\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# подгонка\n",
    "predictions_train = model.predict(X_train)\n",
    "\n",
    "# Результаты\n",
    "x2 = np.arange(0, 120, 1)\n",
    "x3 = np.arange(120, 132, 1)\n",
    "\n",
    "# График позволяет увидеть, насколько модель уловила закономерности\n",
    "plt.plot(x2, y_train, color='blue')\n",
    "plt.plot(x2, predictions_train, color='green')\n",
    "plt.plot(x3, y_test, color='blue')\n",
    "plt.plot(x3, predictions, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning. Набор для тестирования MNIST(28x28 px, 256 grey).\n",
    "# Каскад Хаара(15 признаков, которые ищутся на частях картинки)\n",
    "# Сверточные сети создают аналогичные наборы признаков для решения целевых задач. Фиши пошагово усложняются\n",
    "# Stride - параметр сдвига по картинке\n",
    "# Pooling - уменьшение числа нейронов за счет повышения контрастности и сужения. Веса модифицируются\n",
    "# FC - преображение матрицы в вектор. Можно использовать уже готовую сеть и дообучить ее на своих данных\n",
    "# Dropout - борьба с переподгонкой и декорреляцией\n",
    "\n",
    "# Параметры сети\n",
    "batch_size = 128  # Объем данных для итерации\n",
    "nr_classes = 10  # Число классов (10 цифр)\n",
    "nr_iterations = 20  # Число эпох\n",
    "\n",
    "# Читаем данные (качаем MNIST)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Для сверточной сети картинку вытягиваем в столбец\n",
    "# 784 - это значения для каждого пикселя в картинке 28х28\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "# Уточняем тип данных\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Нормируем (станартизируем) входные значения (256 оттенков)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Делаем 10 бинарных столбцов (так как 10 цифр)\n",
    "Y_train = np_utils.to_categorical(y_train, nr_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nr_classes)\n",
    "\n",
    "#  Описываем сеть. Один внутренний слой\n",
    "model = Sequential()\n",
    "model.add(Dense(196, input_shape=(784,)))  # Число нейронов, число входов\n",
    "model.add(Activation('relu'))\n",
    "# Какая доля будет обучаться (половина нейронов будет спать)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))  # Выходной слой\n",
    "model.add(Activation('softmax'))  # Сумма всех чисел в итоге будет равна 1\n",
    "\n",
    "# Проверяем себя\n",
    "model.summary()\n",
    "\n",
    "# Определяем параметры обучения\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# для воспроизводимости сети. Зерно датчика уже настроено на оптимальный вариант\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Обучаем модель\n",
    "net_res_1 = model.fit(X_train, Y_train,\n",
    "                      batch_size=batch_size, epochs=nr_iterations,\n",
    "                      verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Deep Learning - 2. Набор для тестирования MNIST(28x28 px, 256 grey)\n",
    "# Чтобы определить оптимальное число слоев и нейронов нужно эксперементировать\n",
    "# Теорема существования - всегда можно создать аналог рабочей НС с одним слоем (но большим числом нейронов)\n",
    "# Augmentation - улучшение точности через преобразование(вращение, зеркальность и т.д.) картинок в обучающей выборке\n",
    "# Регуляризация по Тихонову - предотвратить подгонку, найти колинеарность, запретить слишком большие значения\n",
    "\n",
    "train = pd.read_csv(r\"C:\\Users\\Mr Alex\\Files\\FlightPreparens\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\Mr Alex\\Files\\FlightPreparens\\test.csv\")\n",
    "\n",
    "\n",
    "# Разделяем предикторы и отклик\n",
    "Y = train['label']  # Столбец с кодами(10 цифр)\n",
    "X = train.drop(['label'], axis=1)  # Цветовая гамма\n",
    "\n",
    "# Разделяем на обучающую выборку и выборку валидации (чтобы не обучаться на тестовой выборке)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    X.values, Y.values, test_size=0.10, random_state=42)\n",
    "\n",
    "# параметры сети, чтобы их было удобно менять. Batch Normalization\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "# размерность картинки\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# преобразование обучающей выборки\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "# преобразование выборки валидации\n",
    "x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
    "x_val = x_val.astype('float32')\n",
    "x_val /= 255\n",
    "\n",
    "# преобразование тестовой выборки\n",
    "Xtest = test.values\n",
    "Xtest = Xtest.reshape(Xtest.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# преобразование отклика в 10 бинарных перменных\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "# Обучение модели (4 слоя в два этапа)\n",
    "model = Sequential()\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n",
    "                 input_shape=input_shape))  # Первый сверточный слой\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # Второй сверточный слой\n",
    "# Pooling. Сокращаем размерность и контраст\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout. Четверь нейронов выбрасываем из обучения\n",
    "\n",
    "# Переходим на сеть прямого распространения\n",
    "model.add(Flatten())  # Преобразуем матрицу в вектор\n",
    "model.add(Dense(128, activation='relu'))  # Первый слой анализа\n",
    "model.add(Dropout(0.5))  # Dropout\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Второй слой анализа\n",
    "\n",
    "# Определяемся с обучением\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(x_val, y_val))\n",
    "accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test accuracy:', accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Факторный анализ\n",
    "# Сокращение переменных через введение новых, искусственных переменных (факторов), которые их заменяют\n",
    "# Способы поиска наилучших проекций: projection persuit, многомерное шкалирование,карты Sommer\n",
    "# Выявление структур взаимозависимости данных, матриц корреляции\n",
    "# Преодоление мультиколинеарности переменных в регрессионом анализе\n",
    "# R - матрица корелляции=k*k, где k - число столбцов исходной матрицы. Она же дисперсия вектора\n",
    "# U - (уникальности) особенности в данных, которые не удается объеснить факторами. Определяет качество модели\n",
    "\n",
    "# Удаляем столбцы, где данные непрочитались\n",
    "df = df.filter(regex='^(?!.*Unnamed).*$')\n",
    "\n",
    "# Смотрим коэффициенты корелляций. Мало больших значений - плохо для факторного анализа\n",
    "df.corr()\n",
    "\n",
    "# Строим R матрицу корелляций. Много выбросов, есть бимодальности. Но сильной корелляции увы нет\n",
    "scatter_matrix(df)  # Добавление \";\" позволяет показать только график, без цифр\n",
    "\n",
    "# Cтандартизируем переменные\n",
    "df_scaled = preprocessing.scale(df)\n",
    "\n",
    "# Методом поиска главных компонентов проецируем данные на двумерную плоскость и получаем ранжирование компонентов по важности\n",
    "# Уточняем число компонент и источник данных\n",
    "pca = PCA(n_components=5).fit(df_scaled)\n",
    "\n",
    "# Доля разброса в данных, объясняемая главными компонентами\n",
    "print('Влияние компонентов на общий разброс данных: ',\n",
    "      pca.explained_variance_ratio_)\n",
    "# Чистые значения главных компонент\n",
    "meanings = pca.singular_values_\n",
    "\n",
    "# Массив, в котором посчитаны значения факторов, заменяющие исходный набор данных\n",
    "pca_factor = pca.transform(df_scaled)\n",
    "\n",
    "# Запускаем факторный анализ. Сравним влияние факторов\n",
    "# Факторов задаем много, сокращаем пока не получим адекватную группировку\n",
    "fa = FactorAnalysis(n_components=2).fit(df_scaled)\n",
    "\n",
    "# Таблица коэффициентов корреляции - что именно измерили факторы. Семь параметров превращаются в два новых фактора\n",
    "# Обновление данных, где те же колонки, но переменные в них - факториалы\n",
    "pd.DataFrame(fa.components_, columns=df.columns)\n",
    "\n",
    "# Дисперсия остатков U. Видно, что хотя новые факторы очевидны, остатки огромные, то есть переменные объяснены плохо\n",
    "# Если некоторые переменные объясняются очень плохо, то они уникальны и преображать их в факторы не надо\n",
    "pd.Series(fa.noise_variance_, df.columns)\n",
    "\n",
    "# Значения факторов можно применить к конкретным объектам\n",
    "scores = pd.DataFrame(fa.transform(df_scaled), columns=['factor1', 'factor2'])\n",
    "scores['factor1'].sort_values()  # Сортируем по значению фактора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD-разложение Simon Funk'a для рекомендательных систем. Позволяет работать с данными, где много пропусков\n",
    "# Двойной факторный анализ, проведенный одновременно. Матрица триплетов получается из: номер стр, номер стлб., элемент.\n",
    "# Работает с субъективными оценками, поэтому проверяет ее с помощью средних по матрице, по строке, по столбцу\n",
    "# Не стоит минимизировать MSE - потому что нужны рекомендации только с большими значениями\n",
    "\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Создаем модель\n",
    "algo = SVD(n_factors=160,\n",
    "           n_epochs=100,\n",
    "           lr_all=0.005,  # скорость обучения, шаг модификации\n",
    "           reg_all=0.1,  # гамма регулирующая\n",
    "           biased=True,\n",
    "           random_state=42\n",
    "           )\n",
    "# Обучаем модель\n",
    "cross_validate(algo,\n",
    "               data,\n",
    "               measures=['RMSE', 'MAE'],\n",
    "               cv=5,\n",
    "               verbose=True\n",
    "               )\n",
    "\n",
    "# проверяем модель на случайном пользователе\n",
    "userid = str(196)\n",
    "itemid = str(302)\n",
    "actual_rating = 4\n",
    "print(algo.predict(userid, 302, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Калибровка классификаторов. Нужна для рабочих моментов, чтобы лучше понять машину\n",
    "# Уточнение выданных машиной значений, с учетом \"вероятности\" попасть в класс\n",
    "# Можно задать пороговые значения, которые определят класс. В промежутке между ними машина скажет \"не знаю\"\n",
    "# Калибровка - это пересчет выходных значений, чтобы про них м.б. сказать - это Вероятность\n",
    "# Изотоническая регрессия - это линия, у которой вектор не убывает\n",
    "# Логистическая кривая, метод Платта - неубывающая линия, которая приблизит Pi к P\n",
    "\n",
    "\n",
    "# Строим предсказание модели с возможностью посмотреть на вероятности принадлежать к классу\n",
    "y_pred_train2 = model.predict_proba(X_train)\n",
    "y_pred_test2 = model.predict_proba(X_test)\n",
    "\n",
    "# Завершаем построенную машину калибровкой\n",
    "model_sigmoid = CalibratedClassifierCV(model,\n",
    "                                       cv=2,\n",
    "                                       method='sigmoid'  # Или \"isotonic\"\n",
    "                                       )\n",
    "\n",
    "\n",
    "# Обучаем калибровку на выборке валидации\n",
    "model_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "# Смотрим на вероятности после калибровки\n",
    "model_sigmoid.predict_proba(X_test)\n",
    "\n",
    "# Предсказание класса для новых элементов\n",
    "new_item = [1, 1, 1, 1]\n",
    "model.predict([new_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление статистической погрешности для случайной выборки\n",
    "\n",
    "# Расчет объема выборки\n",
    "N = 40000  # Генеральная совокупность\n",
    "P = 0.95  # Доверительный уровень в 95%\n",
    "# коэффициент доверительного уровня (p = 95%, Z=1,96)(p=99%,   Z=2,58)\n",
    "Z = 1.96\n",
    "p = 0.5  # доля респондентов с  наличием исследуемого признака,\n",
    "q = (1 - p)  # доля респондентов, у которых исследуемый признак отсутствует,\n",
    "delta = 0.05  # Задаваемая предельная ошибка выборки.\n",
    "n = (Z**2)*p*q/delta**2  # объем выборки\n",
    "\n",
    "print(\"Рекомендуемый объем выборки для данной аудитории:\", int(n), \"человек\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет ошибки выбоки для доли признака\n",
    "# Случай 1. Генеральная совокупность значительно больше выборки\n",
    "n = 384  # Объем выборки\n",
    "m = 276  # Число объектов выборки с нужными параметрами (True)\n",
    "p = m/n  # Вероятность на основе практических данных\n",
    "sigma = n/2*((p*(1-p)/n*(1-n/N)))**0.5\n",
    "print('Результат выборки один составит: ',\n",
    "      float(\"{0:.1f}\".format(p*100)), \"±\", float(\"{0:.1f}\".format(sigma)), \"%\")\n",
    "\n",
    "# Случай 2. Генеральная совокупность сопоставима с объемом выборки\n",
    "N = 2500\n",
    "delta = Z*((p*q/n)*((N-n)/(N-1)))**0.5\n",
    "print(\"Точность результатов выборки два составит: \",\n",
    "      \"±\", float(\"{0:.1f}\".format(delta*100)), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассчет доверительного интервала\n",
    "P = 0.99  # Доверительный уровень в 99%\n",
    "Z = 2.58  # коэффициент доверительного уровня\n",
    "p = 0.2  # доля респондентов с наличием исследуемого признака,\n",
    "q = (1 - p)  # доля респондентов, у которых исследуемый признак отсутствует,\n",
    "n = 1000  # Объем выборки\n",
    "\n",
    "sigma = Z*(p*q/n)**0.5  # Погрешность оценки\n",
    "\n",
    "print('Точность результатов конкретной выборки составит: ±',\n",
    "      float(\"{0:.2f}\".format(sigma*100)), \"%\")\n",
    "print('Доверительный интервал составит:', float(\"{0:.2f}\".format((p - sigma)*100)), \"% ;\",\n",
    "      float(\"{0:.2f}\".format((p + sigma)*100)), \"%\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "628px",
    "left": "19px",
    "right": "20px",
    "top": "283px",
    "width": "356px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
