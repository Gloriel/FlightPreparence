{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pylab import plot,show,hist\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "from scipy.stats import norm, chi2_contingency\n",
    "import statsmodels.api as sm\n",
    "from numpy import linspace,hstack\n",
    "from pylab import plot,show,hist\n",
    "import pydot\n",
    "#%config InlineBackend.figure_format = 'svg' для большей четкости графиков\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "#Стандартизация данных\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Для построения диаграмм рассеивания\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Иерархический кластерный анализ\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "#Кластерный анализ методом К-средних\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Линейная регрессия\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Расщепление на обучающую и тестовую выборки\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Деревья решений для задачи классификации\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Деревья решений для задач регрессии \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#XGBoost\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "#Нейронные сети\n",
    "from keras.models import Sequential #тип сети\n",
    "from keras.layers import Dense #метод соединения слоев\n",
    "from keras.utils import np_utils #обработка данных под Керас\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "           'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "os.chdir(r'C:\\Users\\Mr Alex\\Documents\\GitHub\\FlightPreparence')\n",
    "data = pd.read_csv('AmesHousing.txt', sep=\"\\t\", header = 0, index_col=False)\n",
    "town = pd.read_csv('town_1959_2.csv', header = 0,)\n",
    "df = pd.read_csv('swiss_bank_notes.csv', index_col=0)\n",
    "beer = pd.read_csv('beverage_r.csv', sep=\";\", index_col='numb.obs')\n",
    "food = pd.read_csv('Protein Consumption in Europe.csv', sep=';', decimal=',', index_col='Country')\n",
    "ass = pd.read_csv('assess.dat', sep='\\t', index_col='NAME')\n",
    "albi = pd.read_csv('Albuquerque Home Prices_data.txt', sep='\\t')\n",
    "noble = pd.read_csv('agedeath.dat.txt', sep='\\s+', header=None, names=['group', 'age', 'index'])\n",
    "inter = pd.read_csv('interference.csv')\n",
    "diamond = pd.read_csv('diamond.dat', header=None, sep='\\s+', names=['weight', 'price'])\n",
    "cred = pd.read_csv('Credit.csv', sep=';', encoding='cp1251')\n",
    "test = pd.read_csv('adult.test', header=None, names=columns, na_values=' ?', skiprows=1)\n",
    "wine = pd.read_csv('Wine.txt', sep='\\t', header=0)\n",
    "sales = pd.read_csv('monthly-car-sales-in-quebec-1960.csv', sep=';', header=0, parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_histograms(x, y):\n",
    "    \"\"\"\n",
    "    Функция, которая построит две гистограммы на одной картинке.\n",
    "    Дополнительно пунктирными линиями указываются средние значения выборок.\n",
    "    x: вектор pd.Series,\n",
    "    y: вектор pd.Series\n",
    "    \"\"\"\n",
    "    x.hist(alpha=0.5, weights=[1./len(x)]*len(x))\n",
    "    y.hist(alpha=0.5, weights=[1./len(y)]*len(y))\n",
    "    plt.axvline(x.mean(), color='red', alpha=0.8, linestyle='dashed')\n",
    "    plt.axvline(y.mean(), color='blue', alpha=0.8, linestyle='dashed')\n",
    "    plt.legend([x.name, y.name])\n",
    "    \n",
    "def regression_coef(model, X, y):\n",
    "    \"\"\"\n",
    "    Функция для определения статистической значимости регрессионных коэффициентов\n",
    "    \"\"\"\n",
    "    coef = pd.DataFrame(zip(['intercept'] + X.columns.tolist(), [model.intercept_] + model.coef_.tolist()),\n",
    "                    columns=['predictor', 'coef'])\n",
    "    X1 = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "    b = np.append(model.intercept_, model.coef_)\n",
    "    MSE = np.sum((model.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n",
    "    var_b = MSE * (np.linalg.inv(np.dot(X1.T, X1)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)\n",
    "    t = b / sd_b\n",
    "    coef['pvalue'] = [2 * (1 - stats.t.cdf(np.abs(i), (len(X1) - 1))) for i in t]\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Классификация - приписание объекта к классу на основании ключевой (группирующей) переменной или совокупности его характеристик\n",
    "#Типы переменных. Количественные(непрерывные, дискретные). Номинальные (несравниваемые). Ранговые (порядковые)\n",
    "#Гистограмма частот - форма распределения количественного признака\n",
    "#Описательная статистика. Меры центральной тенденции. Меры изменчивости (Размах - Xmax-Xmin)\n",
    "#МЦТ. Мода - самый частый признак. Медиана - делит упорядоченное множество пополам. Среднее (Математическое ожидание, EX)\n",
    "#Дисперсия D - средний квадрат отклонений индивидуальных значений от средней величины. С ростом n, дисперсия сокращается\n",
    "#D = сумма(Xинд - Xсред)**2/n-1. Хсред генеральной совокупности обозначается как мю, М\n",
    "#Стандартное отклонение, \"сигма\", sd = D**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Нормальное распределение. Унимодально и симметрично \n",
    "#Центральная предельная теорема. Для выборок стандартная ошибка среднего se=SDинд/n**0.5, где n - число элементов выборки\n",
    "#Если n выборка репрезентативная и число элементов > 30, то se=0.5\n",
    "#Интервал для поиска М генеральной совокупности(доверительный интервал): для 95% выборок Хсред ± 1.96*se включат в себя М"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Число наблюдений(N1), попавших в столбец. H = C*N1 \n",
    "#H = N1/(N*длина интервала) - в таком случае гистограмма будет вероятностной, то есть в пределах единицы\n",
    "#Плотность распределения f(x) позволяет рассчитать вероятность P(A) попаданий в определенный интервал\n",
    "#В гистограмме наибольший вес имеет площадь столбца\n",
    "data['SalePrice'].hist(bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Гистограмма нужна чтобы оценить одну группу\n",
    "#Вероятностная гистограмма \n",
    "data['SalePrice'].hist(density=True, bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ядерная оценка плотности Скотта-Сильвермана - обобщение гистограммы F(t) = (1/n*h)*сумма всех наблюдений K(t-Xi/h)\n",
    "#Распределение Японечникова определяет плотность К - симметричная, неотрицательная, с интегралом=1\n",
    "my_density = gaussian_kde(data['SalePrice'], bw_method = 1) #Метод определяет меру сглаживания\n",
    "x = linspace(min(data['SalePrice']), max(data['SalePrice']),1000)\n",
    "plot(x, my_density(x),'g') #распределение функции\n",
    "hist(data['SalePrice'], density=True, alpha=.3) \n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для сравнения нескольких групп можно сложить гистограммы \n",
    "df.groupby('Status')['Length'].plot.hist(alpha=.6)\n",
    "plt.legend()\n",
    "#Но лучше использовать box-plot. Усы - 1,5 межквартиля. Outlies - 3. Extremes - дальше.\n",
    "#Внимательно смотрим на число наблюдений в каждой из выборок, чтобы проанализировать boxplot\n",
    "data['MS Zoning'].value_counts()\n",
    "ax=data.boxplot(column='SalePrice', by='MS Zoning')\n",
    "ax.get_figure().suptitle('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для определения \"типичного\" объекта совокупности можно использовать среднее(если нет выбросов) или медиану(если есть)\n",
    "#При неравномерном распределении можно убрать выбросы\n",
    "town_2 = town.iloc[2:1004]\n",
    "#Или логарифмировать переменную (для лог-нормального распределения)\n",
    "x = np.log10(town[u'население'])\n",
    "pd.Series(x).hist(bins=45)\n",
    "#Усеченное среднее. Выбрасывается 2,5% самых малых и 2,5% наибольших значений переменной. Для новой БД считается среднее\n",
    "exclude = int(len(town)/100*2.5)\n",
    "redacted_town = town[exclude:len(town)-exclude]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Для получения комплексного сравнения объектов по нескольким переменным\n",
    "#Диагональ показывает ядерную оценку плотности\n",
    "#Матрица состоит из диаграмм рассеивания\n",
    "colors = {'genuine': 'green', 'counterfeit': 'red'}\n",
    "scatter_matrix(df,\n",
    "               # размер картинки\n",
    "               figsize=(6, 6),\n",
    "               # плотность вместо гистограммы на диагонали\n",
    "               diagonal='kde',\n",
    "               # цвета классов\n",
    "               c=df['Status'].replace(colors),\n",
    "               # степень прозрачности точек\n",
    "               alpha=0.2,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#В случае очевидного смешения двух нормальных распределений, можно оценить их более подробно\n",
    "df.groupby('Status')['Diagonal'].plot.hist(alpha=0.6)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#Для сравнения параметров важно их нормализовать\n",
    "data.groupby('MS Zoning')['SalePrice'].plot.hist(density=True)\n",
    "plt.legend()\n",
    "\n",
    "#Если рассевание нельзя разделить линейно, то меняем точку начала координат и выбираем новые параметры для разделения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Иерархический кластерный анализ разделяет объекты на группы (стратификация). Число групп заранее неизвестно\n",
    "#Кластерный анализ позволяет сократить число наблюдений и проинтерпретировать их\n",
    "#Схожесть внутри кластера отображается как расстоянием между близкими объектами на диаграмме кластеров\n",
    "#Расстояние можно рассчитать методами: Евклида(или квадрата Евклида), Блок(Манхеттен), Хэмминга(для слов) и тд.\n",
    "#Манхеттен предпочтительнее, когда нет больших различий в рандомных переменных, потому что вес аномалий тогда меньше\n",
    "#Расстояние между кластерами рассчитывается:\n",
    "#Метод Варда (WARD) - позволяет работать с шаровыми скоплениями\n",
    "#Метод ближайших соседей (позволяет определять ленточные кластеры)\n",
    "#Средневзвешенное расстояние: среднее для суммы всех расстояний (также для ленточных)\n",
    "#Центроид: расстояние между кластерами равно расстоянию между их центрами тяжести\n",
    "#Методы дальнего и ближайшего соседа: расстояние между самыми дальними\\близкими объектами есть межкластер\n",
    "#Метод расстояния Sorencen-Dice Q = 2*|A^B|/|A|+|B|. Не работает если множества слабо пересекаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Алгоритмы кластерного анализа. Объекты группируются с ближайшими, пока нет скачка в расстояниях для следующего слияния\n",
    "#Момент для прекращения слияния определяется дендрограммой (для умеренного числа объектов)\n",
    "#Каменистая осыпь/локоть показывают скачок (резкий взлет графика) шагов объединений, когда кластеризуются тысячи объектов\n",
    "#Задача аналитика: отобрать переменные, выбрать метод стандартизации, установить расстояние между кластерами и между объектами "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Объект, в котором будет хранится информация о последовательном слиянии кластеров\n",
    "#Для функции нужен фрейм, метод межкластера и метод межобъектов\n",
    "link = linkage(beer, 'ward', 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link - матрица (n-1) x 4, где n - число наблюдений. \n",
    "#Каждая строка - результат слияния очередной пары кластеров с номерами link[i, 0] и link[i, 1]. \n",
    "#Новому кластеру присваивается номер n + i \n",
    "#link[i, 2] означает расстояние между слитыми кластерами, а link[i, 3] - размер нового кластера.\n",
    "link[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Построение дендрограммы\n",
    "dn = dendrogram(link, orientation='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ключевые характеристики кластеров\n",
    "#В колонку cluster запишем номер кластера объекта с помощью функции fcluster. \n",
    "#Аргументы: linkage, пороговое значение для межкластера (либо число кластеров), criterion: distance для остановки разбиения \n",
    "# Останавливаем объединение, если расстояние между кластерами превышает 3\n",
    "beer['cluster'] = fcluster(link, 3, criterion='distance')\n",
    "#Доля объектов в кластере, которые имеют соответствующие характеристики\n",
    "beer.groupby(\"cluster\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Кластерный анализ методом К-средних\n",
    "#инициализация модели\n",
    "model = KMeans(n_clusters=2, random_state=42) #random_state - зерно датчика случайных чисел. Для воспроизводимости результата \n",
    "#При каждом новом вызове с одинаковыми random_state модель будет давать одинаковые результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#подгонка модели по данным из БД\n",
    "model.fit(beer)\n",
    "\n",
    "#Результат кластеризации на данных из БД\n",
    "model.labels_\n",
    "\n",
    "#координаты центров кластеров\n",
    "model.cluster_centers_\n",
    "\n",
    "#Добавление в кластер данных. Предсказание для новых наблюдений. Метод predict\n",
    "new_items = [\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "model.predict(new_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Число кластеров можно определить через график локтя для для разного числа кластеров\n",
    "#Метод inertia_ вернёт сумму расстояний от каждой точки данных до центра ближайшего у ней кластера \n",
    "#Кластеризацию можно считать условно хорошей, когда инерция перестаёт сильно уменьшаться при увеличении числа кластеров\n",
    "K = range(1, 11)\n",
    "models = [KMeans(n_clusters=k, random_state=42).fit(beer) for k in K]\n",
    "dist = [model.inertia_ for model in models]\n",
    "\n",
    "#График локтя\n",
    "plt.plot(K, dist, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of distances')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В колонке NR находится номер объекта, его нужно исключить из данных для кластеризации\n",
    "del ass['NR']\n",
    "\n",
    "#Оптимизируем модель, меняя число задаваемых кластеров на основании графика локтя\n",
    "model = KMeans(n_clusters=4, random_state=42)\n",
    "model.fit(ass)\n",
    "ass['cluster'] = model.labels_\n",
    "ass.groupby('cluster').mean()\n",
    "\n",
    "#Смотрим к какому кластеру какие объекты относятся\n",
    "ass['cluster'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверка статистических гипотез:\n",
    "#Гипотеза согласия. Совпадает рандомная функция распределения с нормальным распределением? Самый дешевый и простой вариант\n",
    "#Гипотеза согласия2. Гипотеза об экспоненциальности распределения. Нужна, когда есть переменная времени ожидания\n",
    "#Гипотеза однородности. Совпадают две рандомные функции распредления? Например, чтобы сравнить данные до и после события\n",
    "#Гипотеза независимости. Нулевая гипотеза для рандомных объектов. Проверяется через коэффициент корреляции (скаляры) \n",
    "#Гипотеза о параметре распределения. Определение ключевых параметров. Например одинаковые средние или медианы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Альфа-это уровень значимости(0.05, 0.01. 0.005). Определеяет число ошибок первого рода. На второго рода влияет размер выборки\n",
    "#Т- это статистика критерия. Если T<Cальфа, то верна нулевая гипотеза\n",
    "#Cальфа- это критическое значение. Вероятность отвергнуть правильную гипотезу(T>C) не должна превышать А(альфа)\n",
    "#p-value показывает насколько часто статистика критерия в верной гипотезе будет превышать реальные значения p=P{T>Tэксп}\n",
    "#Если p<A, гипотезу отвергаем. Если p>A, гипотезу не отвергаем. Проверяются все условия, при которых критерий будет работать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Тесты Колмогорова-Смирнова и Shapiro-Wilk позволяют проверить выборку на принадлежность к ГС и нормальность распредеелния\n",
    "\n",
    "#Применяем критерий Шапиро-Вилка после логарифмирования. \n",
    "town = town.set_index(u'номер')\n",
    "plt.hist(np.log10(town[u'население']), bins=50)\n",
    "res = stats.shapiro(np.log10(town[u'население']))\n",
    "print('p-value: ', res[1])\n",
    "#P очень маленькое, поэтому гипотезу о нормальности отвергаем. \n",
    "#Отклонения от нормальности будут несущественны, если убрать выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Тест на гипотезу однородности\n",
    "#За один вариант дизайна выказалось 28 из 100 опрошенных, за второй 20 из 100 опрошенных. \n",
    "#Проверяем, является ли эта разница статистически значимой с помощью критерия хи-квадрат. \n",
    "\n",
    "#Cтроим таблицу сопряжённости.\n",
    "contingency_table = pd.DataFrame([[28, 72], [20, 80]],\n",
    "                                 index=['first', 'second'],\n",
    "                                 columns=['for', 'against'])\n",
    "\n",
    "res = stats.chi2_contingency(contingency_table) #AB-тест. Проверка разных вариантах на схожих выборках\n",
    "print('p-value: {0}'.format(res[1]))\n",
    "\n",
    "#p-value получился достаточно большим, поэтому оснований отвергнуть гипотезу о равенстве долей нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z-метка (организация выборок так, чтобы они мало отличались от нормального распределения)\n",
    "\n",
    "s1 = 135       # успех в выборке А\n",
    "n1 = 1781      # выборка А\n",
    "s2 = 47        # успех в выборке Б\n",
    "n2 = 1443      # выборка Б\n",
    "p1 = s1/n1               #  оценка вероятности успеха выборка А\n",
    "p2 = s2/n2               #  оценка вероятности успеха выборка Б\n",
    "p = (s1 + s2)/(n1+n2)    #  оценка вероятности успеха выборки А+Б\n",
    "z = (p2-p1)/ ((p*(1-p)*((1/n1)+(1/n2)))**0.5) #Z-метка \n",
    "\n",
    "p_value = norm.cdf(z) #Функция распределения нормального распределения\n",
    "\n",
    "#  z-метка и p-значение\n",
    "print(['{:.12f}'.format(a) for a in (abs(z), p_value * 2)])\n",
    "#Нулевая гипотеза отвергнута, статистические доли отличаются\n",
    "\n",
    "#То же самое, но со встроенным методом библиотеки statsmodels\n",
    "z1, p_value1 = sm.stats.proportions_ztest([s1, s2], [n1, n2])\n",
    "print(['{:.12f}'.format(b) for b in (z1, p_value1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Тест Стьюдента на независимость переменных\n",
    "x = noble[noble['group'] == 'sovr']['age']\n",
    "y = noble[noble['group'] == 'aris']['age']\n",
    "x.name, y.name = 'sovr', 'aris'\n",
    "two_histograms(x, y) #Данные условно нормальны. \n",
    "\n",
    "#Проверим c помощью критерия Флигнера-Килина, равны ли дисперсии.\n",
    "res = stats.fligner(x, y)\n",
    "print('p-value: ', res[1]) #p-value низкое, гипотезу о равенстве дисперсий отвергаем, наблюдаемые объекты несвязные \n",
    "\n",
    "#Гипотезу о равенстве средних значений будем проверять с помощью теста Стьюдента при неравных дисперсиях\n",
    "res = stats.ttest_ind(x, y, equal_var=False) #Опция equal_var=False говорит, что равенство дисперсии не предполагать\n",
    "print('p-value: ', res[1]) #P-значение значительно меньше альфы, гипотеза о равенстве отвергается\n",
    "\n",
    "#Ищем зависимость цены от переменной COR=1 и 0. Чтобы применить Стьюдента, проверим нормальность данных и равенство дисперсий\n",
    "#Заменяем -9999 (здесь=пустое) на корректное пустое значение.\n",
    "albi = albi.replace(-9999, np.nan)\n",
    "#Сохраним в отдельные переменные выборки, которые собираемся сравнивать.\n",
    "x = albi[albi['COR'] == 1]['PRICE']\n",
    "y = albi[albi['COR'] == 0]['PRICE']\n",
    "x.name, y.name = 'corner', 'not corner'    \n",
    "\n",
    "two_histograms(x, y)  #Видно, что выбросы не дают применить Стюдента и нужно пробовать Манна-Витни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Распределение Стьюдента (t-distribution) для n<30 - более высокие хвосты распределений.Число степеней свободы df=n-1\n",
    "#t заменяет Z в распределении Стьюдента. t=(Xинд-M)/(sd/n**0.5)\n",
    "#Помимо средних также нужно сравнить дисперсии D (тест Флигнера-Килина) и медианы (много n - тест Муда, мало n - Манн-Витни) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Парный t-критерий Стьюдента.  X1сред - Х2сред = А , se=((sd1**2/n1)+(sd2**2/n2))**0.5 , df=n1+n2-2\n",
    "#При t = A/se и df можно рассчитать p при котором M1-M2=0. То есть разницы между выборками почти нет\n",
    "#Q-Q Plot показывает насколько выборочные значения соответствуют предсказанным(из нормального распределеня)\n",
    "x = inter['DiffCol']\n",
    "y = inter['Black']\n",
    "x.name, y.name = 'DiffCol', 'Black'\n",
    "two_histograms(x, y)\n",
    "\n",
    "#Распределения условно нормальны. Поскольку в наблюдениях содержатся одни и те же люди, выборки связные (парные)\n",
    "res = stats.ttest_rel(x, y) #Метод для парных выборок\n",
    "print('p-value: ', res[1])\n",
    "p-value: 0.0162416779538\n",
    "#p-value низкий, гипотеза на уровне значимости 0.05 будет отвергнута, но на уровне 0.01 уже нет. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U-критерий Манна-Витни переводит значения в ранговую (непараметрическую) шкалу и проверяет НЕ равенство медиан. P{X>Y}=P{X<Y}\n",
    "\n",
    "res = stats.mannwhitneyu(x, y)\n",
    "print('p-value:', res[1])\n",
    "#p-value получилось достаточно большим, поэтому у нас нет оснований отвергнуть гипотезу. Разница медиан в выборках случайна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Корреляция. Scatter-plot или диагратта рассеивания\n",
    "#Сила и направление взаимосвязи определяется ковариацией. cov=Сумма((Xi-Xсред)*(Yi-Yсред))/N-1\n",
    "#Коэффициент корреляции Пирсона находится в промежутке [-1; 1] и считается как Rxy=cov/SDx*SDy\n",
    "#Коэффициент детерминации r**2 показывает влияние дисперсии одной переменной на другую в промежутке [0; 1]\n",
    "#Коэффициент Спирмена позволяет блокировать выбросы через ранги. d=X-Y. Rs=1-6*сумма d**2/N(N**2-1)\n",
    "#Часто корреляция обусловлена скрытой переменной\n",
    "\n",
    "#Корреляция цены и размера\n",
    "plt.scatter(albi['PRICE'], albi['SQFT'])\n",
    "\n",
    "res = stats.pearsonr(albi['PRICE'], albi['SQFT']) #Допускаем что коэфффициент корреляции=0, но гипотеза отвергнута\n",
    "\n",
    "print('Pearson rho: ', res[0])\n",
    "print('p-value: ', res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Стандартизация позволяет сделать вес важных переменных соизмеримым. Min=0(-1), max=1. ИЛИ Z\n",
    "#Z-Стандартизация: преобразование в тип, где М=0, sd = 1. Правило одной, двух и трех \"сигм\"\n",
    "#Z=(Xинд-М)/sd Пример: по таблице Z, где Хсред=150, sd=8, превышать Xинд будет 0.5z или 30%\n",
    "#Z=(Xсред-M)/se =(18,5-20)/0.5 = -3. Вероятность получить такой результат p = 0.0027\n",
    "\n",
    "#Если в БД нет единой метрики, то стандартизируем данные\n",
    "norm = preprocessing.StandardScaler()\n",
    "norm.fit(df)\n",
    "X = norm.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Дисперсионный Анализ. Если межгрупповой показатель изменчивости сильно превышает внутригрупповой, то средние разнятся\n",
    "#SST - общая сумма квадратов показывает общую изменчивость данных. Сумма(Xинд-Xсред)**2  SST = SSW+SSB\n",
    "#SSW - сумма квадратов внутригрупповая. Сумма(X1инд-Х1сред)**2 + ...(XNинд-ХNсред)**2\n",
    "#SSB - сумма квадратов межгрупповая. SSB= n(X1сред - Хсред)**2 + ...n(XNсред-Хсред)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Распределение Фишера, F-значение. F=(ssb/n-1)/(ssw/N-n). При верности нулевой гипотезы значения F очень маленькие\n",
    "#Поправка Бонферрони на множественную проверку гипотез. a = ai/n  НО: мешает получить значимые уровни различия\n",
    "#FDR или критерий Тьюки считает p-уровень для сравниваемых пар Xтэ=Xa-Xб\n",
    "#Двухакторный дисперсионный анализ SStotal=SSW+SSBa +SSBb + SSBa*SSBb\n",
    "#Взаимодействие факторов в ANOVA\n",
    "#Дисперсионный анализ требует нормальности распределения зависимой переменной и гомогенности дисперсии(тест Левена)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Регрессионнный Анализ позволяет исследовать взаимосвязи переменных и делать линию тренда\n",
    "#Простая Линейная Регрессия. Взаимосвязь 2-х переменных. Y-зависимая(отклик) Х-независимая(предиктор) \n",
    "#Y=B0(intercept)+B1(slope). Зачение Y, где линия пересекает ось, угол наклона линии к оси X\n",
    "#Метод наименьших квадратов(МНК) находит оптимальные параметры B0 и B1, чтобы сумма квадратов остатков (SE) была минимальна MSE\n",
    "#Уравнение регрессии Y=B0+B1*X1\n",
    "#B1 = SDy/SDx*Rxy, B0 = (Yсред-B1*Xсред), t = B1/se, df=N-2 Если B1 близка к нулю, то взаимосвязи почти нет\n",
    "#Коэффтцтент Детерминации (выборочная дисперсия) R указывает какой процент вариации отклика определяется влиянием предиктора\n",
    "#R**2 = 1-(SSres/SStotal) доля дисперсии Y, объясняемая регрессионной моделью. Чем больше R , тем лучше\n",
    "#Требования: линейная вхаимосвязь X Y, нормальное распределение остатков, гомоскедатичность(изменчивость) остатков\n",
    "#Избежать ошибок спецификации при линейной регрессии помогает Анализ Остатков. Выявлять колинеарность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albi = albi.replace(-9999, np.nan)\n",
    "print('Rows in the data frame: {0}'.format(len(albi)))\n",
    "print('Rows without NAN: {0}'.format(len(albi.dropna(how='any'))))\n",
    "\n",
    "#Слишком много данных содержат хотя бы одно пропущенное значение, чтобы удалить их все. Смотрим их распределение по колонкам\n",
    "#Функция .apply для всей матрицы. 1й аргумент-применяемая функция, 2й - направление применения (0 к колонкам, 1 ко строкам)\n",
    "albi.apply(lambda x: sum(x.isnull()), axis=0)\n",
    "\n",
    "#Если непоправимо мало данных, удаляем колонку\n",
    "del albi['AGE']\n",
    "del albi['TAX']\n",
    "\n",
    "#Анализируем колонку где можно заменить пропуски\n",
    "#albi['TAX'].hist()\n",
    "\n",
    "#Меняем пропущенные значения на среднее значение по колонке \n",
    "#albi['TAX'] = albi['TAX'].fillna(albi['TAX'].mean())\n",
    "\n",
    "#Строим модель линейной регрессии\n",
    "X = albi.drop('PRICE', axis=1)\n",
    "y = albi['PRICE']\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "#Считаем качество модели (коэффициент  𝑅**2 )\n",
    "print('R^2: {0}'.format(model.score(X, y)))\n",
    "\n",
    "#Промежуточные Регрессионные Коэффициенты от метода model.coef_ и свободный член от метода model.intercept_\n",
    "coef = pd.DataFrame(zip(['intercept'] + X.columns.tolist(), [model.intercept_] + model.coef_.tolist()),\n",
    "                    columns=['predictor', 'coef'])\n",
    "\n",
    "#Матрица показывает базовую цену и вес коэффициентов: 83.17 + 0.29*площадь SQFT + 12.17*удобства и т.д.\n",
    "#Логика показывает, что что-то не то. Проверяем на колинеарность\n",
    "albi.corr()\n",
    "\n",
    "#Видим, что колинеарен TAX. Убираем и снова считаем, в этот раз с p-значением\n",
    "regression_coef(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Полиномиальная регрессия\n",
    "#Предсказать результат не только с помощью переменной (1я модель), но и её квадрата(2я модель) и их обеих (3я модель) \n",
    "#Класс PolynomialFeatures, метод fit_transform сгенерирует из множества фич множество одночленов заданной степени \n",
    "#Например, для степени 2 и фич a, b будут сгенерированы фичи [a, b, a**2, b**2, ab] \n",
    "#при указанном параметре include_bias=True ещё и вектор-свободный член из единиц. \n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "df = pd.read_csv('diamond.dat', header=None, sep='\\s+', names=['weight', 'price'])\n",
    "\n",
    "poly = PolynomialFeatures(\n",
    "                          # Максимальная степень\n",
    "                          degree=2,\n",
    "                          # Не генерировать свободный член\n",
    "                          include_bias=False)\n",
    "y = df['price']\n",
    "X0 = poly.fit_transform(df[['weight']])\n",
    "X0 = pd.DataFrame(X0, columns=['weight', 'weight**2'])\n",
    "\n",
    "X0 = [\n",
    "    # Одна оригинальная переменная weight\n",
    "    X0[['weight']],\n",
    "    # Одна переменная weight**2\n",
    "    X0[['weight**2']],\n",
    "    # Две переменных weight и weight**2\n",
    "    X0.copy()]\n",
    "models = [LinearRegression() for _ in X0]\n",
    "\n",
    "for X, model in zip(X0, models):\n",
    "    model.fit(X, y)\n",
    "    print(model.score(X, y))\n",
    "    \n",
    "#𝑅**2  во всех моделях очень большой и примерно одинаков. Но на самом деле модели различны. Проверим их более тщательно\n",
    "\n",
    "regression_coef(models[0], X0[0], y)\n",
    "regression_coef(models[1], X0[1], y)\n",
    "regression_coef(models[2], X0[2], y)\n",
    "\n",
    "#Коэффициенты показывают спорные моменты в 1 и 3 моделях. 3-я ошибается из-за колинеарности (ложной)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "X2 = sm.add_constant(X0[2])\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Прогнозирование временных рядов\n",
    "#Линейная регрессия плохой метод, но безалтернативен при коротких временных рядах или двух или более факторах сезонности\n",
    "df = pd.read_csv('series_g.csv', sep=';')\n",
    "\n",
    "# Преобразуем строчки с датами в объект datetime\n",
    "df['date'] = pd.to_datetime(df['date'], format='%b %Y') # format показывает что читаем: '%b %Y' трехбуквенный месяц, затем год \n",
    "\n",
    "#Построим график проверить тип тренда (линейный или нет), тип сезонности (аддитивный или мультипликативный), его длину, выбросы\n",
    "#Видим линейный тренд и мультипликативную сезонность. Это подтверждается после логирафмирование цикла \n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "df['series_g'].plot(ax=ax1)\n",
    "ax1.set_title(u'Объём пассажироперевозок')\n",
    "ax1.set_ylabel(u'Тысяч человек')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "pd.Series(np.log10(df['series_g'])).plot(ax=ax2)\n",
    "ax2.set_title(u'log10 от объёма пассажироперевозок')\n",
    "ax2.set_ylabel(u'log10 от тысяч человек')\n",
    "\n",
    "#Вывод: будем строить модель линейной регрессии для приближения логарифма от объёма перевозок. \n",
    "# log𝑦𝑖=𝛽𝑥𝑖+𝑐(𝑥𝑖)+𝜀𝑖, где  𝑦𝑖 объём перевозок,  𝑥𝑖 порядковый номер месяца,  𝑐(𝑥𝑖) сезонная составляющая,  𝜀𝑖  случайный шум\n",
    "#Создадим новый объект класса DateTimeIndex для 12 новых дат (месяцев) с помощью функции pd.date_range. \n",
    "# Создаём последовательсть месяцев. freq='MS' означает первое число каждого месяца из указанного диапазона \n",
    "new_dates = pd.date_range('1961-01-01', '1961-12-01', freq='MS')\n",
    "\n",
    "# Приводим df['date'] к типу Index, объединяем с 12 месяцами, полученными на предыдущем шаге\n",
    "new_dates = pd.Index(df['date']) | new_dates\n",
    "\n",
    "# Создаём датафрейм из одной колонки с расширенным набором дат\n",
    "df2 = pd.DataFrame({'date': new_dates})\n",
    "# Объединяем два датафрейма по колонке 'date'.\n",
    "df = pd.merge(df, df2, on='date', how='right') #Склеиваем по указанной колонке (on) и правилу склейки (how)\n",
    "\n",
    "#Регрессионная переменная month_num - порядковый номер пары (месяц, год). Логарифмируем таргет\n",
    "df['month_num'] = range(1, len(df) + 1)\n",
    "df['log_y'] = np.log10(df['series_g'])\n",
    "\n",
    "#Создадем 12 колонок season_1.., season_12, в которые поместим индикаторы соответствующего месяца\n",
    "#Чтобы избежать колинеарности, исключаем один из месяцев(январь) и делаем его эталоном, с которым сравниваем все остальные\n",
    "#Внутри цикла проверяем, равен ли очередной месяц текущему значению из цикла\n",
    "for x in range(1, 13):\n",
    "    df['season_' + str(x)] = df['date'].dt.month == x\n",
    "    \n",
    "# xrange(2, 13) соответствует всем месяцам с февраля по декабрь\n",
    "season_columns = ['season_' + str(x) for x in range(2, 13)]\n",
    "\n",
    "# Создадим матрицу X и вектор y для обучения модели\n",
    "X = df[['month_num'] + season_columns]\n",
    "y = df['log_y']\n",
    "\n",
    "# Оставим только те строчки, у которых известны значения y (с номером < 144)\n",
    "X1 = X[X.index < 144]\n",
    "y1 = y[y.index < 144]\n",
    "\n",
    "#Настроим регрессионную модель. \"Подгонка\" через .fit\n",
    "model = LinearRegression()\n",
    "model.fit(X1, y1)\n",
    "\n",
    "pred = pd.DataFrame({\n",
    "    'pred': model.predict(X1),\n",
    "    'real': y1})\n",
    "pred.plot()\n",
    "\n",
    "#строим предсказание для всей матрицы X, включая неизвестные 12 месяцев\n",
    "pred = pd.DataFrame({\n",
    "    'pred': model.predict(X),\n",
    "    'real': y})\n",
    "pred.plot()\n",
    "\n",
    "#Экспонируем прогноз, чтобы получить реальные числа\n",
    "pred['number'] = 10**pred['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Логистическая регрессия позволяет исседовать взаимосвязи для зависимой переменной с двумя значениями (0,1)\n",
    "#Множественная регрессия  Y= B0+B1*X1 + ... + BN*XN   Многомерный scatter-plot\n",
    "#Дополнительно требует: мультиколлинеарность(без сильной корреляции или идентичности), нормальное распределение переменных.\n",
    "#t-критерий показывает оказываемое влияние каждого предиктора. Если 0, то влияния нет\n",
    "#Для множественной регрессии используется \"Исправленный\" R**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задача распознавания наименований или порядков через деревья классификации. И чисел через регрессию\n",
    "#Помимо внутренних параметров (заданных изначально), есть еще внешние (задаваемые аналитиком)\n",
    "#Выбор модели с помощью обучающей/тестовой выборок через наименьшую среднюю квадратичную ошибку \n",
    "#Критерий качества Q - сумма модулей ошибок или сумма квадратов ошибок или процент ошибок и т.д.\n",
    "#Валидация - метод проверки выбранной модели на ее адекватность\n",
    "#Регуляризация - инструмент проверки моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CART - Classification and regression trees\n",
    "#деление матрицы прямыми\\гиперплоскостями, чтобы в ограниченных областях доминировали схожие объекты\n",
    "#Узел(node) - множество, которое расщепляется. Родительский, потомок, конечный. \n",
    "#Пороговое значение - эталон для сравнения\n",
    "#Ограничения задаются оператором. На кол-во слоев, на свойство потомков, на родителя, на правила остановки \n",
    "#Чистота - порядок разделения выборки на части, в каждой из которых \"загрязнение\" данных меньше\n",
    "#Критерий загразненности(вероятность принадлежать к классу P) измеряется Энтропией, Индексом Джини или Ошибкой Классификации\n",
    "#Энтропия H1 = -СуммаP*log2P. Индекс Джини H2 = 1-СуммаP**2 = СуммаP*(1-P). Ошибка Классификации H3 = 1-maxP\n",
    "#Дельта H - вклад переменной в очищение. Считаем суммы для каждой и получаем информативность переменной\n",
    "\n",
    "#Задача кредитного скоринга\n",
    "df = pd.read_csv('Credit.csv', sep=';', encoding='cp1251')\n",
    "\n",
    "# Правильный ответ записываем в вектор y\n",
    "y = df[u'кредит']\n",
    "# Удаляем колонку с правильным ответом\n",
    "X = df.drop(u'кредит', axis=1)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Инициализируем и обучаем модель\n",
    "model = DecisionTreeClassifier(random_state=42,\n",
    "                               # функция для impurity ('gini' или 'entropy')\n",
    "                               criterion='gini',\n",
    "                               # максимальная глубина дерева\n",
    "                               max_depth=5,\n",
    "                               # минимальное число элементов в узле для разбиения (может быть долей)\n",
    "                               min_samples_split=5,\n",
    "                               # минимальное число элементов в листе (может быть долей)\n",
    "                               min_samples_leaf=5,\n",
    "                               # минимальное значение дельты impurity\n",
    "                               # min_impurity_decrease=0,\n",
    "                               # веса для классов (можно дополнительно штрафовать за ошибку в нужных классах).\n",
    "                               # поддерживает опцию 'balanced'.\n",
    "                               class_weight=None\n",
    "                               \n",
    "                              )\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "#Для интерпретации получившейся модели изображаем её в виде дерева предикатов (решающих правил)\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "\n",
    "export_graphviz(model,\n",
    "                out_file='tree.dot',\n",
    "                #задать названия фич\n",
    "                #feature_names=X.columns,\n",
    "                class_names=None,\n",
    "                #показывать названия полей у численных значений внутри узла\n",
    "                label='all',\n",
    "                #раскрашивать узлы в цвет преобладающего класса\n",
    "                filled=True,\n",
    "                #показывать значение impurity для каждого узла\n",
    "                impurity=True,\n",
    "                #показывать номера узлов\n",
    "                node_ids=True,\n",
    "                #Показывать доли каждого класса в узлах (а не количество)\n",
    "                proportion=True,\n",
    "                #Повернуть дерево на 90 градусов (вертикальная ориентация)\n",
    "                rotate=True,\n",
    "                #Число точек после запятой для отображаемых дробей\n",
    "                #precision=3\n",
    "               )\n",
    "\n",
    "#Преобразуем файл .dot в .png\n",
    "#node - номер узла, X[1]<=1.5 правило расщепления, gini, samples-доля наблюдений попавших в узел, p-value (p0, pX)\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree.png')\n",
    "Image(\"tree.png\")\n",
    "\n",
    "#Модель позволяет оценить ценность (importance) и эффективность каждой фичи, считая для каждой из сумму дельты H \n",
    "pd.DataFrame({'feature': X.columns,\n",
    "              'importance': model.feature_importances_}).sort_values('importance', \n",
    "            ascending=False\n",
    "            )\n",
    "\n",
    "#Метод predict позволяет получить предсказания классов для входного списка элементов (подаём на вход матрицу)\n",
    "# Предсказание класса для новых элементов\n",
    "new_item = [1, 1, 1, 1]\n",
    "model.predict([new_item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Расщепление на обучающую и тестовые выборки\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
    "                                                    # доля объёма тестового множества\n",
    "                                                    test_size=0.2)\n",
    "#обучаем модель\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Строим предсказание модели на тестовом множестве\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Оценка качества классификатора: доля совпавших ответов в y_pred и y_test, или считаем точность и полноту\n",
    "#Если доля в обучающем выше тестового, означает переобученность модели. Нужно упрощать модель\n",
    "#Матрица ошибок  𝐶=(𝑐𝑖,𝑗) , где  𝑐𝑖,𝑗 количество элементов класса 𝑖 , которым классификатор присвоил класс 𝑗 \n",
    "#Точность(precision) - доля правильно классифицированных объектов в найденных классификатором. \n",
    "#Полнота(recall) - доля этих объектов НА САМОМ ДЕЛЕ\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "#Гармоническое среднее F1 = 2*точность*полнота/(точность+полнота). Считается с помощью classification_report\n",
    "print(metrics.classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Деревья решений для задач регрессии (отклик не дискретный, а непрерывный). Методы схожы с деревом классификации\n",
    "#Предпочтительнии линейной регрессии, когда зависимость не линейная :)\n",
    "#В этом случа Дельта H = сумма квадратов ошибок\n",
    "#Prune (обрезание) - очистка от узлов, которые не нужны, через добавление третьей выборки (валидации)\n",
    "\n",
    "#Случайный лес. Ключевые параметры:\n",
    "#ntree - число деревьев(в начале макс, потом сокращать), mtry - число переменных в выборке (M**0.5)\n",
    "#sampsize - число наблюдений в подвыборке(0.632*N для декорреляции), nodesize - мин. число наблюдений в узле (10) \n",
    "#replace - подвыборка с  возвращением или без\n",
    "#out-of-bag - неиспользуемая при обучении часть выборки, используется в качестве предварительного теста модели\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42, #зерно датчика случайных чисел\n",
    "                               #число деревьев в лесу\n",
    "                               n_estimators=30,\n",
    "                               #функция для дельта H, impurity ('gini' или 'entropy')\n",
    "                               criterion='gini',\n",
    "                               #Макс число слоев\n",
    "                               max_depth=5,\n",
    "                               #Вычислять out-of-bag ошибку\n",
    "                               oob_score=True,\n",
    "                               #использовать результаты предыдущего вызова и нарастить предыдущий лес \n",
    "                               warm_start=False,\n",
    "                               #веса классов для балансировки выборки для обучения\n",
    "                               class_weight=None\n",
    "                               \n",
    "                              )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_pred, y_test))\n",
    "\n",
    "print('Out-of-bag score: {0}'.format(model.oob_score_)) \n",
    "\n",
    "pd.DataFrame({'feature': X.columns,\n",
    "              'importance': model.feature_importances_}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Приемы улучшения классификаторов: stacking, bagging, boosting\n",
    "#Stacking(предсказание на базе предсказаний)\n",
    "#Bagging(усредненное мнение всех моделей), он же случайный лес. Чтобы избежать колинеарности, выборки собираются рандомно\n",
    "#Boosting - обучение на основе ошибок предыдущего классификатора (улучшением слабого классификатора)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GBM - Gradient Boosting Machine. Остановка бустинга, когда очередные циклы перстают улучшать модель\n",
    "#Сумма квадратов ошибок Zi = -2*(Yi - f(Xi))\n",
    "#Метод максимального правдоподобия. Предполагаем наиболее вероятное событие. Критерий качества = P**A*(1-P)**(n-A)\n",
    "#Критерии качества Гаусса и Лапласа универсальны. Для двух классов - биномиальное, для большего - мультиноминальное\n",
    "#При временном промежутке - распределение Пуассона\n",
    "\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "           'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "df = pd.read_csv('adult.data', header=None, names=columns, na_values=' ?')\n",
    "\n",
    "# Удаляем колонку education (потому что есть уже закодированная колонка education-num)\n",
    "df = df.drop('education', axis=1)\n",
    "\n",
    "# Кодируем отклик в бинарные значения\n",
    "df['income'] = df['income'].map({' <=50K': 0, ' >50K': 1})\n",
    "\n",
    "# удаляем строки с NA значениями\n",
    "df = df.dropna()\n",
    "\n",
    "test = pd.read_csv('adult.test', header=None, names=columns, na_values=' ?', skiprows=1)\n",
    "test = test.drop('education', axis=1)\n",
    "test['income'] = test['income'].map({' <=50K.': 0, ' >50K.': 1})\n",
    "test = test.dropna()\n",
    "\n",
    "#Распределение классов в отклике\n",
    "df['income'].value_counts(normalize=True)\n",
    "\n",
    "#Разбиваем дату. Бинаризуем категориальные признаки (one-hot encoding).\n",
    "X_train = pd.get_dummies(df).drop('income', axis=1)\n",
    "y_train = df['income']\n",
    "\n",
    "X_test = pd.get_dummies(test).drop('income', axis=1)\n",
    "y_test = test['income']\n",
    "\n",
    "#В тестовой выборке не хватает одной колонки \n",
    "print(len(X_train.columns))\n",
    "print(len(X_test.columns))\n",
    "\n",
    "#Приводим множество названий колонок к типу set, находим разность двух множеств: Голландии нет в колонке native-county \n",
    "print(set(X_train.columns) - set(X_test.columns))\n",
    "print(set(X_test.columns) - set(X_train.columns))\n",
    "\n",
    "#Добавляем недостающую колонку\n",
    "columns = set(X_train.columns) | set(X_test.columns)\n",
    "X_train = X_train.reindex(columns=columns).fillna(0)\n",
    "X_test = X_test.reindex(columns=columns).fillna(0)\n",
    "\n",
    "#Проверяем совпадение колонок (если да, то True)\n",
    "all(X_train.columns == X_test.columns)\n",
    "\n",
    "#Обучаем модель\n",
    "model = GradientBoostingClassifier(random_state=42,\n",
    "                                   # Число деревьев\n",
    "                                   n_estimators=500,\n",
    "                                   #загрязнение измеряем “mse”, “mae” или “friedman_mse” (mse с улучшениями)  \n",
    "                                   criterion='friedman_mse', \n",
    "                                   #Максимальная глубина каждого дерева\n",
    "                                   #критерий качества ‘deviance’ (кросс-энтропия) или ‘exponential’\n",
    "                                   #‘deviance’ для классификации с вероятностями на выходе\n",
    "                                   loss='deviance', \n",
    "                                   # минимальное уменьшение загрязнения \n",
    "                                   min_impurity_decrease=0.0, \n",
    "                                   # Устарело\n",
    "                                   min_impurity_split=None,\n",
    "                                   # число узлов в дереве\n",
    "                                   max_depth=5,\n",
    "                                   #минимальное число наблюдений в потомке\n",
    "                                   min_samples_leaf=5, \n",
    "                                   #минимальное число наблюдений в родителе\n",
    "                                   min_samples_split=10,\n",
    "                                   #Параметр, уменьшающий переобучение, являющемся весом отдельного дерева (меньше лучше)\n",
    "                                   learning_rate=0.01                                   \n",
    "                                   )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "#Cмотрим важность признаков\n",
    "fi = pd.DataFrame({'features': X_train.columns, 'importance': model.feature_importances_})\n",
    "fi.sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "#Калибровка (интерпретация вероятности)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "model_sigmoid = CalibratedClassifierCV(model, cv=2, method='sigmoid')\n",
    "# method : ‘sigmoid’ or ‘isotonic’\n",
    "\n",
    "# Calibrate probabilities\n",
    "model_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "# View calibrated probabilities\n",
    "model_sigmoid.predict_proba(X_test)[0:11, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Метод градиентного спуска\n",
    "#Метод обратного распределения E = сумма(Yi-Vi)**2 позволяет через MSE находить ошибку и на ее основе исправлять веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "12/12 [==============================] - 0s 668us/step - loss: 43.5210 - accuracy: 0.0840\n",
      "Epoch 2/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 32.8726 - accuracy: 0.0588\n",
      "Epoch 3/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 22.5263 - accuracy: 0.2017\n",
      "Epoch 4/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 12.8994 - accuracy: 0.2101\n",
      "Epoch 5/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 4.7547 - accuracy: 0.2269\n",
      "Epoch 6/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.4066 - accuracy: 0.5462\n",
      "Epoch 7/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.3519 - accuracy: 0.6555\n",
      "Epoch 8/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0499 - accuracy: 0.6555\n",
      "Epoch 9/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0295 - accuracy: 0.6807\n",
      "Epoch 10/300\n",
      "12/12 [==============================] - 0s 748us/step - loss: 1.0315 - accuracy: 0.6387\n",
      "Epoch 11/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0046 - accuracy: 0.6303\n",
      "Epoch 12/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.9712 - accuracy: 0.6723\n",
      "Epoch 13/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.9548 - accuracy: 0.6807\n",
      "Epoch 14/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.9816 - accuracy: 0.6555\n",
      "Epoch 15/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.9543 - accuracy: 0.6555\n",
      "Epoch 16/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.9995 - accuracy: 0.5630\n",
      "Epoch 17/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0229 - accuracy: 0.6218\n",
      "Epoch 18/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.9765 - accuracy: 0.6555\n",
      "Epoch 19/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.9166 - accuracy: 0.6471\n",
      "Epoch 20/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.9104 - accuracy: 0.6471\n",
      "Epoch 21/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.8671 - accuracy: 0.6639\n",
      "Epoch 22/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.8568 - accuracy: 0.6555\n",
      "Epoch 23/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.9560 - accuracy: 0.6975\n",
      "Epoch 24/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.8876 - accuracy: 0.6639\n",
      "Epoch 25/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.8650 - accuracy: 0.6555\n",
      "Epoch 26/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.8254 - accuracy: 0.6639\n",
      "Epoch 27/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.8491 - accuracy: 0.6891\n",
      "Epoch 28/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 0.8248 - accuracy: 0.6723\n",
      "Epoch 29/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.8123 - accuracy: 0.6723\n",
      "Epoch 30/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.8030 - accuracy: 0.6975\n",
      "Epoch 31/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.7975 - accuracy: 0.6891\n",
      "Epoch 32/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.8316 - accuracy: 0.6723\n",
      "Epoch 33/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.7688 - accuracy: 0.6723\n",
      "Epoch 34/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.7809 - accuracy: 0.6891\n",
      "Epoch 35/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.7552 - accuracy: 0.6807\n",
      "Epoch 36/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.7542 - accuracy: 0.6723\n",
      "Epoch 37/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.7441 - accuracy: 0.6891\n",
      "Epoch 38/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.7685 - accuracy: 0.6723\n",
      "Epoch 39/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.7670 - accuracy: 0.6975\n",
      "Epoch 40/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.7457 - accuracy: 0.6639\n",
      "Epoch 41/300\n",
      "12/12 [==============================] - 0s 668us/step - loss: 0.7400 - accuracy: 0.6807\n",
      "Epoch 42/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.7256 - accuracy: 0.6723\n",
      "Epoch 43/300\n",
      "12/12 [==============================] - 0s 752us/step - loss: 0.7106 - accuracy: 0.7059\n",
      "Epoch 44/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 0.7282 - accuracy: 0.6723\n",
      "Epoch 45/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.7207 - accuracy: 0.7143\n",
      "Epoch 46/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.7606 - accuracy: 0.6807\n",
      "Epoch 47/300\n",
      "12/12 [==============================] - 0s 624us/step - loss: 0.7112 - accuracy: 0.6807\n",
      "Epoch 48/300\n",
      "12/12 [==============================] - 0s 712us/step - loss: 0.6864 - accuracy: 0.6891\n",
      "Epoch 49/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.7403 - accuracy: 0.6639\n",
      "Epoch 50/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.7126 - accuracy: 0.6807\n",
      "Epoch 51/300\n",
      "12/12 [==============================] - 0s 747us/step - loss: 0.6919 - accuracy: 0.6891\n",
      "Epoch 52/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.6746 - accuracy: 0.6975\n",
      "Epoch 53/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 0.6753 - accuracy: 0.7143\n",
      "Epoch 54/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.6613 - accuracy: 0.7059\n",
      "Epoch 55/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.6654 - accuracy: 0.7311\n",
      "Epoch 56/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.6544 - accuracy: 0.7143\n",
      "Epoch 57/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.6776 - accuracy: 0.6807\n",
      "Epoch 58/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.6528 - accuracy: 0.6891\n",
      "Epoch 59/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.6294 - accuracy: 0.6891\n",
      "Epoch 60/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.6470 - accuracy: 0.6723\n",
      "Epoch 61/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.6460 - accuracy: 0.6975\n",
      "Epoch 62/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 0.6480 - accuracy: 0.6975\n",
      "Epoch 63/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.6441 - accuracy: 0.6723\n",
      "Epoch 64/300\n",
      "12/12 [==============================] - 0s 626us/step - loss: 0.6386 - accuracy: 0.7059\n",
      "Epoch 65/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.6461 - accuracy: 0.6723\n",
      "Epoch 66/300\n",
      "12/12 [==============================] - 0s 626us/step - loss: 0.6165 - accuracy: 0.6723\n",
      "Epoch 67/300\n",
      "12/12 [==============================] - 0s 748us/step - loss: 0.5997 - accuracy: 0.6891\n",
      "Epoch 68/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.6041 - accuracy: 0.7143\n",
      "Epoch 69/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 0.6102 - accuracy: 0.7227\n",
      "Epoch 70/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.5815 - accuracy: 0.7143\n",
      "Epoch 71/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.5815 - accuracy: 0.7311\n",
      "Epoch 72/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.5799 - accuracy: 0.7143\n",
      "Epoch 73/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 0.5651 - accuracy: 0.7143\n",
      "Epoch 74/300\n",
      "12/12 [==============================] - 0s 670us/step - loss: 0.5615 - accuracy: 0.7143\n",
      "Epoch 75/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.5897 - accuracy: 0.7227\n",
      "Epoch 76/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.5868 - accuracy: 0.7311\n",
      "Epoch 77/300\n",
      "12/12 [==============================] - 0s 752us/step - loss: 0.6115 - accuracy: 0.7059\n",
      "Epoch 78/300\n",
      "12/12 [==============================] - 0s 711us/step - loss: 0.5641 - accuracy: 0.6975\n",
      "Epoch 79/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 0.5565 - accuracy: 0.6807\n",
      "Epoch 80/300\n",
      "12/12 [==============================] - 0s 668us/step - loss: 0.5401 - accuracy: 0.6975\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 664us/step - loss: 0.5951 - accuracy: 0.7311\n",
      "Epoch 82/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 0.6437 - accuracy: 0.7227\n",
      "Epoch 83/300\n",
      "12/12 [==============================] - 0s 714us/step - loss: 0.5375 - accuracy: 0.7395\n",
      "Epoch 84/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.6066 - accuracy: 0.7227\n",
      "Epoch 85/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 0.6681 - accuracy: 0.6555\n",
      "Epoch 86/300\n",
      "12/12 [==============================] - 0s 709us/step - loss: 0.6455 - accuracy: 0.7563\n",
      "Epoch 87/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.5379 - accuracy: 0.7311\n",
      "Epoch 88/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.5712 - accuracy: 0.6975\n",
      "Epoch 89/300\n",
      "12/12 [==============================] - 0s 748us/step - loss: 0.5849 - accuracy: 0.7563\n",
      "Epoch 90/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.5072 - accuracy: 0.7143\n",
      "Epoch 91/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.5472 - accuracy: 0.7143\n",
      "Epoch 92/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.5220 - accuracy: 0.7311\n",
      "Epoch 93/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.5065 - accuracy: 0.7143\n",
      "Epoch 94/300\n",
      "12/12 [==============================] - 0s 712us/step - loss: 0.5031 - accuracy: 0.7647\n",
      "Epoch 95/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 0.4955 - accuracy: 0.7479\n",
      "Epoch 96/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 0.4884 - accuracy: 0.7479\n",
      "Epoch 97/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.5031 - accuracy: 0.7227\n",
      "Epoch 98/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.4738 - accuracy: 0.7227\n",
      "Epoch 99/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.4879 - accuracy: 0.7395\n",
      "Epoch 100/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.4593 - accuracy: 0.7731\n",
      "Epoch 101/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.4669 - accuracy: 0.7311\n",
      "Epoch 102/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.4887 - accuracy: 0.7479\n",
      "Epoch 103/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.4766 - accuracy: 0.7143\n",
      "Epoch 104/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.4573 - accuracy: 0.7395\n",
      "Epoch 105/300\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2393 - accuracy: 0.80 - 0s 669us/step - loss: 0.4414 - accuracy: 0.7479\n",
      "Epoch 106/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.4564 - accuracy: 0.7563\n",
      "Epoch 107/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.4713 - accuracy: 0.7731\n",
      "Epoch 108/300\n",
      "12/12 [==============================] - 0s 748us/step - loss: 0.4573 - accuracy: 0.7563\n",
      "Epoch 109/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.4581 - accuracy: 0.7563\n",
      "Epoch 110/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.4646 - accuracy: 0.7815\n",
      "Epoch 111/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.4332 - accuracy: 0.7731\n",
      "Epoch 112/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.4095 - accuracy: 0.7983\n",
      "Epoch 113/300\n",
      "12/12 [==============================] - 0s 752us/step - loss: 0.4155 - accuracy: 0.7731\n",
      "Epoch 114/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 0.4346 - accuracy: 0.7731\n",
      "Epoch 115/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.4129 - accuracy: 0.8067\n",
      "Epoch 116/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 0.3946 - accuracy: 0.8067\n",
      "Epoch 117/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.3964 - accuracy: 0.8235\n",
      "Epoch 118/300\n",
      "12/12 [==============================] - 0s 632us/step - loss: 0.3893 - accuracy: 0.8235\n",
      "Epoch 119/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.4044 - accuracy: 0.8151\n",
      "Epoch 120/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.3851 - accuracy: 0.8319\n",
      "Epoch 121/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.3980 - accuracy: 0.8319\n",
      "Epoch 122/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.3911 - accuracy: 0.7983\n",
      "Epoch 123/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.3803 - accuracy: 0.8151\n",
      "Epoch 124/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.3726 - accuracy: 0.8571\n",
      "Epoch 125/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.3575 - accuracy: 0.8824\n",
      "Epoch 126/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.3108 - accuracy: 0.8571\n",
      "Epoch 127/300\n",
      "12/12 [==============================] - 0s 748us/step - loss: 0.3353 - accuracy: 0.8739\n",
      "Epoch 128/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 0.3791 - accuracy: 0.7983\n",
      "Epoch 129/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.3656 - accuracy: 0.8319\n",
      "Epoch 130/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.3751 - accuracy: 0.8487\n",
      "Epoch 131/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.3026 - accuracy: 0.8908\n",
      "Epoch 132/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2807 - accuracy: 0.8908\n",
      "Epoch 133/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2950 - accuracy: 0.8824\n",
      "Epoch 134/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.2829 - accuracy: 0.8992\n",
      "Epoch 135/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2923 - accuracy: 0.8571\n",
      "Epoch 136/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2814 - accuracy: 0.9160\n",
      "Epoch 137/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2802 - accuracy: 0.8487\n",
      "Epoch 138/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2582 - accuracy: 0.9160\n",
      "Epoch 139/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.3279 - accuracy: 0.8487\n",
      "Epoch 140/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.3051 - accuracy: 0.8908\n",
      "Epoch 141/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.3593 - accuracy: 0.8487\n",
      "Epoch 142/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.3487 - accuracy: 0.8739\n",
      "Epoch 143/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.3167 - accuracy: 0.9076\n",
      "Epoch 144/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2577 - accuracy: 0.8992\n",
      "Epoch 145/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 0.2817 - accuracy: 0.8824\n",
      "Epoch 146/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.3039 - accuracy: 0.8908\n",
      "Epoch 147/300\n",
      "12/12 [==============================] - 0s 668us/step - loss: 0.3164 - accuracy: 0.8487\n",
      "Epoch 148/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.3036 - accuracy: 0.8908\n",
      "Epoch 149/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2301 - accuracy: 0.9412\n",
      "Epoch 150/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2327 - accuracy: 0.9244\n",
      "Epoch 151/300\n",
      "12/12 [==============================] - 0s 602us/step - loss: 0.2416 - accuracy: 0.9160\n",
      "Epoch 152/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.2484 - accuracy: 0.9244\n",
      "Epoch 153/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.2399 - accuracy: 0.9244\n",
      "Epoch 154/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2373 - accuracy: 0.8992\n",
      "Epoch 155/300\n",
      "12/12 [==============================] - 0s 708us/step - loss: 0.2451 - accuracy: 0.9412\n",
      "Epoch 156/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2118 - accuracy: 0.9244\n",
      "Epoch 157/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2460 - accuracy: 0.9328\n",
      "Epoch 158/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2178 - accuracy: 0.9160\n",
      "Epoch 159/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 0.2460 - accuracy: 0.8992\n",
      "Epoch 160/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 581us/step - loss: 0.2019 - accuracy: 0.9244\n",
      "Epoch 161/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2219 - accuracy: 0.8992\n",
      "Epoch 162/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2278 - accuracy: 0.9076\n",
      "Epoch 163/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.2280 - accuracy: 0.9496\n",
      "Epoch 164/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2400 - accuracy: 0.9328\n",
      "Epoch 165/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2057 - accuracy: 0.9580\n",
      "Epoch 166/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.2239 - accuracy: 0.9244\n",
      "Epoch 167/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 0.1897 - accuracy: 0.9412\n",
      "Epoch 168/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.2214 - accuracy: 0.9076\n",
      "Epoch 169/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2224 - accuracy: 0.9328\n",
      "Epoch 170/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 0.1954 - accuracy: 0.9328\n",
      "Epoch 171/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2063 - accuracy: 0.9412\n",
      "Epoch 172/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2337 - accuracy: 0.9244\n",
      "Epoch 173/300\n",
      "12/12 [==============================] - 0s 668us/step - loss: 0.2346 - accuracy: 0.9076\n",
      "Epoch 174/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.2411 - accuracy: 0.9076\n",
      "Epoch 175/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.2061 - accuracy: 0.9160\n",
      "Epoch 176/300\n",
      "12/12 [==============================] - 0s 630us/step - loss: 0.2476 - accuracy: 0.8824\n",
      "Epoch 177/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.2362 - accuracy: 0.9412\n",
      "Epoch 178/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1774 - accuracy: 0.9328\n",
      "Epoch 179/300\n",
      "12/12 [==============================] - 0s 752us/step - loss: 0.1843 - accuracy: 0.9412\n",
      "Epoch 180/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 0.1968 - accuracy: 0.9496\n",
      "Epoch 181/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.1950 - accuracy: 0.9412\n",
      "Epoch 182/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.1680 - accuracy: 0.9496\n",
      "Epoch 183/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2470 - accuracy: 0.9412\n",
      "Epoch 184/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.3214 - accuracy: 0.8824\n",
      "Epoch 185/300\n",
      "12/12 [==============================] - 0s 680us/step - loss: 0.3271 - accuracy: 0.9160\n",
      "Epoch 186/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2702 - accuracy: 0.9076\n",
      "Epoch 187/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.2407 - accuracy: 0.8824\n",
      "Epoch 188/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2033 - accuracy: 0.9412\n",
      "Epoch 189/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.1948 - accuracy: 0.9328\n",
      "Epoch 190/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.2069 - accuracy: 0.8992\n",
      "Epoch 191/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2487 - accuracy: 0.9160\n",
      "Epoch 192/300\n",
      "12/12 [==============================] - 0s 668us/step - loss: 0.1698 - accuracy: 0.9580\n",
      "Epoch 193/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1789 - accuracy: 0.9412\n",
      "Epoch 194/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.1805 - accuracy: 0.9496\n",
      "Epoch 195/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1877 - accuracy: 0.9412\n",
      "Epoch 196/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.1901 - accuracy: 0.9328\n",
      "Epoch 197/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1733 - accuracy: 0.9412\n",
      "Epoch 198/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1652 - accuracy: 0.9412\n",
      "Epoch 199/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.1907 - accuracy: 0.9328\n",
      "Epoch 200/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.1805 - accuracy: 0.9580\n",
      "Epoch 201/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 0.1743 - accuracy: 0.9412\n",
      "Epoch 202/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.1944 - accuracy: 0.9496\n",
      "Epoch 203/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1866 - accuracy: 0.9328\n",
      "Epoch 204/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1748 - accuracy: 0.9580\n",
      "Epoch 205/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.1827 - accuracy: 0.9496\n",
      "Epoch 206/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1486 - accuracy: 0.9496\n",
      "Epoch 207/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.1635 - accuracy: 0.9412\n",
      "Epoch 208/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.1561 - accuracy: 0.9496\n",
      "Epoch 209/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2185 - accuracy: 0.9076\n",
      "Epoch 210/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1703 - accuracy: 0.9580\n",
      "Epoch 211/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2383 - accuracy: 0.9244\n",
      "Epoch 212/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 0.2061 - accuracy: 0.9412\n",
      "Epoch 213/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1869 - accuracy: 0.9328\n",
      "Epoch 214/300\n",
      "12/12 [==============================] - 0s 628us/step - loss: 0.2208 - accuracy: 0.9328\n",
      "Epoch 215/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.1560 - accuracy: 0.9580\n",
      "Epoch 216/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.2053 - accuracy: 0.9244\n",
      "Epoch 217/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1896 - accuracy: 0.9328\n",
      "Epoch 218/300\n",
      "12/12 [==============================] - 0s 626us/step - loss: 0.2132 - accuracy: 0.9160\n",
      "Epoch 219/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.1595 - accuracy: 0.9496\n",
      "Epoch 220/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1429 - accuracy: 0.9496\n",
      "Epoch 221/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.1467 - accuracy: 0.9496\n",
      "Epoch 222/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1631 - accuracy: 0.9412\n",
      "Epoch 223/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1508 - accuracy: 0.9580\n",
      "Epoch 224/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1398 - accuracy: 0.9496\n",
      "Epoch 225/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1738 - accuracy: 0.9412\n",
      "Epoch 226/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1761 - accuracy: 0.9496\n",
      "Epoch 227/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1715 - accuracy: 0.9160\n",
      "Epoch 228/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1463 - accuracy: 0.9664\n",
      "Epoch 229/300\n",
      "12/12 [==============================] - 0s 565us/step - loss: 0.1359 - accuracy: 0.9580\n",
      "Epoch 230/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 0.1358 - accuracy: 0.9580\n",
      "Epoch 231/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1504 - accuracy: 0.9580\n",
      "Epoch 232/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.1323 - accuracy: 0.9496\n",
      "Epoch 233/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 0.1714 - accuracy: 0.9496\n",
      "Epoch 234/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.1712 - accuracy: 0.9412\n",
      "Epoch 235/300\n",
      "12/12 [==============================] - 0s 670us/step - loss: 0.2032 - accuracy: 0.8992\n",
      "Epoch 236/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1853 - accuracy: 0.9412\n",
      "Epoch 237/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1180 - accuracy: 0.9580\n",
      "Epoch 238/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1979 - accuracy: 0.9160\n",
      "Epoch 239/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 586us/step - loss: 0.1203 - accuracy: 0.9664\n",
      "Epoch 240/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1419 - accuracy: 0.9664\n",
      "Epoch 241/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.1813 - accuracy: 0.9412\n",
      "Epoch 242/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1371 - accuracy: 0.9664\n",
      "Epoch 243/300\n",
      "12/12 [==============================] - 0s 833us/step - loss: 0.1265 - accuracy: 0.9496\n",
      "Epoch 244/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1481 - accuracy: 0.9496\n",
      "Epoch 245/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 0.1319 - accuracy: 0.9580\n",
      "Epoch 246/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.1371 - accuracy: 0.9496\n",
      "Epoch 247/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1431 - accuracy: 0.9748\n",
      "Epoch 248/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1341 - accuracy: 0.9412\n",
      "Epoch 249/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.1324 - accuracy: 0.9748\n",
      "Epoch 250/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1816 - accuracy: 0.9160\n",
      "Epoch 251/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.2262 - accuracy: 0.9244\n",
      "Epoch 252/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 0.1895 - accuracy: 0.9160\n",
      "Epoch 253/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1486 - accuracy: 0.9328\n",
      "Epoch 254/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 0.1700 - accuracy: 0.9328\n",
      "Epoch 255/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 0.1493 - accuracy: 0.9328\n",
      "Epoch 256/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.1284 - accuracy: 0.9496\n",
      "Epoch 257/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1335 - accuracy: 0.9580\n",
      "Epoch 258/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1256 - accuracy: 0.9664\n",
      "Epoch 259/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.1240 - accuracy: 0.9664\n",
      "Epoch 260/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1365 - accuracy: 0.9496\n",
      "Epoch 261/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 0.1351 - accuracy: 0.9580\n",
      "Epoch 262/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 0.1342 - accuracy: 0.9580\n",
      "Epoch 263/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.1578 - accuracy: 0.9412\n",
      "Epoch 264/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1100 - accuracy: 0.9580\n",
      "Epoch 265/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1274 - accuracy: 0.9580\n",
      "Epoch 266/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1353 - accuracy: 0.9496\n",
      "Epoch 267/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.1298 - accuracy: 0.9580\n",
      "Epoch 268/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1275 - accuracy: 0.9496\n",
      "Epoch 269/300\n",
      "12/12 [==============================] - 0s 748us/step - loss: 0.1187 - accuracy: 0.9664\n",
      "Epoch 270/300\n",
      "12/12 [==============================] - 0s 748us/step - loss: 0.1202 - accuracy: 0.9496\n",
      "Epoch 271/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 0.1325 - accuracy: 0.9580\n",
      "Epoch 272/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1239 - accuracy: 0.9580\n",
      "Epoch 273/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1318 - accuracy: 0.9664\n",
      "Epoch 274/300\n",
      "12/12 [==============================] - 0s 748us/step - loss: 0.1420 - accuracy: 0.9496\n",
      "Epoch 275/300\n",
      "12/12 [==============================] - 0s 752us/step - loss: 0.1303 - accuracy: 0.9580\n",
      "Epoch 276/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1409 - accuracy: 0.9496\n",
      "Epoch 277/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.1928 - accuracy: 0.9160\n",
      "Epoch 278/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1419 - accuracy: 0.9412\n",
      "Epoch 279/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1351 - accuracy: 0.9496\n",
      "Epoch 280/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.1326 - accuracy: 0.9664\n",
      "Epoch 281/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1157 - accuracy: 0.9580\n",
      "Epoch 282/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1405 - accuracy: 0.9580\n",
      "Epoch 283/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.1716 - accuracy: 0.9160\n",
      "Epoch 284/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 0.1179 - accuracy: 0.9580\n",
      "Epoch 285/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1432 - accuracy: 0.9412\n",
      "Epoch 286/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1331 - accuracy: 0.9412\n",
      "Epoch 287/300\n",
      "12/12 [==============================] - 0s 668us/step - loss: 0.1548 - accuracy: 0.9496\n",
      "Epoch 288/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1536 - accuracy: 0.9328\n",
      "Epoch 289/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.2054 - accuracy: 0.9160\n",
      "Epoch 290/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1211 - accuracy: 0.9580\n",
      "Epoch 291/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1171 - accuracy: 0.9748\n",
      "Epoch 292/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1090 - accuracy: 0.9496\n",
      "Epoch 293/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.1069 - accuracy: 0.9664\n",
      "Epoch 294/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1100 - accuracy: 0.9580\n",
      "Epoch 295/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.1082 - accuracy: 0.9664\n",
      "Epoch 296/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 0.1249 - accuracy: 0.9580\n",
      "Epoch 297/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1276 - accuracy: 0.9496\n",
      "Epoch 298/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1131 - accuracy: 0.9496\n",
      "Epoch 299/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 0.1362 - accuracy: 0.9496\n",
      "Epoch 300/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 0.1256 - accuracy: 0.9580\n",
      "Epoch 1/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 177.2482 - accuracy: 0.4118\n",
      "Epoch 2/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0871 - accuracy: 0.4118\n",
      "Epoch 3/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 4/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 5/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 6/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0839 - accuracy: 0.4118\n",
      "Epoch 7/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 8/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0876 - accuracy: 0.4118\n",
      "Epoch 9/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 10/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 11/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 12/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 13/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 14/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 15/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 16/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 17/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 18/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 666us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 19/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 20/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 21/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 22/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0867 - accuracy: 0.4118\n",
      "Epoch 23/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 24/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 25/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 26/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 27/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 28/300\n",
      "12/12 [==============================] - 0s 661us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 29/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 30/300\n",
      "12/12 [==============================] - 0s 647us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 31/300\n",
      "12/12 [==============================] - 0s 636us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 32/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 33/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 34/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 35/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 36/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 37/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 38/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 39/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 40/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 41/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 42/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 43/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0839 - accuracy: 0.4118\n",
      "Epoch 44/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 45/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 46/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0838 - accuracy: 0.4118\n",
      "Epoch 47/300\n",
      "12/12 [==============================] - 0s 726us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 48/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 49/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 50/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 51/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 52/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 53/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 54/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 55/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 56/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 57/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 58/300\n",
      "12/12 [==============================] - 0s 643us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 59/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 60/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 1.0837 - accuracy: 0.4118\n",
      "Epoch 61/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 62/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 63/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0868 - accuracy: 0.4118\n",
      "Epoch 64/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 65/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 66/300\n",
      "12/12 [==============================] - 0s 603us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 67/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 68/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 69/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 70/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 71/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 72/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 73/300\n",
      "12/12 [==============================] - 0s 668us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 74/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 75/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 76/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 77/300\n",
      "12/12 [==============================] - 0s 580us/step - loss: 1.0861 - accuracy: 0.4118\n",
      "Epoch 78/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 79/300\n",
      "12/12 [==============================] - 0s 673us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 80/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 81/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 82/300\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0913 - accuracy: 0.40 - 0s 583us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 83/300\n",
      "12/12 [==============================] - 0s 629us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 84/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 85/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 86/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 87/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 88/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 89/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 90/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 91/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0839 - accuracy: 0.4118\n",
      "Epoch 92/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 93/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 94/300\n",
      "12/12 [==============================] - 0s 600us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 95/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 96/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 97/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 584us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 99/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 100/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 101/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 102/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 103/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 104/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 105/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 106/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 107/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0873 - accuracy: 0.4118\n",
      "Epoch 108/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 109/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 110/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 111/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 112/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 113/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 114/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 115/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 116/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 117/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 118/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 119/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 120/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 121/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 122/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 123/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 124/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 125/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 126/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 127/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 128/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 1.0838 - accuracy: 0.4118\n",
      "Epoch 129/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0861 - accuracy: 0.4118\n",
      "Epoch 130/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 131/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 132/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 133/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 134/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0839 - accuracy: 0.4118\n",
      "Epoch 135/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 136/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 137/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 138/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 139/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 140/300\n",
      "12/12 [==============================] - 0s 503us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 141/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0867 - accuracy: 0.4118\n",
      "Epoch 142/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0871 - accuracy: 0.4118\n",
      "Epoch 143/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 144/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 145/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 146/300\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0788 - accuracy: 0.50 - 0s 667us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 147/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 148/300\n",
      "12/12 [==============================] - 0s 626us/step - loss: 1.0836 - accuracy: 0.4118\n",
      "Epoch 149/300\n",
      "12/12 [==============================] - 0s 612us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 150/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 151/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 152/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 153/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 154/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 155/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 156/300\n",
      "12/12 [==============================] - 0s 627us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 157/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 158/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 159/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 160/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 161/300\n",
      "12/12 [==============================] - 0s 627us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 162/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 163/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0884 - accuracy: 0.4118\n",
      "Epoch 164/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0872 - accuracy: 0.4118\n",
      "Epoch 165/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 166/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 167/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 168/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 169/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 170/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 171/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 172/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 173/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 174/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 175/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 176/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 177/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 667us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 178/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 179/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 180/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 181/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 182/300\n",
      "12/12 [==============================] - 0s 624us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 183/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 184/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 185/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 186/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 187/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 188/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 189/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 190/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 191/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 192/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0839 - accuracy: 0.4118\n",
      "Epoch 193/300\n",
      "12/12 [==============================] - 0s 580us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 194/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 195/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 196/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 197/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 198/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 199/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 200/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 201/300\n",
      "12/12 [==============================] - 0s 668us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 202/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 203/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 204/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 205/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 206/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 207/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 208/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0870 - accuracy: 0.4118\n",
      "Epoch 209/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 210/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 211/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 212/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 213/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 214/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 215/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 216/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 217/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 218/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 219/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 220/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 221/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 222/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 223/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0871 - accuracy: 0.4118\n",
      "Epoch 224/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 225/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 226/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0838 - accuracy: 0.4118\n",
      "Epoch 227/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 228/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 229/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 230/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 231/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 232/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 233/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 234/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 235/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 236/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0865 - accuracy: 0.4118\n",
      "Epoch 237/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 238/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 239/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 240/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 241/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 242/300\n",
      "12/12 [==============================] - 0s 606us/step - loss: 1.0861 - accuracy: 0.4118\n",
      "Epoch 243/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 244/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 245/300\n",
      "12/12 [==============================] - 0s 580us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 246/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 247/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 248/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0839 - accuracy: 0.4118\n",
      "Epoch 249/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 250/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 251/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 252/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 253/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 254/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 255/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 256/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 667us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 257/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 258/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 259/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 260/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 261/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 262/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 263/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0839 - accuracy: 0.4118\n",
      "Epoch 264/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 265/300\n",
      "12/12 [==============================] - 0s 499us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 266/300\n",
      "12/12 [==============================] - 0s 592us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 267/300\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 268/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 269/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 270/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 271/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 272/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 273/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 274/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0871 - accuracy: 0.4118\n",
      "Epoch 275/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 276/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 277/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 278/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 279/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 280/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 281/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 282/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 283/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 284/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 285/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 286/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0839 - accuracy: 0.4118\n",
      "Epoch 287/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 288/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 289/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 290/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 291/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 292/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0837 - accuracy: 0.4118\n",
      "Epoch 293/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 294/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 295/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0838 - accuracy: 0.4118\n",
      "Epoch 296/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 297/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 298/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 299/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 300/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 1/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.1132 - accuracy: 0.3697\n",
      "Epoch 2/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0909 - accuracy: 0.4118\n",
      "Epoch 3/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 4/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 5/300\n",
      "12/12 [==============================] - 0s 564us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 6/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 7/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 8/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 9/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 10/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 11/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0874 - accuracy: 0.4118\n",
      "Epoch 12/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 13/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 14/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 15/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 16/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 17/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0873 - accuracy: 0.4118\n",
      "Epoch 18/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0875 - accuracy: 0.4118\n",
      "Epoch 19/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 20/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0872 - accuracy: 0.4118\n",
      "Epoch 21/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0865 - accuracy: 0.4118\n",
      "Epoch 22/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 23/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 24/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0868 - accuracy: 0.4118\n",
      "Epoch 25/300\n",
      "12/12 [==============================] - 0s 599us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 26/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 27/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 28/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 1.0869 - accuracy: 0.4118\n",
      "Epoch 29/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 30/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 31/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 32/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 33/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 34/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 35/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 36/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 667us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 37/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 38/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 39/300\n",
      "12/12 [==============================] - 0s 626us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 40/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 41/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 42/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 43/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 44/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 45/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0866 - accuracy: 0.4118\n",
      "Epoch 46/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 47/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0867 - accuracy: 0.4118\n",
      "Epoch 48/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 49/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0882 - accuracy: 0.4118\n",
      "Epoch 50/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 51/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 52/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 53/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 54/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 55/300\n",
      "12/12 [==============================] - 0s 531us/step - loss: 1.0870 - accuracy: 0.4118\n",
      "Epoch 56/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 57/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 58/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 59/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0873 - accuracy: 0.4118\n",
      "Epoch 60/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 61/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 62/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0882 - accuracy: 0.4118\n",
      "Epoch 63/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 64/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 65/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 66/300\n",
      "12/12 [==============================] - 0s 498us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 67/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 68/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 69/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 70/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 71/300\n",
      "12/12 [==============================] - 0s 497us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 72/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 73/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 74/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 75/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 76/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 77/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 78/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 79/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 80/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0868 - accuracy: 0.4118\n",
      "Epoch 81/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0865 - accuracy: 0.4118\n",
      "Epoch 82/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 83/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0873 - accuracy: 0.4118\n",
      "Epoch 84/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 85/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 86/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 87/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 88/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 89/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 90/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0904 - accuracy: 0.4118\n",
      "Epoch 91/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 92/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 93/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 94/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 95/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 96/300\n",
      "12/12 [==============================] - 0s 627us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 97/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 98/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 99/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0872 - accuracy: 0.4118\n",
      "Epoch 100/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 101/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0866 - accuracy: 0.4118\n",
      "Epoch 102/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 103/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 104/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0887 - accuracy: 0.4118\n",
      "Epoch 105/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0835 - accuracy: 0.4118\n",
      "Epoch 106/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 107/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 108/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 109/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 110/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0837 - accuracy: 0.4118\n",
      "Epoch 111/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 112/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0865 - accuracy: 0.4118\n",
      "Epoch 113/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 114/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 115/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 116/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 664us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 117/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 118/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 119/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 120/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 121/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 122/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 123/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 124/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 125/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 126/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 127/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 128/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 129/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 130/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 131/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 132/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 133/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0837 - accuracy: 0.4118\n",
      "Epoch 134/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0879 - accuracy: 0.4118\n",
      "Epoch 135/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0886 - accuracy: 0.4118\n",
      "Epoch 136/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 137/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 138/300\n",
      "12/12 [==============================] - 0s 629us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 139/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 140/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 141/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 142/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 143/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 1.0861 - accuracy: 0.4118\n",
      "Epoch 144/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 145/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 146/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 147/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0861 - accuracy: 0.4118\n",
      "Epoch 148/300\n",
      "12/12 [==============================] - 0s 748us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 149/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0866 - accuracy: 0.4118\n",
      "Epoch 150/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 151/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 152/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 153/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 154/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 155/300\n",
      "12/12 [==============================] - 0s 567us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 156/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 157/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0881 - accuracy: 0.4118\n",
      "Epoch 158/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0908 - accuracy: 0.4118\n",
      "Epoch 159/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 160/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 161/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 162/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0874 - accuracy: 0.4118\n",
      "Epoch 163/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 164/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 165/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 166/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 167/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 168/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 169/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 170/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0892 - accuracy: 0.4118\n",
      "Epoch 171/300\n",
      "12/12 [==============================] - 0s 643us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 172/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 173/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 174/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 175/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 176/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0861 - accuracy: 0.4118\n",
      "Epoch 177/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0847 - accuracy: 0.4118\n",
      "Epoch 178/300\n",
      "12/12 [==============================] - 0s 668us/step - loss: 1.0870 - accuracy: 0.4118\n",
      "Epoch 179/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 180/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 181/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 182/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0895 - accuracy: 0.4118\n",
      "Epoch 183/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 184/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 185/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 186/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 187/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 188/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 189/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 190/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0836 - accuracy: 0.4118\n",
      "Epoch 191/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 192/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0861 - accuracy: 0.4118\n",
      "Epoch 193/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0881 - accuracy: 0.4118\n",
      "Epoch 194/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 195/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 583us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 196/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 197/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 198/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 199/300\n",
      "12/12 [==============================] - 0s 639us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 200/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0872 - accuracy: 0.4118\n",
      "Epoch 201/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0895 - accuracy: 0.4118\n",
      "Epoch 202/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 203/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 204/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0888 - accuracy: 0.4118\n",
      "Epoch 205/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 206/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 207/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0850 - accuracy: 0.4118\n",
      "Epoch 208/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 209/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 210/300\n",
      "12/12 [==============================] - 0s 748us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 211/300\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0564 - accuracy: 0.40 - 0s 583us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 212/300\n",
      "12/12 [==============================] - 0s 643us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 213/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 214/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0858 - accuracy: 0.4118\n",
      "Epoch 215/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 216/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 217/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 218/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0875 - accuracy: 0.4118\n",
      "Epoch 219/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0871 - accuracy: 0.4118\n",
      "Epoch 220/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 221/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0839 - accuracy: 0.4118\n",
      "Epoch 222/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0856 - accuracy: 0.4118\n",
      "Epoch 223/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 224/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 225/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0838 - accuracy: 0.4118\n",
      "Epoch 226/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 227/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 228/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 229/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 230/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0872 - accuracy: 0.4118\n",
      "Epoch 231/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0861 - accuracy: 0.4118\n",
      "Epoch 232/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0840 - accuracy: 0.4118\n",
      "Epoch 233/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 1.0873 - accuracy: 0.4118\n",
      "Epoch 234/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 235/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 236/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 237/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0861 - accuracy: 0.4118\n",
      "Epoch 238/300\n",
      "12/12 [==============================] - 0s 633us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 239/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 240/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0880 - accuracy: 0.4118\n",
      "Epoch 241/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 242/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 243/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 244/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 245/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 246/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0873 - accuracy: 0.4118\n",
      "Epoch 247/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0860 - accuracy: 0.4118\n",
      "Epoch 248/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 249/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 250/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 251/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 252/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0887 - accuracy: 0.4118\n",
      "Epoch 253/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 254/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0869 - accuracy: 0.4118\n",
      "Epoch 255/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0896 - accuracy: 0.4118\n",
      "Epoch 256/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0885 - accuracy: 0.4118\n",
      "Epoch 257/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 258/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 259/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 260/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 261/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0875 - accuracy: 0.4118\n",
      "Epoch 262/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0839 - accuracy: 0.4118\n",
      "Epoch 263/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 264/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0838 - accuracy: 0.4118\n",
      "Epoch 265/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 266/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0842 - accuracy: 0.4118\n",
      "Epoch 267/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0895 - accuracy: 0.4118\n",
      "Epoch 268/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0868 - accuracy: 0.4118\n",
      "Epoch 269/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0859 - accuracy: 0.4118\n",
      "Epoch 270/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 271/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 272/300\n",
      "12/12 [==============================] - 0s 664us/step - loss: 1.0849 - accuracy: 0.4118\n",
      "Epoch 273/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0844 - accuracy: 0.4118\n",
      "Epoch 274/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 583us/step - loss: 1.0837 - accuracy: 0.4118\n",
      "Epoch 275/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 276/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0846 - accuracy: 0.4118\n",
      "Epoch 277/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 278/300\n",
      "12/12 [==============================] - 0s 500us/step - loss: 1.0863 - accuracy: 0.4118\n",
      "Epoch 279/300\n",
      "12/12 [==============================] - 0s 747us/step - loss: 1.0883 - accuracy: 0.4118\n",
      "Epoch 280/300\n",
      "12/12 [==============================] - 0s 581us/step - loss: 1.0852 - accuracy: 0.4118\n",
      "Epoch 281/300\n",
      "12/12 [==============================] - 0s 621us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 282/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0857 - accuracy: 0.4118\n",
      "Epoch 283/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 284/300\n",
      "12/12 [==============================] - 0s 752us/step - loss: 1.0848 - accuracy: 0.4118\n",
      "Epoch 285/300\n",
      "12/12 [==============================] - 0s 584us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 286/300\n",
      "12/12 [==============================] - 0s 666us/step - loss: 1.0853 - accuracy: 0.4118\n",
      "Epoch 287/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0836 - accuracy: 0.4118\n",
      "Epoch 288/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0855 - accuracy: 0.4118\n",
      "Epoch 289/300\n",
      "12/12 [==============================] - 0s 593us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 290/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0868 - accuracy: 0.4118\n",
      "Epoch 291/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0851 - accuracy: 0.4118\n",
      "Epoch 292/300\n",
      "12/12 [==============================] - 0s 665us/step - loss: 1.0883 - accuracy: 0.4118\n",
      "Epoch 293/300\n",
      "12/12 [==============================] - 0s 582us/step - loss: 1.0866 - accuracy: 0.4118\n",
      "Epoch 294/300\n",
      "12/12 [==============================] - 0s 585us/step - loss: 1.0843 - accuracy: 0.4118\n",
      "Epoch 295/300\n",
      "12/12 [==============================] - 0s 667us/step - loss: 1.0841 - accuracy: 0.4118\n",
      "Epoch 296/300\n",
      "12/12 [==============================] - 0s 502us/step - loss: 1.0864 - accuracy: 0.4118\n",
      "Epoch 297/300\n",
      "12/12 [==============================] - 0s 586us/step - loss: 1.0845 - accuracy: 0.4118\n",
      "Epoch 298/300\n",
      "12/12 [==============================] - 0s 583us/step - loss: 1.0862 - accuracy: 0.4118\n",
      "Epoch 299/300\n",
      "12/12 [==============================] - 0s 669us/step - loss: 1.0854 - accuracy: 0.4118\n",
      "Epoch 300/300\n",
      "12/12 [==============================] - 0s 750us/step - loss: 1.0874 - accuracy: 0.4118\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002904F44C1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.3737 - accuracy: 0.8644\n",
      "\n",
      "Accuracy: 86.44%\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000029051C7A310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.0932 - accuracy: 0.3729\n",
      "\n",
      "Accuracy2: 37.29%\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000290533BC4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 487us/step - loss: 1.0937 - accuracy: 0.3729\n",
      "\n",
      "Accuracy3: 37.29%\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002905371B670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002905371B550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029051C7A0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[2.61392500e-02 1.84928195e-03 9.72011507e-01]\n",
      " [1.06049076e-01 1.44709542e-01 7.49241352e-01]\n",
      " [1.14454573e-03 9.98456717e-01 3.98764183e-04]\n",
      " [9.43152189e-01 6.51249569e-03 5.03352508e-02]\n",
      " [1.76165413e-05 9.99579728e-01 4.02613223e-04]]\n",
      "[[0.32000455 0.26878783 0.41120762]\n",
      " [0.32000455 0.26878783 0.41120762]\n",
      " [0.32000455 0.26878783 0.41120762]\n",
      " [0.32000455 0.26878783 0.41120762]\n",
      " [0.32000455 0.26878783 0.41120762]]\n",
      "[[0.3223921  0.26088637 0.41672155]\n",
      " [0.3223921  0.26088637 0.41672155]\n",
      " [0.3223921  0.26088637 0.41672155]\n",
      " [0.3223921  0.26088637 0.41672155]\n",
      " [0.3223921  0.26088637 0.41672155]]\n"
     ]
    }
   ],
   "source": [
    "#Нейронные сети. Deep Learning\n",
    "#Активационная функция = сумма(Wi*Xi) от числа входов нейрона. \n",
    "#Логистическая функция f(x)=e**x/(1+e**x) или гиперболический тангенс\n",
    "#ReLU функция f(x) = max(0, X) проще, но чуть менее точная и сложнее в добавлении параметров\n",
    "#Подбор архитектуры НС позволяет оптимизировать число нейронов, настроив входной, скрытые, выходной слои. \n",
    "#Сети прямого распространения: в пределах слоя нейроны не связаны, передают только в след. слой, перепрыгивать нельзя\n",
    "#Обучение НС = определение значений ВЕСОВ (внутренних параметров)  каждого соединения. Остальное задается аналитиком заранее\n",
    "#Keras модули: архитектура, входные значения, условия обучения, оценка качества\n",
    "\n",
    "wine['Desired1(3)'].value_counts(normalize=True)\n",
    "\n",
    "#Именуем предикторы и отклик\n",
    "y = wine['Desired1(3)']\n",
    "X = wine.drop('Desired1(3)', axis=1)\n",
    "\n",
    "#расщепление на выборки с указанием объёма тестового множества\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12345, test_size=0.33)\n",
    "\n",
    "#Преобразование в np.array для Keras\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "#Поскольку больше двух классов и они не упорядочены, то разбиваем колонку \"y\" на три, с бинарными значениями\n",
    "y_train_bin = np_utils.to_categorical(y_train)\n",
    "y_test_bin = np_utils.to_categorical(y_test)\n",
    "y_train_bin[0:5]\n",
    "\n",
    "#Метод скорейшего(градиентного) спуска SGD. Улучшение метода: Momentum, Nesterov momentum, Adam и др.\n",
    "#Argmin, правило остановки: число итераций или малое уменьшение функции\n",
    "#Начальная точка(инициализация). Начальные значение д.б. минимальным, ближе к нулю (кроме свободных слагаемых)\n",
    "#График зависимости критерия каач-ва Q от номера итерации. Малая скорость обучения(0.001 и т.д.) дает качество, но идет дольше\n",
    "#Входные значения рекомендуется стандартизовать(снижает риск насыщения) (Xi-Xmin)/(Xmax-Xmin)\n",
    "#Batch - коррекция весов после каждой эпохи. Насыщение - отсутствие коррекций. Gradient clipping - блок от больших поправок\n",
    "\n",
    "#Создаем модели. Для небольших данных можно всего пару слоев в 5-7 нейронов\n",
    "#Инициализация. Присвоение стартовых весов\n",
    "init = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None) #Зерно не указано, контроля над обучением меньше\n",
    "init_2 = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=12345) #Усеченное нормальное распределение. Инициация весов\n",
    "init_3 = initializers.Constant(value = 1e-3) #Инициация свободных членов\n",
    "\n",
    "model = Sequential() #Указываем на тип модели (сеть прямого распространения)\n",
    "model.add(Dense(9, input_dim=13, activation='relu')) #Первый слой, 9 нейронов, входные значения (13 предикторов)\n",
    "model.add(Dense(10, activation='relu', )) #Втоой слой, 10 нейронов\n",
    "model.add(Dense(3, activation='softmax')) #Третий слой, ранжировка софтмаксом для стандартизации и активации. Три выхода\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(9, input_dim=13, activation='relu'))\n",
    "model2.add(Dense(10, activation='relu' ))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(9, input_dim=13, activation='relu', kernel_initializer=init_2, bias_initializer=init_3))\n",
    "model3.add(Dense(10, activation='relu', kernel_initializer=init_2, bias_initializer=init_3 ))\n",
    "model3.add(Dense(3, activation='softmax', kernel_initializer=init_2, bias_initializer=init_3))\n",
    "\n",
    "#Categorical crossentropy (CC) используется для определения вероятности принадлежности объекта к классу (упорядочному)\n",
    "\n",
    "#Компилируем: optimizer(rmsprop или adam), loss function(categorical_crossentropy(классификация) или mse(регрессия)). Точность\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "sgd2 = optimizers.SGD(lr=0.02, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=sgd2, metrics=['accuracy'])\n",
    "\n",
    "sgd3 = optimizers.SGD(lr=0.02, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=sgd3, metrics=['accuracy'])\n",
    "\n",
    "#Обучаем модель: 300 эпох, пропуск 10 элементов до смены весов\n",
    "model.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "model2.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "model3.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "\n",
    "#Проверяем на тестовом множестве\n",
    "scores = model.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "scores2 = model2.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy2: %.2f%%\" % (scores2[1]*100))\n",
    "\n",
    "scores3 = model3.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy3: %.2f%%\" % (scores3[1]*100))\n",
    "\n",
    "\n",
    "#Рассчитываем предикторы\n",
    "predictions = model.predict(X_test)\n",
    "predictions2 = model2.predict(X_test)\n",
    "predictions3 = model3.predict(X_test)\n",
    "#round predictions\n",
    "#rounded = [round(x[0]) for x in predictions]\n",
    "#print(rounded)\n",
    "print(predictions[0:5])\n",
    "print(predictions2[0:5])\n",
    "print(predictions3[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 82332072.0000 - mean_absolute_percentage_error: 57.6390\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 999us/step - loss: 67629448.0000 - mean_absolute_percentage_error: 50.6311\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 57471532.0000 - mean_absolute_percentage_error: 45.3311\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 47555856.0000 - mean_absolute_percentage_error: 40.1855\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 41630244.0000 - mean_absolute_percentage_error: 37.1666\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 37347452.0000 - mean_absolute_percentage_error: 35.6267\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 34635620.0000 - mean_absolute_percentage_error: 34.6937\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 33117012.0000 - mean_absolute_percentage_error: 34.0659\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 599us/step - loss: 31804642.0000 - mean_absolute_percentage_error: 33.6313\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 30793902.0000 - mean_absolute_percentage_error: 33.0751\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 29634014.0000 - mean_absolute_percentage_error: 32.4574\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 28487156.0000 - mean_absolute_percentage_error: 31.8054\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 27214432.0000 - mean_absolute_percentage_error: 31.0344\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 25797904.0000 - mean_absolute_percentage_error: 30.1561\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 24459014.0000 - mean_absolute_percentage_error: 29.2398\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 23089922.0000 - mean_absolute_percentage_error: 28.3581\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 333us/step - loss: 21706870.0000 - mean_absolute_percentage_error: 27.3799\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 20496622.0000 - mean_absolute_percentage_error: 26.4872\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 19249098.0000 - mean_absolute_percentage_error: 25.6127\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 18273020.0000 - mean_absolute_percentage_error: 24.7872\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 17403414.0000 - mean_absolute_percentage_error: 24.1591\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 721us/step - loss: 16622757.0000 - mean_absolute_percentage_error: 23.5413\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 15986988.0000 - mean_absolute_percentage_error: 23.0447\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 15441784.0000 - mean_absolute_percentage_error: 22.6628\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 15051925.0000 - mean_absolute_percentage_error: 22.4022\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 14756794.0000 - mean_absolute_percentage_error: 22.1846\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 14455240.0000 - mean_absolute_percentage_error: 22.0193\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 14290380.0000 - mean_absolute_percentage_error: 21.9529\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 14122922.0000 - mean_absolute_percentage_error: 21.8318\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 13971597.0000 - mean_absolute_percentage_error: 21.7426\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 13851150.0000 - mean_absolute_percentage_error: 21.6712\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 13751735.0000 - mean_absolute_percentage_error: 21.6231\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 13643939.0000 - mean_absolute_percentage_error: 21.5931\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 13543951.0000 - mean_absolute_percentage_error: 21.5416\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 13466306.0000 - mean_absolute_percentage_error: 21.5022\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 13374065.0000 - mean_absolute_percentage_error: 21.4480\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 13303307.0000 - mean_absolute_percentage_error: 21.3927\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 13217799.0000 - mean_absolute_percentage_error: 21.3420\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 13127250.0000 - mean_absolute_percentage_error: 21.2710\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 13051734.0000 - mean_absolute_percentage_error: 21.2032\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 12959096.0000 - mean_absolute_percentage_error: 21.1156\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 12890015.0000 - mean_absolute_percentage_error: 21.0503\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 12800007.0000 - mean_absolute_percentage_error: 20.9752\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 12728753.0000 - mean_absolute_percentage_error: 20.9117\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 12660040.0000 - mean_absolute_percentage_error: 20.8497\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 12583151.0000 - mean_absolute_percentage_error: 20.7900\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 12513851.0000 - mean_absolute_percentage_error: 20.7349\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 12441224.0000 - mean_absolute_percentage_error: 20.6694\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 12359179.0000 - mean_absolute_percentage_error: 20.6037\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 676us/step - loss: 12292009.0000 - mean_absolute_percentage_error: 20.5573\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 12220121.0000 - mean_absolute_percentage_error: 20.5017\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 12147452.0000 - mean_absolute_percentage_error: 20.4491\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 12075540.0000 - mean_absolute_percentage_error: 20.3897\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 12005057.0000 - mean_absolute_percentage_error: 20.3317\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 991us/step - loss: 11938642.0000 - mean_absolute_percentage_error: 20.2847\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 11860504.0000 - mean_absolute_percentage_error: 20.2211\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 11786222.0000 - mean_absolute_percentage_error: 20.1486\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 11716106.0000 - mean_absolute_percentage_error: 20.0899\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 11651730.0000 - mean_absolute_percentage_error: 20.0326\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 11575895.0000 - mean_absolute_percentage_error: 19.9765\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 11502676.0000 - mean_absolute_percentage_error: 19.9103\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 11431769.0000 - mean_absolute_percentage_error: 19.8490\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 11363926.0000 - mean_absolute_percentage_error: 19.7920\n",
      "Epoch 64/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1000us/step - loss: 11290487.0000 - mean_absolute_percentage_error: 19.7325\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 11221243.0000 - mean_absolute_percentage_error: 19.6685\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 11148555.0000 - mean_absolute_percentage_error: 19.6006\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 11078321.0000 - mean_absolute_percentage_error: 19.5381\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 11013134.0000 - mean_absolute_percentage_error: 19.4793\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 10941378.0000 - mean_absolute_percentage_error: 19.4090\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 10880294.0000 - mean_absolute_percentage_error: 19.3533\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 10803645.0000 - mean_absolute_percentage_error: 19.2949\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 10744814.0000 - mean_absolute_percentage_error: 19.2439\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 10670396.0000 - mean_absolute_percentage_error: 19.1798\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 10605793.0000 - mean_absolute_percentage_error: 19.1188\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 10540136.0000 - mean_absolute_percentage_error: 19.0598\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 10475716.0000 - mean_absolute_percentage_error: 18.9937\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 838us/step - loss: 10408051.0000 - mean_absolute_percentage_error: 18.9407\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 10340337.0000 - mean_absolute_percentage_error: 18.8831\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 10278390.0000 - mean_absolute_percentage_error: 18.8232\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 837us/step - loss: 10210463.0000 - mean_absolute_percentage_error: 18.7615\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 10145764.0000 - mean_absolute_percentage_error: 18.7017\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 10080408.0000 - mean_absolute_percentage_error: 18.6461\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 10015909.0000 - mean_absolute_percentage_error: 18.5858\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9948082.0000 - mean_absolute_percentage_error: 18.5168\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 9887455.0000 - mean_absolute_percentage_error: 18.4575\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 927us/step - loss: 9825253.0000 - mean_absolute_percentage_error: 18.3987\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 9763038.0000 - mean_absolute_percentage_error: 18.3379\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 9697106.0000 - mean_absolute_percentage_error: 18.2734\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9634892.0000 - mean_absolute_percentage_error: 18.2167\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9578400.0000 - mean_absolute_percentage_error: 18.1649\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 9508766.0000 - mean_absolute_percentage_error: 18.0927\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9445677.0000 - mean_absolute_percentage_error: 18.0386\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 9391186.0000 - mean_absolute_percentage_error: 17.9926\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 9336295.0000 - mean_absolute_percentage_error: 17.9449\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 9268376.0000 - mean_absolute_percentage_error: 17.8813\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 9201573.0000 - mean_absolute_percentage_error: 17.8054\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 9143520.0000 - mean_absolute_percentage_error: 17.7475\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 9085322.0000 - mean_absolute_percentage_error: 17.6918\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9029850.0000 - mean_absolute_percentage_error: 17.6336\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 668us/step - loss: 8972787.0000 - mean_absolute_percentage_error: 17.5759\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8905867.0000 - mean_absolute_percentage_error: 17.5090\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8860955.0000 - mean_absolute_percentage_error: 17.4518\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 8788709.0000 - mean_absolute_percentage_error: 17.3859\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8735007.0000 - mean_absolute_percentage_error: 17.3370\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 8684150.0000 - mean_absolute_percentage_error: 17.2775\n",
      "Epoch 106/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8629554.0000 - mean_absolute_percentage_error: 17.2306\n",
      "Epoch 107/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 8565503.0000 - mean_absolute_percentage_error: 17.1700\n",
      "Epoch 108/300\n",
      "3/3 [==============================] - 0s 658us/step - loss: 8509511.0000 - mean_absolute_percentage_error: 17.1107\n",
      "Epoch 109/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8451867.0000 - mean_absolute_percentage_error: 17.0500\n",
      "Epoch 110/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 8397021.0000 - mean_absolute_percentage_error: 16.9954\n",
      "Epoch 111/300\n",
      "3/3 [==============================] - 0s 999us/step - loss: 8336787.5000 - mean_absolute_percentage_error: 16.9362\n",
      "Epoch 112/300\n",
      "3/3 [==============================] - 0s 334us/step - loss: 8289211.0000 - mean_absolute_percentage_error: 16.8869\n",
      "Epoch 113/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 8229886.5000 - mean_absolute_percentage_error: 16.8217\n",
      "Epoch 114/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8175472.5000 - mean_absolute_percentage_error: 16.7645\n",
      "Epoch 115/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 8121916.5000 - mean_absolute_percentage_error: 16.7074\n",
      "Epoch 116/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 8067638.0000 - mean_absolute_percentage_error: 16.6501\n",
      "Epoch 117/300\n",
      "3/3 [==============================] - 0s 682us/step - loss: 8015893.0000 - mean_absolute_percentage_error: 16.5925\n",
      "Epoch 118/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7971719.0000 - mean_absolute_percentage_error: 16.5403\n",
      "Epoch 119/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7915624.0000 - mean_absolute_percentage_error: 16.4741\n",
      "Epoch 120/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7860048.0000 - mean_absolute_percentage_error: 16.4192\n",
      "Epoch 121/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7808495.0000 - mean_absolute_percentage_error: 16.3647\n",
      "Epoch 122/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 7758988.5000 - mean_absolute_percentage_error: 16.3132\n",
      "Epoch 123/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7711336.0000 - mean_absolute_percentage_error: 16.2618\n",
      "Epoch 124/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7656822.0000 - mean_absolute_percentage_error: 16.1975\n",
      "Epoch 125/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7611127.0000 - mean_absolute_percentage_error: 16.1472\n",
      "Epoch 126/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7556267.5000 - mean_absolute_percentage_error: 16.0889\n",
      "Epoch 127/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 7519749.5000 - mean_absolute_percentage_error: 16.0444\n",
      "Epoch 128/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7466271.0000 - mean_absolute_percentage_error: 15.9859\n",
      "Epoch 129/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7409750.5000 - mean_absolute_percentage_error: 15.9228\n",
      "Epoch 130/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7362217.0000 - mean_absolute_percentage_error: 15.8692\n",
      "Epoch 131/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7318487.0000 - mean_absolute_percentage_error: 15.8203\n",
      "Epoch 132/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 7265914.5000 - mean_absolute_percentage_error: 15.7522\n",
      "Epoch 133/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 7222526.5000 - mean_absolute_percentage_error: 15.7043\n",
      "Epoch 134/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 7170160.0000 - mean_absolute_percentage_error: 15.6433\n",
      "Epoch 135/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7125248.0000 - mean_absolute_percentage_error: 15.5918\n",
      "Epoch 136/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7081887.0000 - mean_absolute_percentage_error: 15.5390\n",
      "Epoch 137/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 7041366.0000 - mean_absolute_percentage_error: 15.4910\n",
      "Epoch 138/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 6994280.0000 - mean_absolute_percentage_error: 15.4324\n",
      "Epoch 139/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 6941119.5000 - mean_absolute_percentage_error: 15.3676\n",
      "Epoch 140/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 6324799.0000 - mean_absolute_percentage_error: 13.777 - 0s 667us/step - loss: 6900215.5000 - mean_absolute_percentage_error: 15.3145\n",
      "Epoch 141/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6857938.5000 - mean_absolute_percentage_error: 15.2551\n",
      "Epoch 142/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6813054.5000 - mean_absolute_percentage_error: 15.2048\n",
      "Epoch 143/300\n",
      "3/3 [==============================] - 0s 999us/step - loss: 6771819.0000 - mean_absolute_percentage_error: 15.1451\n",
      "Epoch 144/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6725822.5000 - mean_absolute_percentage_error: 15.0977\n",
      "Epoch 145/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 6687914.0000 - mean_absolute_percentage_error: 15.0504\n",
      "Epoch 146/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6648792.5000 - mean_absolute_percentage_error: 15.0013\n",
      "Epoch 147/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 6599754.0000 - mean_absolute_percentage_error: 14.9443\n",
      "Epoch 148/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6557511.5000 - mean_absolute_percentage_error: 14.8937\n",
      "Epoch 149/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 6515253.5000 - mean_absolute_percentage_error: 14.8409\n",
      "Epoch 150/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6477661.5000 - mean_absolute_percentage_error: 14.7883\n",
      "Epoch 151/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 6434610.0000 - mean_absolute_percentage_error: 14.7390\n",
      "Epoch 152/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 6395569.0000 - mean_absolute_percentage_error: 14.6921\n",
      "Epoch 153/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 6356263.5000 - mean_absolute_percentage_error: 14.6445\n",
      "Epoch 154/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 6316819.0000 - mean_absolute_percentage_error: 14.5930\n",
      "Epoch 155/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 6274944.5000 - mean_absolute_percentage_error: 14.5502\n",
      "Epoch 156/300\n",
      "3/3 [==============================] - 0s 992us/step - loss: 6237320.5000 - mean_absolute_percentage_error: 14.5086\n",
      "Epoch 157/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6200731.5000 - mean_absolute_percentage_error: 14.4638\n",
      "Epoch 158/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6159765.0000 - mean_absolute_percentage_error: 14.4150\n",
      "Epoch 159/300\n",
      "3/3 [==============================] - 0s 663us/step - loss: 6123679.0000 - mean_absolute_percentage_error: 14.3676\n",
      "Epoch 160/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6103823.0000 - mean_absolute_percentage_error: 14.3411\n",
      "Epoch 161/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 6047957.0000 - mean_absolute_percentage_error: 14.2777\n",
      "Epoch 162/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 6016697.0000 - mean_absolute_percentage_error: 14.2345\n",
      "Epoch 163/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5976211.5000 - mean_absolute_percentage_error: 14.1877\n",
      "Epoch 164/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5942305.0000 - mean_absolute_percentage_error: 14.1437\n",
      "Epoch 165/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5901593.5000 - mean_absolute_percentage_error: 14.0949\n",
      "Epoch 166/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5869926.0000 - mean_absolute_percentage_error: 14.0481\n",
      "Epoch 167/300\n",
      "3/3 [==============================] - 0s 676us/step - loss: 5832790.0000 - mean_absolute_percentage_error: 14.0020\n",
      "Epoch 168/300\n",
      "3/3 [==============================] - 0s 333us/step - loss: 5805727.5000 - mean_absolute_percentage_error: 13.9690\n",
      "Epoch 169/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5771484.5000 - mean_absolute_percentage_error: 13.9194\n",
      "Epoch 170/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5731857.0000 - mean_absolute_percentage_error: 13.8698\n",
      "Epoch 171/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5715265.0000 - mean_absolute_percentage_error: 13.8452\n",
      "Epoch 172/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5665859.5000 - mean_absolute_percentage_error: 13.7842\n",
      "Epoch 173/300\n",
      "3/3 [==============================] - 0s 837us/step - loss: 5635281.0000 - mean_absolute_percentage_error: 13.7401\n",
      "Epoch 174/300\n",
      "3/3 [==============================] - 0s 675us/step - loss: 5599204.0000 - mean_absolute_percentage_error: 13.6959\n",
      "Epoch 175/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5566848.0000 - mean_absolute_percentage_error: 13.6503\n",
      "Epoch 176/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5536239.0000 - mean_absolute_percentage_error: 13.6087\n",
      "Epoch 177/300\n",
      "3/3 [==============================] - 0s 998us/step - loss: 5506757.5000 - mean_absolute_percentage_error: 13.5701\n",
      "Epoch 178/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5475532.5000 - mean_absolute_percentage_error: 13.5217\n",
      "Epoch 179/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5440535.0000 - mean_absolute_percentage_error: 13.4741\n",
      "Epoch 180/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5413110.0000 - mean_absolute_percentage_error: 13.4366\n",
      "Epoch 181/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5385664.5000 - mean_absolute_percentage_error: 13.3991\n",
      "Epoch 182/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5351527.5000 - mean_absolute_percentage_error: 13.3483\n",
      "Epoch 183/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5321524.0000 - mean_absolute_percentage_error: 13.3084\n",
      "Epoch 184/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 5290274.0000 - mean_absolute_percentage_error: 13.2668\n",
      "Epoch 185/300\n",
      "3/3 [==============================] - 0s 999us/step - loss: 5263153.0000 - mean_absolute_percentage_error: 13.2298\n",
      "Epoch 186/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5234519.0000 - mean_absolute_percentage_error: 13.1920\n",
      "Epoch 187/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5208259.0000 - mean_absolute_percentage_error: 13.1549\n",
      "Epoch 188/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5179492.5000 - mean_absolute_percentage_error: 13.1170\n",
      "Epoch 189/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 666us/step - loss: 5151939.0000 - mean_absolute_percentage_error: 13.0822\n",
      "Epoch 190/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5121201.0000 - mean_absolute_percentage_error: 13.0381\n",
      "Epoch 191/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5095197.5000 - mean_absolute_percentage_error: 12.9939\n",
      "Epoch 192/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 5067539.5000 - mean_absolute_percentage_error: 12.9535\n",
      "Epoch 193/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5052824.5000 - mean_absolute_percentage_error: 12.9234\n",
      "Epoch 194/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 5013452.5000 - mean_absolute_percentage_error: 12.8704\n",
      "Epoch 195/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4986039.0000 - mean_absolute_percentage_error: 12.8419\n",
      "Epoch 196/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 4960609.5000 - mean_absolute_percentage_error: 12.8062\n",
      "Epoch 197/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4942370.0000 - mean_absolute_percentage_error: 12.7862\n",
      "Epoch 198/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4911218.5000 - mean_absolute_percentage_error: 12.7439\n",
      "Epoch 199/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4888162.0000 - mean_absolute_percentage_error: 12.7110\n",
      "Epoch 200/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4864839.5000 - mean_absolute_percentage_error: 12.6726\n",
      "Epoch 201/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4832176.0000 - mean_absolute_percentage_error: 12.6153\n",
      "Epoch 202/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4812186.0000 - mean_absolute_percentage_error: 12.5848\n",
      "Epoch 203/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4787764.0000 - mean_absolute_percentage_error: 12.5425\n",
      "Epoch 204/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4765418.5000 - mean_absolute_percentage_error: 12.5175\n",
      "Epoch 205/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4738073.5000 - mean_absolute_percentage_error: 12.4761\n",
      "Epoch 206/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4717124.0000 - mean_absolute_percentage_error: 12.4433\n",
      "Epoch 207/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4697225.0000 - mean_absolute_percentage_error: 12.4131\n",
      "Epoch 208/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4672163.5000 - mean_absolute_percentage_error: 12.3800\n",
      "Epoch 209/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4647913.0000 - mean_absolute_percentage_error: 12.3529\n",
      "Epoch 210/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4630770.5000 - mean_absolute_percentage_error: 12.3252\n",
      "Epoch 211/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4603812.0000 - mean_absolute_percentage_error: 12.2839\n",
      "Epoch 212/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4584352.5000 - mean_absolute_percentage_error: 12.2629\n",
      "Epoch 213/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4562662.5000 - mean_absolute_percentage_error: 12.2399\n",
      "Epoch 214/300\n",
      "3/3 [==============================] - 0s 837us/step - loss: 4541213.0000 - mean_absolute_percentage_error: 12.2213\n",
      "Epoch 215/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4521778.5000 - mean_absolute_percentage_error: 12.1964\n",
      "Epoch 216/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4500311.5000 - mean_absolute_percentage_error: 12.1577\n",
      "Epoch 217/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4480480.5000 - mean_absolute_percentage_error: 12.1273\n",
      "Epoch 218/300\n",
      "3/3 [==============================] - 0s 995us/step - loss: 4457227.5000 - mean_absolute_percentage_error: 12.0791\n",
      "Epoch 219/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4438330.0000 - mean_absolute_percentage_error: 12.0393\n",
      "Epoch 220/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4421835.5000 - mean_absolute_percentage_error: 11.9993\n",
      "Epoch 221/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4403141.0000 - mean_absolute_percentage_error: 11.9593\n",
      "Epoch 222/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4382367.0000 - mean_absolute_percentage_error: 11.9294\n",
      "Epoch 223/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4371372.5000 - mean_absolute_percentage_error: 11.9087\n",
      "Epoch 224/300\n",
      "3/3 [==============================] - 0s 674us/step - loss: 4345582.5000 - mean_absolute_percentage_error: 11.8715\n",
      "Epoch 225/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4323834.5000 - mean_absolute_percentage_error: 11.8552\n",
      "Epoch 226/300\n",
      "3/3 [==============================] - 0s 334us/step - loss: 4319551.5000 - mean_absolute_percentage_error: 11.8638\n",
      "Epoch 227/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4290578.5000 - mean_absolute_percentage_error: 11.8311\n",
      "Epoch 228/300\n",
      "3/3 [==============================] - 0s 668us/step - loss: 4272020.5000 - mean_absolute_percentage_error: 11.7968\n",
      "Epoch 229/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4254519.0000 - mean_absolute_percentage_error: 11.7660\n",
      "Epoch 230/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4236803.0000 - mean_absolute_percentage_error: 11.7287\n",
      "Epoch 231/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4223127.0000 - mean_absolute_percentage_error: 11.6944\n",
      "Epoch 232/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4205003.5000 - mean_absolute_percentage_error: 11.6655\n",
      "Epoch 233/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4186484.7500 - mean_absolute_percentage_error: 11.6398\n",
      "Epoch 234/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4174309.7500 - mean_absolute_percentage_error: 11.6038\n",
      "Epoch 235/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4153728.7500 - mean_absolute_percentage_error: 11.5666\n",
      "Epoch 236/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4143759.5000 - mean_absolute_percentage_error: 11.5514\n",
      "Epoch 237/300\n",
      "3/3 [==============================] - 0s 674us/step - loss: 4124933.2500 - mean_absolute_percentage_error: 11.5292\n",
      "Epoch 238/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4106206.2500 - mean_absolute_percentage_error: 11.4998\n",
      "Epoch 239/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4089408.7500 - mean_absolute_percentage_error: 11.4680\n",
      "Epoch 240/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4075593.5000 - mean_absolute_percentage_error: 11.4353\n",
      "Epoch 241/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4058858.5000 - mean_absolute_percentage_error: 11.4013\n",
      "Epoch 242/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4047420.5000 - mean_absolute_percentage_error: 11.3777\n",
      "Epoch 243/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 4030776.2500 - mean_absolute_percentage_error: 11.3494\n",
      "Epoch 244/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 4018042.2500 - mean_absolute_percentage_error: 11.3283\n",
      "Epoch 245/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 4003485.5000 - mean_absolute_percentage_error: 11.3164\n",
      "Epoch 246/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 3989985.5000 - mean_absolute_percentage_error: 11.2957\n",
      "Epoch 247/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3975714.5000 - mean_absolute_percentage_error: 11.2788\n",
      "Epoch 248/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3961666.0000 - mean_absolute_percentage_error: 11.2506\n",
      "Epoch 249/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3948774.0000 - mean_absolute_percentage_error: 11.2274\n",
      "Epoch 250/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3934534.7500 - mean_absolute_percentage_error: 11.1999\n",
      "Epoch 251/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3921588.0000 - mean_absolute_percentage_error: 11.1774\n",
      "Epoch 252/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 667us/step - loss: 3913987.5000 - mean_absolute_percentage_error: 11.1665\n",
      "Epoch 253/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3896949.5000 - mean_absolute_percentage_error: 11.1422\n",
      "Epoch 254/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3882766.0000 - mean_absolute_percentage_error: 11.1007\n",
      "Epoch 255/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3870155.5000 - mean_absolute_percentage_error: 11.0715\n",
      "Epoch 256/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3857847.5000 - mean_absolute_percentage_error: 11.0578\n",
      "Epoch 257/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 3844594.0000 - mean_absolute_percentage_error: 11.0491\n",
      "Epoch 258/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 3831598.5000 - mean_absolute_percentage_error: 11.0373\n",
      "Epoch 259/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3819849.7500 - mean_absolute_percentage_error: 11.0224\n",
      "Epoch 260/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3809088.7500 - mean_absolute_percentage_error: 11.0051\n",
      "Epoch 261/300\n",
      "3/3 [==============================] - 0s 333us/step - loss: 3799628.2500 - mean_absolute_percentage_error: 11.0010\n",
      "Epoch 262/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3793117.5000 - mean_absolute_percentage_error: 11.0019\n",
      "Epoch 263/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3778538.2500 - mean_absolute_percentage_error: 10.9813\n",
      "Epoch 264/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 3762262.2500 - mean_absolute_percentage_error: 10.9529\n",
      "Epoch 265/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3754792.0000 - mean_absolute_percentage_error: 10.9378\n",
      "Epoch 266/300\n",
      "3/3 [==============================] - 0s 696us/step - loss: 3741527.2500 - mean_absolute_percentage_error: 10.9140\n",
      "Epoch 267/300\n",
      "3/3 [==============================] - 0s 995us/step - loss: 3731821.7500 - mean_absolute_percentage_error: 10.8898\n",
      "Epoch 268/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3718831.5000 - mean_absolute_percentage_error: 10.8601\n",
      "Epoch 269/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3710553.7500 - mean_absolute_percentage_error: 10.8397\n",
      "Epoch 270/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 3701209.2500 - mean_absolute_percentage_error: 10.8159\n",
      "Epoch 271/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3692428.7500 - mean_absolute_percentage_error: 10.8017\n",
      "Epoch 272/300\n",
      "3/3 [==============================] - 0s 999us/step - loss: 3681625.2500 - mean_absolute_percentage_error: 10.7839\n",
      "Epoch 273/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3671238.7500 - mean_absolute_percentage_error: 10.7778\n",
      "Epoch 274/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 3660500.0000 - mean_absolute_percentage_error: 10.7768\n",
      "Epoch 275/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3649646.5000 - mean_absolute_percentage_error: 10.7689\n",
      "Epoch 276/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3640750.5000 - mean_absolute_percentage_error: 10.7687\n",
      "Epoch 277/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 3635818.2500 - mean_absolute_percentage_error: 10.7639\n",
      "Epoch 278/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3625356.0000 - mean_absolute_percentage_error: 10.7592\n",
      "Epoch 279/300\n",
      "3/3 [==============================] - 0s 556us/step - loss: 3619506.7500 - mean_absolute_percentage_error: 10.7512\n",
      "Epoch 280/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 3609513.2500 - mean_absolute_percentage_error: 10.7359\n",
      "Epoch 281/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3597647.7500 - mean_absolute_percentage_error: 10.7149\n",
      "Epoch 282/300\n",
      "3/3 [==============================] - 0s 666us/step - loss: 3585596.5000 - mean_absolute_percentage_error: 10.6637\n",
      "Epoch 283/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3581398.2500 - mean_absolute_percentage_error: 10.6460\n",
      "Epoch 284/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3573647.7500 - mean_absolute_percentage_error: 10.6246\n",
      "Epoch 285/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3572590.5000 - mean_absolute_percentage_error: 10.6221\n",
      "Epoch 286/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3557017.2500 - mean_absolute_percentage_error: 10.5930\n",
      "Epoch 287/300\n",
      "3/3 [==============================] - 0s 673us/step - loss: 3547341.7500 - mean_absolute_percentage_error: 10.5927\n",
      "Epoch 288/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3539719.0000 - mean_absolute_percentage_error: 10.5979\n",
      "Epoch 289/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3531563.2500 - mean_absolute_percentage_error: 10.6043\n",
      "Epoch 290/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 3523897.7500 - mean_absolute_percentage_error: 10.5932\n",
      "Epoch 291/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3529744.2500 - mean_absolute_percentage_error: 10.5747\n",
      "Epoch 292/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 3507610.0000 - mean_absolute_percentage_error: 10.5579\n",
      "Epoch 293/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 3501010.7500 - mean_absolute_percentage_error: 10.5501\n",
      "Epoch 294/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3493939.7500 - mean_absolute_percentage_error: 10.5431\n",
      "Epoch 295/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3490102.7500 - mean_absolute_percentage_error: 10.5401\n",
      "Epoch 296/300\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3484192.2500 - mean_absolute_percentage_error: 10.5310\n",
      "Epoch 297/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3471127.2500 - mean_absolute_percentage_error: 10.5077\n",
      "Epoch 298/300\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 3465624.2500 - mean_absolute_percentage_error: 10.4983\n",
      "Epoch 299/300\n",
      "3/3 [==============================] - 0s 667us/step - loss: 3460873.5000 - mean_absolute_percentage_error: 10.4899\n",
      "Epoch 300/300\n",
      "3/3 [==============================] - 0s 999us/step - loss: 3451681.5000 - mean_absolute_percentage_error: 10.4673\n",
      "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x0000029053649280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 9479204.0000 - mean_absolute_percentage_error: 15.7532\n",
      "\n",
      "MAPE: 15.75%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x290521c3f40>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Нейронные сети: Прогнозирование\n",
    "#Уменьшение накапливаемости ошибки м.б. только с параллельным обучением нескольких сетей (и со снижением качества в целом)\n",
    "#Наиболее важны самые недавние (или самые высокоранжированные) наблюдения. Их оптимально ставить в тестовую\n",
    "#MSE, MAE или Mean absolute percentage error (MAPE) - показывают относительную ошибку в регрессии\n",
    "#MAPE = 1/n *сумма(Yi-Yпрогноз)/Yi*100%\n",
    "\n",
    "\n",
    "sales = pd.read_csv('monthly-car-sales-in-quebec-1960.csv', sep=';', header=0, parse_dates=[0])\n",
    "\n",
    "#Преобразуем данные\n",
    "sales_2 = pd.DataFrame()\n",
    "\n",
    "for i in range(12,0,-1):\n",
    "    sales_2['t-'+str(i)] = sales.iloc[:,1].shift(i) #Новые колонки, где значения сдвинуты с обратным временным шагом(помесячно)\n",
    "\n",
    "sales_2['t'] = sales.iloc[:,1].values #Дублируем первоначальную колонку\n",
    "sales_4 = sales_2[12:] #Отрезаем первые 12 строк\n",
    "\n",
    "#Задаем предикторы и отклик\n",
    "y = sales_4['t']\n",
    "X = sales_4.drop('t', axis=1)\n",
    "\n",
    "#Разделяем на обучающую и тестовую. Тестовая - последние наблюдения\n",
    "X_train = X[:91]\n",
    "y_train = y[:91]\n",
    "X_test  = X[91:]\n",
    "y_test  = y[91:]\n",
    "\n",
    "#Преобразование в np.array\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "#Создаем, компилируем и обучаем модель\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=None)\n",
    "\n",
    "#оценка качества модели на тестовом множестве\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nMAPE: %.2f%%\" % (scores[1]))\n",
    "\n",
    "# Вычисляем прогноз\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Вычисляем подгонку\n",
    "predictions_train = model.predict(X_train)\n",
    "\n",
    "#График с результатами numpy.arange(start, stop, step, dtype=None)\n",
    "x2 = np.arange(0, 91, 1)\n",
    "x3 = np.arange(91, 96, 1)\n",
    "\n",
    "plt.plot(x2, y_train, color='blue')\n",
    "plt.plot(x2, predictions_train, color='green')\n",
    "plt.plot(x3, y_test, color='blue')\n",
    "plt.plot(x3, predictions, color='red') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вычисление статистической погрешности для случайной выборки\n",
    "\n",
    "#Расчет объема выборки\n",
    "N = 40000 #Генеральная совокупность\n",
    "P = 0.95 #Доверительный уровень в 95% \n",
    "Z = 1.96 #коэффициент доверительного уровня (p = 95%, Z=1,96)(p=99%,   Z=2,58)\n",
    "p = 0.5 #доля респондентов с  наличием исследуемого признака,\n",
    "q = (1 - p) #доля респондентов, у которых исследуемый признак отсутствует,\n",
    "delta = 0.05 #Задаваемая предельная ошибка выборки.\n",
    "n = (Z**2)*p*q/delta**2 #объем выборки\n",
    "\n",
    "print(\"Рекомендуемый объем выборки для данной аудитории:\", int(n), \"человек\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Расчет ошибки выбоки для доли признака\n",
    "#Случай 1. Генеральная совокупность значительно больше выборки\n",
    "n = 384 #Объем выборки\n",
    "m = 276 #Число объектов выборки с нужными параметрами (True)\n",
    "p = m/n #Вероятность на основе практических данных\n",
    "sigma = n/2*((p*(1-p)/n*(1-n/N)))**0.5 \n",
    "print('Результат выборки один составит: ', \\\n",
    "      float(\"{0:.1f}\".format(p*100)), \"±\", float(\"{0:.1f}\".format(sigma)), \"%\")\n",
    "\n",
    "#Случай 2. Генеральная совокупность сопоставима с объемом выборки\n",
    "N = 2500\n",
    "delta = Z*((p*q/n)*((N-n)/(N-1)))**0.5 \n",
    "print(\"Точность результатов выборки два составит: \", \"±\", float(\"{0:.1f}\".format(delta*100)), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Рассчет доверительного интервала\n",
    "P = 0.99 #Доверительный уровень в 99% \n",
    "Z = 2.58 #коэффициент доверительного уровня \n",
    "p = 0.2 #доля респондентов с наличием исследуемого признака,\n",
    "q = (1 - p) #доля респондентов, у которых исследуемый признак отсутствует,\n",
    "n = 1000 #Объем выборки\n",
    "\n",
    "sigma = Z*(p*q/n)**0.5 #Погрешность оценки\n",
    "\n",
    "print('Точность результатов конкретной выборки составит: ±', float(\"{0:.2f}\".format(sigma*100)), \"%\")\n",
    "print('Доверительный интервал составит:', float(\"{0:.2f}\".format((p - sigma)*100)), \"% ;\", \\\n",
    "                                            float(\"{0:.2f}\".format((p + sigma)*100)), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ненормальные распределения\n",
    "#Сгладить распределение, уменьшив шкалу на основании полезности данных, удалив аномалии\n",
    "#Логарифмировать переменные (не забываем про ноль в исходной переменной). Схлопывает экстремальные значения\n",
    "#Логарифмирование отлично работает с ассиметричными распределениями\n",
    "#Если логарифмы переменных зависимы линейно, то значит сами переменные зависят нелинейно \n",
    "#Применить непараметрический критерий (ранговое распределение)\n",
    "#Преобразование Бокса-Кокса подбирает оптимальную степень для возведения в нее mathworks.com/help/finance/boxcox.html \n",
    "#Bootstrap и метод Монте-Карло. Сравнивать медиану, мин, макс, 13-процентиль, среднее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Проверка гипотезы о нормальности распределения.<br>\n",
    "H0: $X \\sim N(\\cdot, \\cdot)$<br>\n",
    "H1: $X \\nsim N(\\cdot, \\cdot)$<br>\n",
    "Критерий Шапиро-Уилка [scipy.stats.shapiro](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.shapiro.html).<br>\n",
    "\n",
    "\n",
    "* Критерий согласия Стьюдента.<br>\n",
    "H0: $\\mu = M$<br>\n",
    "H1: $\\mu \\ne M$<br>\n",
    "[scipy.stats.ttest_1samp](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_1samp.html).<br>\n",
    "\n",
    "\n",
    "* Проверка гипотезы о равенстве средних значений.<br>\n",
    "H0: $\\mu_1 = \\mu_2$<br>\n",
    "H1: $\\mu_1 \\ne \\mu_2$<br>\n",
    "Распределение выборок должно быть близко к нормальному.<br>\n",
    "  * Для несвязных выборок: [scipy.stats.ttest_ind](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_ind.html).<br>\n",
    "  * Для связных выборок: [scipy.stats.ttest_rel](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html).<br>\n",
    "  \n",
    "\n",
    "* Проверка гипотезы о равенстве медиан.<br>\n",
    "  * Для несвязных выборок: критерий Манна-Уитни [scipy.stats.mannwhitneyu](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html).<br>\n",
    "  * Для связных выборок: критерий Уилкоксона [scipy.stats.wilcoxon](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html).<br>\n",
    "  * Критерий Муда [scipy.stats.median_test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.median_test.html).<br>\n",
    "\n",
    "\n",
    "* Проверка гипотезы о равенстве дисперсий.<br>\n",
    "H0: $\\sigma_1 = \\sigma_2$<br>\n",
    "H1: $\\sigma_1 \\neq \\sigma_2$<br>\n",
    "Критерий Флингера-Килина [scipy.stats.fligner](https://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.stats.fligner.html).<br>\n",
    "\n",
    "\n",
    "* Проверка гипотезы о равенстве долей категориального признака.<br>\n",
    "H0: $p_1 = p_2$<br>\n",
    "H1: $p_1 \\ne p_2$<br>\n",
    "Критерий хи-квадрат [scipy.stats.chi2_contingency](https://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.stats.chi2_contingency.html).<br>\n",
    "\n",
    "\n",
    "* Проверка гипотезы о независимости (корреляция).<br>\n",
    "H0: X и Y независимы<br>\n",
    "H1: X и Y зависимы<br>\n",
    "  * Для непрерыных величин: корреляция Пирсона [scipy.stats.pearsonr](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html),<br>\n",
    "  * Для дискретных величин: корреляция Спирмэна [scipy.stats.kendalltau](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.kendalltau.html).<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
