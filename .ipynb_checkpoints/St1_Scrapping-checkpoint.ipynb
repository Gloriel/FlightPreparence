{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Полезные ссылки<br>\n",
    "  * [Web Scraping](https://coderlessons.com/tutorials/python-technologies/izuchite-python-web-scraping/python-web-scraping-kratkoe-rukovodstvo)<br>\n",
    "  * [Python - Regular Expressions](https://www.tutorialspoint.com/python/python_reg_expressions.htm)<br>\n",
    "  * [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)<br>\n",
    "  * [AWS Amazon](https://aws.amazon.com/ru/getting-started/)<br> \n",
    "  * [Список парсеров](https://habr.com/ru/company/click/blog/494020/)<br> \n",
    "  * [Конкурентный анализ](https://blog.click.ru/semantics/kak-besplatno-sobrat-klyuchevye-slova-i-obyavleniya-konkurentov/)\n",
    "  * [ВКшка](https://vc.ru/marketing/138412-instrukciya-kak-ispolzovat-parsing-vkontakte)\n",
    "  * [Soup](https://pythonru.com/biblioteki/parsing-na-python-s-beautiful-soup)\n",
    "  * [Scrapy](https://pythonru.com/biblioteki/sozdanie-parserov-s-pomoshhju-scrapy-i-python)\n",
    "  * [Библиотека Супа](https://www.crummy.com/software/BeautifulSoup/bs4/doc.ru/)\n",
    "  * [Парсинг Хабр](https://habr.com/ru/post/504900/)\n",
    "  \n",
    "«Inspect» (CTRL+SHIFT+I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T09:01:13.063981Z",
     "start_time": "2021-03-05T09:01:12.507978Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import lxml\n",
    "from lxml import html \n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import boto3\n",
    "from urllib.request import urlopen\n",
    "import datetime\n",
    "import random\n",
    "import sqlite3\n",
    "import pymysql\n",
    "import os\n",
    "import webbrowser\n",
    "import json\n",
    "import re\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T08:47:19.631059Z",
     "start_time": "2021-03-05T08:47:19.623034Z"
    }
   },
   "outputs": [],
   "source": [
    "def studio_list():\n",
    "\n",
    "    url = 'https://ugoloc.ru/studios/list.json'\n",
    "    json_data = urllib.request.urlopen(url).read().decode()\n",
    "\n",
    "    id = list()\n",
    "    name = list()\n",
    "    metro = list()\n",
    "    address = list()\n",
    "    phone = list()\n",
    "    email = list()\n",
    "\n",
    "    for i in range(len(json.loads(json_data)['features'])):\n",
    "        id.append(json.loads(json_data)['features'][i]['studio']['id'])\n",
    "        name.append(json.loads(json_data)['features'][i]['studio']['name'])\n",
    "        metro.append(json.loads(json_data)['features'][i]['studio']['metro'])\n",
    "        address.append(json.loads(json_data)[\n",
    "                       'features'][i]['studio']['address'])\n",
    "        phone.append(json.loads(json_data)['features'][i]['studio']['phone'])\n",
    "        email.append(json.loads(json_data)['features'][i]['studio']['email'])\n",
    "\n",
    "    return pd.DataFrame.from_dict({'studio_id': id,\n",
    "                                   'name': name,\n",
    "                                   'metro': metro,\n",
    "                                   'address': address,\n",
    "                                   'phone': phone,\n",
    "                                   'email': email}).set_index('studio_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T08:48:30.168049Z",
     "start_time": "2021-03-05T08:48:29.910373Z"
    }
   },
   "outputs": [],
   "source": [
    "url_studio = 'https://ugoloc.ru/studios/show/' + str(584)\n",
    "html = urllib.request.urlopen(url_studio).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "halls_html = soup.find_all('a', href=re.compile('studios/hall/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T08:48:46.675662Z",
     "start_time": "2021-03-05T08:48:46.663658Z"
    }
   },
   "outputs": [],
   "source": [
    "def hall_list(studio_id):\n",
    "    \n",
    "    st_id = list()\n",
    "    hall_id = list()\n",
    "    name = list()\n",
    "    is_hall = list()\n",
    "    square = list()\n",
    "    ceiling = list()\n",
    "    \n",
    "    for id in studio_id:\n",
    "        url_studio = 'https://ugoloc.ru/studios/show/' + str(id)\n",
    "        html = urllib.request.urlopen(url_studio).read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        halls_html = soup.find_all('a', href=re.compile('studios/hall/'))\n",
    "        halls = dict()\n",
    "        for hall in halls_html:\n",
    "            if int(hall.get('href').replace('/studios/hall/','')) not in hall_id:\n",
    "                st_id.append(id)\n",
    "                name.append(hall['title'])\n",
    "                hall_id.append(int(hall.get('href').replace('/studios/hall/','')))\n",
    "                if 'грим' in str.lower(hall['title']):\n",
    "                    is_hall.append(0)\n",
    "                else:\n",
    "                    is_hall.append(1)\n",
    "                    \n",
    "                url_hall = 'https://ugoloc.ru/studios/hall/' + str(hall.get('href').replace('/studios/hall/',''))\n",
    "                html_hall = urllib.request.urlopen(url_hall).read()\n",
    "                soup_hall = BeautifulSoup(html_hall, \"html.parser\")\n",
    "                \n",
    "                try:\n",
    "                    square.append(int(soup_hall.find_all('div', class_='param-value')[0].contents[0]))\n",
    "                except:\n",
    "                    square.append(np.nan)\n",
    "                try:\n",
    "                    ceiling.append(float(soup_hall.find_all('div', class_='param-value')[1].contents[0]))\n",
    "                except:\n",
    "                    ceiling.append(np.nan)\n",
    "    \n",
    "    return pd.DataFrame.from_dict({'studio_id': st_id, \n",
    "                                   'hall_id': hall_id, \n",
    "                                   'name': name, \n",
    "                                   'is_hall': is_hall,\n",
    "                                   'square': square,\n",
    "                                   'ceiling': ceiling\n",
    "                                  }).set_index('hall_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T08:21:00.035236Z",
     "start_time": "2021-03-05T08:20:58.756294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Скарпинг элементов с одной страницы\n",
    "    \n",
    "url = 'https://quotes.toscrape.com/'\n",
    "response = r.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "quotes = soup.find_all('span', class_='text')\n",
    "authors = soup.find_all('small', class_='author')\n",
    "tags = soup.find_all('div', class_='tags')\n",
    "\n",
    "for i in range(0, len(quotes)):\n",
    "    print(quotes[i].text)\n",
    "    print('--' + authors[i].text)\n",
    "    tagsforquote = tags[i].find_all('a', class_='tag')\n",
    "    for tagforquote in tagsforquote:\n",
    "        print(tagforquote.text)\n",
    "    print('\\n')\n",
    "    \n",
    "url = 'https://scrapingclub.com/exercise/list_basic/?page=1'\n",
    "response = r.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "items = soup.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "\n",
    "for n, i in enumerate(items, start=1):\n",
    "    itemName = i.find('h4', class_='card-title').text.strip()\n",
    "    itemPrice = i.find('h5').text\n",
    "    print(f'{n}:  {itemPrice} за {itemName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T13:25:14.123467Z",
     "start_time": "2021-03-04T13:25:10.498756Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Скарпинг элементов с нескольких страниц\n",
    "url = 'https://scrapingclub.com/exercise/list_basic/?page=1'\n",
    "response = r.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "items = soup.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "\n",
    "for n, i in enumerate(items, start=1):\n",
    "    itemName = i.find('h4', class_='card-title').text.strip()\n",
    "    itemPrice = i.find('h5').text\n",
    "    print(f'{n}:  {itemPrice} за {itemName}')\n",
    "\n",
    "# Ищем адреса страниц и собираем их в список. UL - галвный, LI - указатели на список    \n",
    "pages = soup.find('ul', class_='pagination')\n",
    "urls = []\n",
    "links = pages.find_all('a', class_='page-link')\n",
    "\n",
    "# Проверяем валидность полученных адресов\n",
    "for link in links:\n",
    "    pageNum = int(link.text) if link.text.isdigit() else None\n",
    "    if pageNum != None:\n",
    "        hrefval = link.get('href')\n",
    "        urls.append(hrefval)\n",
    "\n",
    "# Проходим по списку адресов и забираем по каждому из них нужные элементы        \n",
    "for slug in urls:\n",
    "    newUrl = url.replace('?page=1', slug)\n",
    "    response = r.get(newUrl)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    items = soup.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "    for n, i in enumerate(items, start=n):\n",
    "        itemName = i.find('h4', class_='card-title').text.strip()\n",
    "        itemPrice = i.find('h5').text\n",
    "        print(f'{n}:  {itemPrice} за {itemName}')\n",
    "        \n",
    "# Оптимизированный аналог\n",
    "\n",
    "url = 'https://scrapingclub.com/exercise/list_basic/'\n",
    "params = {'page': 1}\n",
    "# задаем число больше номера первой страницы, для старта цикла\n",
    "pages = 2\n",
    "n = 1\n",
    "\n",
    "while params['page'] <= pages:\n",
    "    response = r.get(url, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    items = soup.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "\n",
    "    for n, i in enumerate(items, start=n):\n",
    "        itemName = i.find('h4', class_='card-title').text.strip()\n",
    "        itemPrice = i.find('h5').text\n",
    "        print(f'{n}:  {itemPrice} за {itemName}')\n",
    "\n",
    "    # [-2] предпоследнее значение, потому что последнее \"Next\"\n",
    "    last_page_num = int(soup.find_all('a', class_='page-link')[-2].text)\n",
    "    pages = last_page_num if pages < last_page_num else pages\n",
    "    params['page'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T13:28:06.911156Z",
     "start_time": "2021-03-04T13:28:06.889150Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T08:08:58.590753Z",
     "start_time": "2021-03-05T08:08:57.344038Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:24:51.986610Z",
     "start_time": "2021-03-03T06:24:42.208835Z"
    }
   },
   "outputs": [],
   "source": [
    "#Объект ответа в HTTP. Получаем инфу о контенте \n",
    "url = \"https://authoraditiagarwal.com/wpcontent/uploads/2018/05/MetaSlider_ThinkBig-1080x180.jpg\"\n",
    "\n",
    "r = r.get(url, allow_redirects=True) #запрос для выполнения HTTP-запросов GET для URL\n",
    "for headers in r.headers: \n",
    "    print(headers)\n",
    "    \n",
    "#загружаем контент\n",
    "\n",
    "r = r.get(url) \n",
    "with open(\"ThinkBig.png\",'wb') as f:\n",
    "   f.write(r.content)\n",
    "\n",
    "print (r.headers.get('content-type'))\n",
    "print (r.headers.get('Server'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:25:30.315319Z",
     "start_time": "2021-03-03T06:25:30.284377Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#запишем захваченные данные в файл CSV с именем dataprocessing.csv в этой папке\n",
    "f = csv.writer(open(' dataprocessing.csv ','w'))\n",
    "f.writerow(['Title'])\n",
    "f.writerow([soup.title.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T07:07:57.828142Z",
     "start_time": "2021-03-03T07:07:57.381392Z"
    }
   },
   "outputs": [],
   "source": [
    "#scraping Wikipedia to find out all the countries in Asia.\n",
    "\n",
    "\n",
    "website_url = r.get('https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area').text\n",
    "soup = BeautifulSoup(website_url, 'lxml')\n",
    "My_table = soup.find('table',{'class': 'wikitable sortable'})\n",
    "links = My_table.find_all(\"a\")\n",
    "Countries = []\n",
    "for link in links:\n",
    "    Countries.append(link.get('title'))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"Country\"] = Countries\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T07:08:17.367914Z",
     "start_time": "2021-03-03T07:08:17.044538Z"
    }
   },
   "outputs": [],
   "source": [
    "# для хранения данных в корзину S3 нам нужно создать клиент S3 \n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = \"our-content\"\n",
    "#Созбдаем сегмент S3\n",
    "s3.create_bucket(Bucket = bucket_name, ACL = 'public-read')\n",
    "s3.put_object(Bucket = bucket_name, Key = '', Body = data, ACL = \"public-read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T07:08:37.066129Z",
     "start_time": "2021-03-03T07:08:34.931961Z"
    }
   },
   "outputs": [],
   "source": [
    "#Подключаемся к SQL серверу\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1',user='root', passwd = None, db = 'mysql',\n",
    "charset = 'utf8', port=None)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE scrap\")\n",
    "random.seed(datetime.datetime.now())\n",
    "def store(title, content):\n",
    "   cur.execute('INSERT INTO scrap_pages (title, content) VALUES ''(\"%s\",\"%s\")', (title, content))\n",
    "   cur.connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Запускаем ХромДрайвер\n",
    "driver = webdriver.Chrome()  # Optional argument, if not specified will search path.\n",
    "driver.get('http://www.google.com/');\n",
    "search_box = driver.find_element_by_name('q')\n",
    "search_box.send_keys('ChromeDriver')\n",
    "search_box.submit()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлечение веб данных о системе пользователя\n",
    "import user_agents\n",
    "ua = user_agents.parse(ua)\n",
    "ua.is_bot\n",
    "ua.is_mobile\n",
    "ua.os.family\n",
    "ua.browser.family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#robots.txt — это файл, используемый для идентификации частей сайта, которые сканерам разрешено просматривать \n",
    "#Sitemap - карты сайта, которые помогают сканерам находить контент без необходимости сканировать каждую страницу"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
