{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ссылки \n",
    "  * [Разведочный анализ данных](https://www.kaggle.com/emstrakhov/eda-with-pandas).<br>\n",
    "  * [Построение и отбор признаков](https://habr.com/ru/company/ods/blog/325422/)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T12:39:39.247603Z",
     "start_time": "2021-02-05T12:39:37.691864Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from functools import reduce\n",
    "import seaborn as sns\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "sns.set_style(style='white')\n",
    "sns.set(rc={\n",
    "    'figure.figsize': (12, 7),\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.grid': True, 'grid.color': '.9',\n",
    "    'axes.linewidth': 1.0,\n",
    "    'grid.linestyle': u'-'}, font_scale=1.5)\n",
    "custom_colors = [\"#3498db\", \"#95a5a6\", \"#34495e\", \"#2ecc71\", \"#e74c3c\"]\n",
    "sns.set_palette(custom_colors)\n",
    "\n",
    "os.chdir(r'C:\\Users\\Mr Alex\\Documents\\GitHub\\FlightPreparence')\n",
    "\n",
    "# Рабочие фреймы\n",
    "df = pd.read_csv('train.csv')\n",
    "tsd = pd.read_csv('test.csv')\n",
    "# Объединенный список из фреймов\n",
    "comb = pd.concat([df, tsd], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T12:39:40.553726Z",
     "start_time": "2021-02-05T12:39:40.143695Z"
    }
   },
   "outputs": [],
   "source": [
    "#Преобразуем объектные столбцы в числовые \n",
    "df['date'] = df['date'].map({'S': 0, 'C': 1, 'Q': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем объектные значения в колонке в числовые автоматически\n",
    "df['date'] = LabelEncoder().fit_transform(df['date'])\n",
    "\n",
    "# Или выносим их в новый фрейм (для анализа)\n",
    "label_encoder = LabelEncoder()\n",
    "mapped_education = pd.Series(label_encoder.fit_transform(df['data']))\n",
    "mapped_education.value_counts().plot.barh()\n",
    "print(dict(enumerate(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем Dtype колонки в числовые, если нужна внутренняя редактура\n",
    "df['data'] = df['data'].str.replace(r'\\D+', '') #Удаляем все НЕцифры\n",
    "df['data'] = pd.to_numeric(df['data'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Извлекаем определенные фразы из объектных строк\n",
    "for dataset in df:\n",
    "    dataset['data_new'] = dataset['data'].str.extract(' ([A-Za-z]+)\\.') #В данном случае слова, в конце которых стоит точка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем float в int\n",
    "df['data'] = pd.to_numeric(df['data'], downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разбиваем колонки на новые, в каждой из которых только свои уникальные значения из прежней колонки\n",
    "pd.get_dummies(df.Data, prefix=\"Emb\", drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T12:39:52.460355Z",
     "start_time": "2021-02-05T12:39:52.455355Z"
    }
   },
   "outputs": [],
   "source": [
    "# Выделяем числовые колонки в отдельную группу, получаем список заголовков колонок\n",
    "num_feat = [x for x in comb.columns if comb[x].dtype !=\"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделяем номинальные колонки в отдельную группу\n",
    "cat_feat = [x for x in comb.columns if comb[x].dtype == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выделяем упорядоченные колонки в отдельную группу (из номинальных, самостоятельно)\n",
    "ord_feat = ['data','data1','data2']\n",
    "cat_feat.remove(ord_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем список колонок, которые будут после обработки удалены из индекса. И добавляем в индекс новые\n",
    "to_remove = []\n",
    "to_remove.append('data')\n",
    "num_feat.append('new_data')\n",
    "ord_feat.append('new_data')\n",
    "cat_feat.append('new_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка на корреляцию всех числовых колонок\n",
    "corr_matrix = comb[num_feat].corr()\n",
    "plt.figure(figsize=(16,12))\n",
    "sns.heatmap(corr_matrix.T, annot=True, cbar=False, cmap='coolwarm');\n",
    "\n",
    "# Отдельно выделяем все, что выше 0.8\n",
    "corr_matrix = comb[num_feat].corr()\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(corr_matrix.T, annot=True, mask= corr_matrix < 0.8 ,cbar=False, cmap='coolwarm');\n",
    "\n",
    "# Ищем колонки с наибольшей корреляцией предикторов к отклику, удаляем отклик\n",
    "corr_ser = comb[num_feat].corr()['y'].sort_values(ascending=False).drop(\"y\")\n",
    "fig, ax = plt.subplots(figsize=(10,12))\n",
    "sns.barplot(x=corr_ser.values, y=corr_ser.index, palette=\"rocket_r\")\n",
    "plt.title(\"Корреляция числовых предикторов с откликом\");\n",
    "\n",
    "# Удаление топ-корреляций из фреймов и из списка числовых колонок\n",
    "high_correlated_var = [\"data\",'data1','data2']\n",
    "comb = comb.drop(high_correlated_var, axis=1)\n",
    "\n",
    "for c in high_correlated_var:\n",
    "    num_feat.remove(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Замена пропущенных значений одной из переменных из колонки\n",
    "df.Data.fillna(df.Data.mode()[0], inplace = True)\n",
    "\n",
    "# Вариант 2, для категориальных колонок\n",
    "for col in cat_feat:\n",
    "    if comb[col].isna().sum() > 0:\n",
    "        comb[col] = comb[col].fillna(value=\"NA\") #Заменяем пробелы объектом 'NA'\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обработка категориальных колонок\n",
    "new_data = comb['data'].apply(lambda x: 1 if x != \"NA\" else 0).astype(\"object\") #Заменить все NA\n",
    "new_data = comb['data'].apply(lambda x: x if x == \"Predict\" else \"Other\") #Разделить на доминирующий предикт и всех остальных\n",
    "\n",
    "# Plot обновляемых данных\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "\n",
    "sns.countplot(new_data, ax=axes[0])\n",
    "sns.boxplot(x=new_data.values, y='y', data=comb, ax=axes[1])\n",
    "axes[0].set_xlabel(\"New Data\")\n",
    "axes[0].set_ylabel(\"Y\")\n",
    "axes[1].set_xlabel(\"New Data\")\n",
    "axes[1].set_ylabel(\"Y\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание новой колонки на основе других (математические действия)\n",
    "new_data = (comb['data1'].astype(int) - comb['data'].astype(int))\n",
    "comb['new_data'] = pd.Series(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обработка номинальных колонок\n",
    "\n",
    "for col in ord_feat:  \n",
    "    print(f\" Column '{col}' has unique values {comb[col].unique()}\")\n",
    "    \n",
    "ord_map = {\"NA\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4,\"Ex\":5}\n",
    "for col in ord_feat:        \n",
    "    if len(comb[col].unique()) <= 6 and col !=\"data\":\n",
    "        comb[col] = comb[col].map(ord_map)\n",
    "        comb[col] = comb[col].astype(int)        \n",
    "    elif col in ['data1', 'data2']:\n",
    "        comb[col] = comb[col].astype(int)\n",
    "    else:\n",
    "        print('Dat all')\n",
    "\n",
    "#Включаем обработанные упорядоченные в числовые\n",
    "for col in ord_feat:\n",
    "    num_feat.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Фильтр фрейма, оставить только строки с опредленным значением в одной из колонок\n",
    "print( 'Before:', len(df) )\n",
    "gwa_codes = [code for code in df.Code.unique() if 'GWA_' in code]\n",
    "df = df[df.Code.isin(gwa_codes)]\n",
    "print( 'After:', len(df) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Заполнить пустые значения средним по колонке\n",
    "df['data'] = df.groupby(['data1']).Data.apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cоздание новой колонки, аггрегирующей данные из нескольких\n",
    "new_col = ['data','data1', 'data2', 'data3']\n",
    "comb['new_data'] = np.zeros(len(comb)).reshape(len(comb),1) #Создаем новую пустую колонку длинной в список\n",
    "\n",
    "#Переносим в новую колонку все значения из старых\n",
    "for col in new_col:\n",
    "    comb['new_data'] += comb[col] \n",
    "    \n",
    "# Удаляем старые колонки из фрейма и из списка числовых колонок\n",
    "to_remove = ['data','data1', 'data2', 'data3']\n",
    "for c in to_remove:\n",
    "    comb.drop(c, axis=1, inplace=True)\n",
    "    num_feat.remove(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T14:17:09.733936Z",
     "start_time": "2021-02-03T14:17:09.105935Z"
    }
   },
   "outputs": [],
   "source": [
    "#Удаляем лишние колонки\n",
    "df.drop(to_remove, inplace=True, axis=1)\n",
    "\n",
    "#Или\n",
    "for col in to_remove:\n",
    "    comb.drop(col, axis=1, inplace=True)\n",
    "    cat_feat.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим первые 10 строк\n",
    "df = df.drop(np.arange(10), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим всех пассажиров с переменной меньше 10 и больше 50 \n",
    "df1 = df.drop(df[(df['data'] < 10) | (df['data'] > 50)].index)\n",
    "df1.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем бинарные столбцы в численные. Колонку y тоже\n",
    "df['data'] = df['data'].map({'predict1': 0, 'predict2': 1, 'predict3': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Приводим множество названий колонок к типу set, находим разность двух множеств: \n",
    "print(set(X_train.columns) - set(X_test.columns))\n",
    "print(set(X_test.columns) - set(X_train.columns))\n",
    "\n",
    "#Добавляем недостающую колонку. Смотрим, стоит ли склеивать отдельные переменные в более крупные классы\n",
    "columns = set(X_train.columns) | set(X_test.columns)\n",
    "X_train = X_train.reindex(columns=columns).fillna(0)\n",
    "X_test = X_test.reindex(columns=columns).fillna(0)\n",
    "\n",
    "#Проверяем совпадение колонок (если да, то True)\n",
    "all(X_train.columns == X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Группировка колонки по двум другим переменным. Медиана группы подставлена в пропущенные строки \n",
    "grp = df.groupby(['data1', 'data2'])  \n",
    "df.Data = grp.Data.apply(lambda x: x.fillna(x.median()))\n",
    "df.Data.fillna(df.Data.median, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приводим множество названий колонок к типу set, находим разность двух множеств: Голландии нет в колонке native-county\n",
    "print(set(X_train.columns) - set(X_test.columns))\n",
    "print(set(X_test.columns) - set(X_train.columns))\n",
    "\n",
    "# Добавляем недостающую колонку. Смотрим, стоит ли склеивать отдельные переменные в более крупные классы\n",
    "columns = set(X_train.columns) | set(X_test.columns)\n",
    "X_train = X_train.reindex(columns=columns).fillna(0)\n",
    "X_test = X_test.reindex(columns=columns).fillna(0)\n",
    "\n",
    "# Проверяем совпадение колонок (если да, то True)\n",
    "all(X_train.columns == X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Абсорбируем и удаляем лишние знаки из объектов во всех строках колонки во фрейме и записываем их в новую колонку\n",
    "for dataset in comb:\n",
    "    dataset['data_new'] = df['data'].str.extract(' ([A-Za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Построение облачного графика из объектов, где размер коррелирует с частотой\n",
    "wc = WordCloud(width = 1000,height = 450,background_color = 'white').generate(str(df.Data_new.values))\n",
    "plt.imshow(wc, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n",
    "\n",
    "df.Data_new.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Если в БД нет единой метрики, то стандартизируем данные\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# StandartScaling хоть и не делает распределение нормальным в строгом смысле слова, но защищает от выбросов\n",
    "shapiro(StandardScaler().fit_transform(data)) #Смотрим значение статистики и p-value\n",
    "\n",
    "#Cтандартизируем переменные 2\n",
    "df_scaled = preprocessing.scale(data)\n",
    "#Методом поиска главных компонентов проецируем данные на двумерную плоскость и получаем ранжирование компонентов по важности \n",
    "pca = PCA(n_components=5).fit(df_scaled) #Уточняем число компонент и источник данных \n",
    "\n",
    "#Доля разброса в данных, объясняемая главными компонентами\n",
    "print('Влияние компонентов на общий разброс данных: ', pca.explained_variance_ratio_)\n",
    "\n",
    "# MinMax Scaling переносит все точки на заданный отрезок (обычно (0, 1) и полезен для визуализации на отрезке(0, 255)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "MinMaxScaler().fit_transform(data)\n",
    "\n",
    "# Если данные описываются логнормальным распределением, их можно легко привести к честному нормальному распределению\n",
    "from scipy.stats import lognorm\n",
    "data = lognorm(s=1).rvs(1000) #Дата с тяжелым правым хвостом\n",
    "shapiro(np.log(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление низковариативных фич, у которых дисперсия ниже определенного уровня. То есть они маловажны\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.datasets import make_classification\n",
    "x_data, y_data = make_classification()\n",
    "x_data.shape\n",
    "VarianceThreshold(.7).fit_transform(x_data).shape\n",
    "x_data.shape\n",
    "VarianceThreshold(.8).fit_transform(x_data).shape\n",
    "x_data.shape\n",
    "VarianceThreshold(.9).fit_transform(x_data).shape\n",
    "\n",
    "# ИЛИ\n",
    "x_data_kbest = SelectKBest(f_classif, k=5).fit_transform(x_data, y_data)\n",
    "\n",
    "# ИЛИ Sequential Feature Selection\n",
    "selector = SequentialFeatureSelector(LogisticRegression(\n",
    "), scoring='neg_log_loss', verbose=2, k_features=3, forward=False, n_jobs=-1)\n",
    "selector.fit(x_data_scaled, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Категориальные колонки не имеют естественного порядка, поэтому преобразуем (еще методы: OneHotEncode и Label Encoding)\n",
    "dummy_comb = pd.get_dummies(comb[cat_feat], drop_first=True) #Разделяем все кат. предикторы по отдельным колонкам\n",
    "\n",
    "for col in cat_feat:\n",
    "    comb.drop(col, axis=1, inplace=True) #Удаляем оригинальные категориальные колонки\n",
    "    \n",
    "comb_with_dummies = pd.concat([comb, dummy_comb], axis=1) #Расширенный новый фрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращаем разделение данных на train/test\n",
    "clean_train_df = comb_with_dummies[comb_with_dummies[\"Y\"] > 0].copy()\n",
    "clean_test_df = comb_with_dummies[comb_with_dummies[\"Y\"].isna()].copy().drop(\"Y\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Чистка от слишком ассиметричных колонок через функции перекоса и эксцесса\n",
    "skewed_features = clean_train_df[num_feat].skew().sort_values(ascending=False)\n",
    "skewed_features = skewed_features[skewed_features > 0.5] #Перекос больше |0.5| и эксцесс >|2| - абнормальны\n",
    "skewed_index = skewed_features.index\n",
    "\n",
    "fig = plt.figure(figsize=(12,4), dpi=120)\n",
    "skewed_features.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xticks(horizontalalignment=\"right\")\n",
    "plt.title(\"Перекос свыше лимита 0.5 \")\n",
    "plt.tight_layout();\n",
    "\n",
    "right_skewness_col = ['data', 'data1']\n",
    "for col in right_skewness_col:\n",
    "    clean_train_df.drop(col, axis=1, inplace=True)\n",
    "    clean_test_df.drop(col, axis=1, inplace= True)\n",
    "    num_feat.remove(col)\n",
    "    \n",
    "skewed_index = skewed_index.drop(['data','data1'])\n",
    "#Удаляем аномалии (через ограничение на полторы сигмы)\n",
    "for col in skewed_index:\n",
    "    q3 = np.quantile(clean_train_df[col], 0.75)\n",
    "    q1 = np.quantile(clean_train_df[col], 0.25)\n",
    "    iqr = q3 - q1\n",
    "    # Upper limit for outliers\n",
    "    upper_limit = q3 + (1.5*iqr)\n",
    "    col_limit =  clean_train_df[col].apply(lambda x: x <= upper_limit)\n",
    "    clean_train_df = clean_train_df[col_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация текста\n",
    "# Bag of Words: создаем вектор длиной в словарь, для каждого слова считаем количество вхождений в текст\n",
    "# подставляем это число на соответствующую позицию в векторе\n",
    "\n",
    "from functools import reduce\n",
    "dictionary = list(enumerate(set(reduce(lambda x, y: x + y, df))))\n",
    "def vectorize(text):\n",
    "    vector = np.zeros(len(dictionary))\n",
    "    for i, word in dictionary:\n",
    "        num = 0\n",
    "        for w in text:\n",
    "            if w == word:\n",
    "                num += 1\n",
    "        if num:\n",
    "            vector[i] = num\n",
    "    return vector\n",
    "for t in texts:\n",
    "    print(vectorize(t))\n",
    "\n",
    "# Для сохранения порядка слов в тексте можно использовать использовать N-граммы     \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(ngram_range=(1,2)) # Уточняем, что следует учитывать не только отдельные слова, но и сочетания\n",
    "vect.fit_transform(['sentence', 'sentence1']).toarray()\n",
    "vect.vocabulary_\n",
    "\n",
    "# Можно генерировать N-граммы из букв (поиск родственных слов или опечаток)\n",
    "from scipy.spatial.distance import euclidean\n",
    "vect = CountVectorizer(ngram_range=(3,3), analyzer='char_wb')\n",
    "n1, n2, n3, n4 = vect.fit_transform(['word', 'word1', 'word2', 'word3']).toarray()\n",
    "euclidean(n1, n2)\n",
    "\n",
    "# TF-IDF позволяет находить более редкие слова (они более важные или ключевые)\n",
    "# Другие методы: bag of sites, bag of apps, bag of events, Word2Vec, Glove, Fasttext и др.\n",
    "# Готовая модель https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Работа с изображениями. \n",
    "# Предобученная сверточная сеть state of the art. Оторвать последние слои и подставить свои:\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from scipy.misc import face\n",
    "\n",
    "resnet_settings = {'include_top': False, 'weights': 'imagenet'}\n",
    "resnet = ResNet50(**resnet_settings)\n",
    "img = image.array_to_img(face())\n",
    "img = img.resize((224, 224)) # внимательнее к ресайзу\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0) # нужно дополнительное измерение, т.к. модель рассчитана на работу с массивом изображений\n",
    "features = resnet.predict(x)\n",
    "\n",
    "# Поиск текста на картинке\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "img = 'image.jpg'\n",
    "img = requests.get(img)\n",
    "img = Image.open(BytesIO(img.content))\n",
    "text = pytesseract.image_to_string(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлечение геоданных\n",
    "import reverse_geocoder as revgc\n",
    "revgc.search((df.latitude, df.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дата и время\n",
    "# Библиотека для автоматической генерации признаков из временных рядов: https://github.com/blue-yonder/tsfresh\n",
    "# Разбиение по дням недели\n",
    "df['dow'] = df['data'].apply(lambda x: x.date().weekday())\n",
    "df['is_weekend'] = df['dow'].apply(lambda x: 1 if x.date().weekday() in (5, 6) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подготовка к стандартизации. Унификация колонок\n",
    "row_id = pd.Series(clean_test_df[\"Id\"]) #Сохраняем колонку ИД для прогноза\n",
    "clean_train_df = clean_train_df.drop(\"Id\", axis=1).astype(\"float64\")\n",
    "clean_test_df = clean_test_df.drop(\"Id\", axis=1).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Финальная подготовка данных\n",
    "# Разделение на обучающую и тестовую\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = clean_train_df.drop([\"Y\"], axis=1)\n",
    "y = clean_train_df['Y']\n",
    "\n",
    "# Разделение на обучающую и валидационную\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Стандартизация\n",
    "num_feat.remove('Y')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_Xtrain = X_train.copy()\n",
    "scaled_Xtest = X_test.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_Xtrain[num_feat] = scaler.fit_transform(scaled_Xtrain[num_feat])\n",
    "scaled_Xtest[num_feat] = scaler.transform(scaled_Xtest[num_feat])\n",
    "\n",
    "# Итоговые фреймы для моделирования\n",
    "X_train\n",
    "X_test\n",
    "y_train\n",
    "y_test\n",
    "scaled_Xtrain\n",
    "scaled_Xtest"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
