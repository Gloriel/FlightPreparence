{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [Пример моделирования](https://www.kaggle.com/godzill22/house-price-prediction-improved-model-top-8)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T06:43:34.734340Z",
     "start_time": "2021-02-04T06:43:33.832310Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import missingno\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.stats import skew\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "comb = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    "\n",
    "# Create function for model evaluation\n",
    "def model_evaluation(algo,algoname):\n",
    "    \"\"\"\n",
    "    This function  fit and  evaluate \n",
    "    given algorithm. It takes 3 arguments:\n",
    "    \n",
    "    First: algorithm of a choice without parentheses.\n",
    "    Second: the name of a algorithm as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit given model\n",
    "    algo.fit(scaled_Xtrain, y_train)\n",
    "    y_pred = algo.predict(scaled_Xtest)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # R-squared \n",
    "    r2score = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"**{algoname} Metrics**\")\n",
    "    print(f\"**MAE: {mae:}\")\n",
    "    print(f\"**RMSE: {rmse:}\")\n",
    "    print(f\"**R-squared: {r2score:.2f}%\")\n",
    "    \n",
    "    return mae, rmse, r2score, y_pred, algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great place to start for someone who ask what algorithm I shoud use is sklearn algorithm cheat-sheet https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html. Follow their recommendation I am going to choose first Stochastic Gradient Descent. Let's create Stochastic Gradient Descent first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGDRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base model\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_base_model = SGDRegressor(random_state=101)\n",
    "\n",
    "sgd_base_mae, sgd_base_rmse, sgd_base_r2score, sgd_y_pred, _ = model_evaluation(sgd_base_model, \n",
    "                                                                                \"SGDRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(y_pred, algoname):\n",
    "    \"\"\"\n",
    "    Function plots probability and residuals plot\n",
    "    \"\"\"\n",
    "    residuals = pd.Series(y_test - y_pred, \n",
    "                          name=\"residuals\")\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2, \n",
    "                             nrows=2, \n",
    "                             figsize=(14,4), \n",
    "                             dpi=120)\n",
    "    # Plot probability\n",
    "    sp.stats.probplot(residuals, plot=axes[0,0])\n",
    "    # Plot kde\n",
    "    sns.distplot(residuals, ax=axes[0,1], hist=False)\n",
    "    # Plot residuals\n",
    "    sns.scatterplot(x=y_test, y=residuals, ax=axes[1,0])\n",
    "    axes[1,0].axhline(y=0, c='red',ls='--')\n",
    "    # Plot distribution\n",
    "    sns.boxplot(residuals, ax=axes[1,1])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(sgd_y_pred, \"SGDRegressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr_model = GradientBoostingRegressor()\n",
    "gbr_base_mae, gbr_base_rmse, gbr_base_r2score, gbr_y_pred, gbr_model = model_evaluation(gbr_model, \n",
    "                                                                                        \"GradientBostingRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(gbr_y_pred, \"Gradient Boosting Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr_model = RandomForestRegressor()\n",
    "rfr_base_mae, rfr_base_rmse, rfr_base_r2score, rfr_y_pred, rfr_model = model_evaluation(rfr_model, \n",
    "                                                                                        \"RandomForestRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(rfr_y_pred, \"Random Forest Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extreme Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgboost_model = XGBRegressor()\n",
    "xgboost_base_mae, xgboost_base_rmse, xgboost_base_r2score, xgboost_y_pred, xgboost_model = model_evaluation(xgboost_model, \n",
    "                                                                                                            \"Extreme Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(xgboost_y_pred, \"Extreme Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNeighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_model = KNeighborsRegressor()\n",
    "knn_base_mae, knn_base_rmse, knn_base_r2score, knn_y_pred, knn_model = model_evaluation(knn_model, \n",
    "                                                                                        \"KNeighborsRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(knn_y_pred, \"KNeighborsRegressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submmit to Kaggle with the best score to Kaggle competition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler and copy dataset\n",
    "sc = StandardScaler()\n",
    "scaled_X = X.copy()\n",
    "scaled_test = clean_test_df.copy()\n",
    "\n",
    "# Scale the data\n",
    "scaled_X[num_feat] = sc.fit_transform(X[num_feat])\n",
    "scaled_test[num_feat] = sc.transform(clean_test_df[num_feat])\n",
    "\n",
    "# Instantiate the final model\n",
    "# final_base_model = GradientBoostingRegressor()\n",
    "\n",
    "# Fit the model\n",
    "# final_base_model.fit(scaled_X, y)\n",
    "\n",
    "# final_predictions = final_base_model.predict(scaled_test)\n",
    "\n",
    "\n",
    "# Make predictions and save it to the dataframe\n",
    "# final_base_model_df = pd.DataFrame({\"id\":row_id,\n",
    "                                    # \"SalePrice\": np.expm1(final_predictions)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_base_model_df.to_csv(\"house_price_final_base_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ElasticNetCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to use ElasticNetCV in the base line models predictions as it will allow me to choose between Ridge(L2 regularization) or Lasso (L1 regularization). The benefit is that elastic net allows a balance of both penalties, which can result in better performance than a model with either one or the other penalty on some problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "elastic_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1])\n",
    "\n",
    "el_base_mae, el_base_rmse, el_base_r2score, el_base_y_pred, elastic_model = model_evaluation(elastic_model,\n",
    "                                                                                             \"ElasticNetCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(el_base_y_pred, \"ElasticNetCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_model.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_cv_model = LassoCV(eps=0.01, n_alphas=200, cv=10, max_iter=1000000)\n",
    "\n",
    "\n",
    "lassoCV_mae, lassoCV_rmse, lassoCV_r2score, lassoCV_y_pred, lasso_cv_model = model_evaluation(lasso_cv_model, \"LassoCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(lassoCV_y_pred, \"LassoCV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RidgeCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "ridge_cv_mae, ridge_cv_rmse, ridge_cv_r2, ridge_cv_y_pred, ridge_model = model_evaluation(ridge_model,\n",
    "                                                                                          \"RidgeCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(ridge_cv_y_pred, \"RidgeCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_base_model = SVR()\n",
    "\n",
    "svr_base_mae, svr_base_rmse, svr_base_r2score, svr_base_y_red, svr_base_model = model_evaluation(svr_base_model, \n",
    "                                                                                                 \"Support Vector Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(svr_base_y_red, \"SVR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoostRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cat_base = CatBoostRegressor(verbose=0, random_state=101)\n",
    "\n",
    "cat_base_mae, cat_base_rmse, cat_base_r2, cat_base_y_pred, cat_base_model = model_evaluation(cat_base,\n",
    "                                                                                             \"CatBoostRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = cat_base.get_feature_importance(prettified=True)\n",
    "\n",
    "# Plotting top 20 features' importance\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.barplot(feat_imp['Importances'][:20],feat_imp['Feature Id'][:20], orient = 'h', palette=\"coolwarm_r\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from catboost import Pool\n",
    "\n",
    "# Feature importance Interactive Plot \n",
    "\n",
    "train_pool = Pool(scaled_Xtrain)\n",
    "val_pool = Pool(scaled_Xtest)\n",
    "\n",
    "explainer = shap.TreeExplainer(cat_base_model) # insert your model\n",
    "shap_values = explainer.shap_values(train_pool) # insert your train Pool object\n",
    "\n",
    "shap.summary_plot(shap_values, scaled_Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model scores metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_score_df = pd.DataFrame({\"Model\":[\"SGDRegressor\", \"GradientBoostingRegressor\",\n",
    "                                       \"RandomForestRegressor\", \"Extreme Gradient Boosting\",\n",
    "                                       \"KNeighborsRegressor\" , \"LassoCV\", \"SVR\", \"RidgeCV\",\n",
    "                                       \"CatBoost\"],\n",
    "                              \n",
    "                              \"R-square\":[sgd_base_r2score, gbr_base_r2score, rfr_base_r2score,\n",
    "                                         xgboost_base_r2score, knn_base_r2score, lassoCV_r2score,\n",
    "                                         svr_base_r2score, ridge_cv_r2, cat_base_r2],\n",
    "                              \n",
    "                              \"RMSE\":[sgd_base_rmse, gbr_base_rmse, rfr_base_rmse, xgboost_base_rmse,\n",
    "                                      knn_base_rmse, lassoCV_rmse, svr_base_rmse, ridge_cv_rmse,\n",
    "                                      cat_base_rmse],\n",
    "                              \n",
    "                              \"MAE\": [sgd_base_mae, gbr_base_mae, rfr_base_mae, xgboost_base_mae,\n",
    "                                      knn_base_mae, lassoCV_mae, svr_base_mae, ridge_cv_mae,\n",
    "                                      cat_base_mae]})\n",
    "\n",
    "base_score_df = base_score_df.sort_values(by=[\"R-square\"], \n",
    "                                          ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**Base Models Metrics**\")\n",
    "base_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the table above\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "sns.barplot(x=\"Model\", y=\"R-square\", data=base_score_df, ax=ax, palette=\"magma\")\n",
    "sns.lineplot(x=\"Model\", y=\"RMSE\", data=base_score_df, color=\"red\", ax=ax,legend='brief', label=\"rmse\")\n",
    "sns.lineplot(x=\"Model\", y=\"MAE\", data=base_score_df, color='green', ax=ax, legend='brief', label=\"mae\")\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\")\n",
    "plt.title(\"Regression Model Performance Metrics\")\n",
    "plt.ylabel(\"R_squared\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submmit Voting Ensamble Model with base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "ensemble1_model = VotingRegressor(estimators=[(\"ridgecv\", ridge_model),\n",
    "                                             (\"catboost\", cat_base_model),\n",
    "                                             (\"gbr\", gbr_model),\n",
    "                                             (\"lassocv\", lasso_cv_model),\n",
    "                                             (\"svr\", svr_base_model),\n",
    "                                             (\"forest\", rfr_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble1_mae, ensemble1_rmse, ensemble1_r2, _ , ensemble1_model = model_evaluation(ensemble1_model,\n",
    "                                                                                    \"Voting Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submmit ensemble model to the competition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# ensemble_model1.fit(scaled_X, y)\n",
    "\n",
    "# final_ensemble = ensemble_model1.predict(scaled_test)\n",
    "\n",
    "\n",
    "# Make predictions and save it to the dataframe\n",
    "# final_base_ensemble_df = pd.DataFrame({\"id\":row_id,\n",
    "                                       # \"SalePrice\": np.expm1(final_ensemble)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_base_ensemble_df.to_csv(\"house_price_final_ensemble_base_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV for the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV is a exhaustive search over specified hyperparameters values for an estimator. It allow us to find the best combination of best parameters for a chosen model. I will split data again with test size of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def model_gridsearchCV(algo,param,name):\n",
    "    \"\"\"\n",
    "    Function will perform gridsearchCV for given algorithm\n",
    "    and parameter grid. Returns grid model, y_pred. Prints out \n",
    "    mean absolute error, root mean squared error, R-square score\n",
    "    \"\"\"\n",
    "    # Instatiate base model\n",
    "    model = algo()\n",
    "    \n",
    "    # Instantiate grid for a model\n",
    "    model_grid = GridSearchCV(model, \n",
    "                             param,\n",
    "                             scoring=\"r2\",\n",
    "                             # verbose=2,\n",
    "                             n_jobs=-1,\n",
    "                             cv=3)\n",
    "    # Fit the grid model\n",
    "    model_grid.fit(scaled_Xtrain, y_train)\n",
    "    \n",
    "    # Make prediction\n",
    "    y_pred = model_grid.predict(scaled_Xtest)\n",
    "    \n",
    "    # Evaluate model\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2score = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Print \n",
    "    print(f\"**{name} with GridSearchCV**\")\n",
    "    print(f\"MAE: {mae:}\")\n",
    "    print(f\"RMSE: {rmse:}\")\n",
    "    print(f\"R-squared: {r2score:.2f}%\")\n",
    "    \n",
    "    return mae, rmse, r2score, y_pred, model_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GradientBoostingRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {#\"loss\":[\"ls\",\"lad\",\"huber\",\"quantile\"],\n",
    "              \"learning_rate\": [ 0.01, 0.1, 0.3, 1],\n",
    "              \"subsample\": [0.5, 0.2, 0.1],\n",
    "              \"n_estimators\": [500, 1000],\n",
    "              \"max_depth\": [3,6,8]}\n",
    "\n",
    "gbr_grid_mae, gbr_grid_rmse, gbr_grid_r2, _ , gbr_grid = model_gridsearchCV(GradientBoostingRegressor, \n",
    "                                                                            param_grid,\n",
    "                                                                            \"GradientBoostingRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [500,1000, 1500],\n",
    "              \"max_features\": ['auto','sqrt'],\n",
    "              \"max_depth\": [None,5,10,],\n",
    "              \"min_samples_split\": [2,5,10],\n",
    "              \"min_samples_leaf\": [1,2,5,10]}\n",
    "\n",
    "rfr_grid_mae, rfr_grid_rmse, rfr_grid_r2, _ , rfr_grid_model = model_gridsearchCV(RandomForestRegressor,\n",
    "                                                                                  param_grid,\n",
    "                                                                                  \"RandomForestRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"kernel\":[\"linear\",\"rbf\",],\n",
    "              \"gamma\": [\"scale\",\"auto\"],\n",
    "              \"C\": [0.1, 0.5, 1, 10],\n",
    "              \"epsilon\": [0.1, 0.01, 0.001]}\n",
    "\n",
    "svr_grid_mae, svr_grid_rmse, svr_grid_r2, svr_grid_y_pred, svr_grid_model = model_gridsearchCV(SVR,\n",
    "                                                                                param_grid,\n",
    "                                                                               \"SVR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_grid_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge**\n",
    "\n",
    "RidgeCV had the best metrics and I want to see if GridSearchCV can improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "param_grid = {\"solver\": [\"auto\",\"svd\",\"lsqr\",\"saga\"],\n",
    "              \"max_iter\": [1000, 10000],\n",
    "              \"tol\": [1e-3,1e-2],\n",
    "              \"alpha\": [0.1, 1.0, 10.0, 30.0]}\n",
    "\n",
    "ridge_gr_mae, ridge_gr_rmse, ridge_gr_r2,_ , ridge_gr_model = model_gridsearchCV(Ridge,\n",
    "                                                                                 param_grid,\n",
    "                                                                                 \"Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_gr_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extreme Gradient Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a warning when running this algoritm, but It should not prevent your code from running, nor should it lead to different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"learning_rate\":[0.05, 0.10, 0.15, 0.20, 0.30],\n",
    "              \"max_depth\":[3,4,5,6,8,15],\n",
    "              \"min_child_weight\":[1,3,5,7],\n",
    "              \"gamma\":[0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "              \"colsample_bytree\":[0.3, 0.4, 0.5, 0.7]}\n",
    "\n",
    "xboost_gr_mae, xboost_gr_rmse, xboost_gr_r2, _ , xboost_gr_model = model_gridsearchCV(XGBRegressor,\n",
    "                                                                                      param_grid,\n",
    "                                                                                      \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xboost_gr_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoostRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_grid = {'iterations': [250,100,500,1000],\n",
    "              #'learning_rate': [0.01,0.1,0.2,0.3],\n",
    "              #'depth': [4, 6],\n",
    "              #'l2_leaf_reg': [3,1,5,10,100]}\n",
    "\n",
    "\n",
    "# cat_grid_mae, cat_grid_rmse, cat_grid_r2, _ , cat_grid_model = model_gridsearchCV(CatBoostRegressor,\n",
    "                                                                                  # param_grid,\n",
    "                                                                                  # \"CatBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost with GridSearchCV**\n",
    "1. MAE: 0.08686650731538992\n",
    "2. RMSE: 0.12366782337837257\n",
    "3. R-squared: 0.92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grCV_metrics_df = pd.DataFrame({\"Model\":[\"GradientBoostingRegressor\", \"RandomForestRegressor\", \n",
    "                                         \"SVR\", \"Ridge\", \"XGBRegressor\", \"CatBoost\"],\n",
    "                                        \n",
    "                                \"R-square\":[gbr_grid_r2, rfr_grid_r2, svr_grid_r2, \n",
    "                                            ridge_gr_r2, xboost_gr_r2, cat_grid_r2],\n",
    "                                        \n",
    "                                \"RMSE\":[gbr_grid_rmse, rfr_grid_rmse, svr_grid_rmse, \n",
    "                                        ridge_gr_rmse, xboost_gr_rmse, cat_grid_rmse],\n",
    "                                        \n",
    "                                \"MAE\":[gbr_grid_mae, rfr_grid_mae, svr_grid_mae, \n",
    "                                      ridge_gr_mae, xboost_gr_mae, cat_grid_mae]})\n",
    "\n",
    "grCV_mertics_df = grCV_metrics_df.sort_values(by=[\"R-square\"],\n",
    "                                              ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"**GridSearchCV Models Metrics**\")\n",
    "grCV_mertics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the table above\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "list_order = list(grCV_mertics_df['Model'].values)\n",
    "# R-squared\n",
    "sns.barplot(x=\"Model\", y=\"R-square\", \n",
    "            data=grCV_metrics_df, ax=ax, \n",
    "            palette=\"magma\", order= list_order)\n",
    "# Root Mean Squared Error\n",
    "sns.lineplot(x=\"Model\", y=\"RMSE\", data=grCV_metrics_df, \n",
    "             color=\"red\", ax=ax,legend='brief', label=\"rmse\")\n",
    "# Mean Absolute Error\n",
    "sns.lineplot(x=\"Model\", y=\"MAE\", data=grCV_metrics_df, \n",
    "             color='green', ax=ax, legend='brief', label=\"mae\")\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\")\n",
    "plt.title(\"Regression Models with GridSearchCV Metrics\")\n",
    "plt.ylabel(\"R_squared\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ensemble model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble2_model = VotingRegressor(estimators=[(\"ridgecv\", ridge_gr_model.estimator),\n",
    "                                             (\"catboost\", cat_grid_model.estimator),\n",
    "                                             (\"gbr\", gbr_grid.estimator),\n",
    "                                             (\"lassocv\", lasso_cv_model),\n",
    "                                             (\"svr\", svr_base_model),\n",
    "                                             (\"forest\", rfr_model.base_estimator)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "ensemble2_model.fit(scaled_Xtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble model\n",
    "ensemble2_y_pred = ensemble2_model.predict(scaled_Xtest)\n",
    "\n",
    "ensemble2_mae = mean_absolute_error(y_test, ensemble2_y_pred)\n",
    "ensemble2_rmse = np.sqrt(mean_squared_error(y_test, ensemble2_y_pred))\n",
    "    \n",
    "# R-squared \n",
    "ensemble2_r2 = r2_score(y_test, ensemble2_y_pred)\n",
    "    \n",
    "print(f\"**VotingRegressor Metrics**\")\n",
    "print(f\"**MAE: {ensemble2_mae}\")\n",
    "print(f\"**RMSE: {ensemble2_rmse}\")\n",
    "print(f\"**R-squared: {ensemble2_r2:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions submmision to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ensemble = VotingRegressor(estimators=[(\"gbr\", gbr_grid.estimator),\n",
    "                                            (\"forest\", rfr_grid_model.estimator),\n",
    "                                            (\"svr\", svr_grid_model.estimator),\n",
    "                                            (\"ridge\", ridge_gr_model.estimator),\n",
    "                                            (\"xgboost\", xboost_gr_model.estimator),\n",
    "                                            (\"catboost\", cat_grid_model.estimator)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "best_ensemble.fit(scaled_X, y)\n",
    "\n",
    "final_ensemble2 = best_ensemble.predict(scaled_test)\n",
    "\n",
    "\n",
    "#Make predictions and save it to the dataframe\n",
    "final_ensemble_df = pd.DataFrame({\"id\":row_id,\"SalePrice\": np.expm1(final_ensemble2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ensemble_df.to_csv(\"house_price_grid_ensemble_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to admit that spent all that time on testing didn't help to improve the model in this case .After submmision to Kaggle competition I end up in top 8%. Now, I need to figure it out what to do next or which techniques I could implement to make this model even more robust. I've got some ideas already in my mind, anyway, back to reading and searching. So, if you Kagglers have some ideas let me know. Don't forget to leave feadback."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "665px",
    "left": "12px",
    "right": "20px",
    "top": "216px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
