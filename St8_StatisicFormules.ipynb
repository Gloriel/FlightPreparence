{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полезные ссылки:<br>\n",
    "* [Формулы на LaTeX](http://www.machinelearning.ru/wiki/images/e/e4/latex_examples.pdf)<br>\n",
    "* [Теория и практика на Хабр](https://habr.com/ru/company/ods/blog/322534/)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.827448Z",
     "start_time": "2021-02-15T09:13:57.793421Z"
    },
    "code_folding": [
     91
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pylab import plot,show,hist\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "from scipy.stats import norm, chi2_contingency\n",
    "import statsmodels.api as sm\n",
    "from numpy import linspace,hstack\n",
    "import pydot\n",
    "#%config InlineBackend.figure_format = 'svg' для большей четкости графиков\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "#Стандартизация данных\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Для построения диаграмм рассеивания\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Графика для интерпретации моделей\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "\n",
    "#Иерархический кластерный анализ\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "#Кластерный анализ методом К-средних\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Линейная регрессия\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Полиномиальная регрессия\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Логистическая регрессия\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Расщепление на обучающую и тестовую выборки\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Деревья решений для задачи классификации\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Деревья решений для задач регрессии \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Калибровка деревьев решений\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "#XGBoost\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "#Нейронные сети\n",
    "from keras.models import Sequential #тип сети\n",
    "from keras.layers import Dense #метод соединения слоев\n",
    "from keras.utils import np_utils #обработка данных под Керас\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "#Deep Learning\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Факторный анализ\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#SVD - Singular Value decomposition\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "\n",
    "# os.chdir(r'C:\\Users\\Mr Alex\\Documents\\GitHub\\FlightPreparence')\n",
    "\n",
    "# columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "#            'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "#            'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "#df = pd.read_csv('AmesHousing.txt', sep=\"\\t\", header = 0, encoding='cp1251', index_col=False)\n",
    "#df = pd.read_csv('beverage_r.csv', sep=\";\", decimal=',', parse_dates=[0], index_col='numb.obs')\n",
    "#df = pd.read_csv('diamond.dat', header=None, sep='\\s+', names=['weight', 'price'])\n",
    "#df = pd.read_csv('adult.test', header=None, names=columns, na_values=' ?', skiprows=1)\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.832421Z",
     "start_time": "2021-02-15T09:13:57.828444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Классификация: приписание объекта к классу на основе ключевой(группирующей) переменной или совокупности его характеристик\n",
    "# Типы переменных. Количественные(непрерывные, дискретные). Номинальные (несравниваемые). Ранговые (порядковые)\n",
    "# Гистограмма частот - форма распределения количественного признака\n",
    "# Описательная статистика. Меры центральной тенденции. Меры изменчивости (Размах - Xmax-Xmin)\n",
    "# МЦТ. Мода - самый частый признак. Медиана - делит упорядоченное множество пополам. Среднее (Математическое ожидание, EX)\n",
    "# Дисперсия D - средний квадрат отклонений индивидуальных значений от средней величины. С ростом n, дисперсия сокращается\n",
    "# D = сумма(Xинд - Xсред)**2/n-1. Хсред генеральной совокупности обозначается как мю, М\n",
    "# Стандартное отклонение, \"сигма\", sd = D**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.837441Z",
     "start_time": "2021-02-15T09:13:57.834421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Нормальное распределение. Унимодально и симметрично\n",
    "# Центральная предельная теорема. Для выборок стандартная ошибка среднего se=SDинд/n**0.5, где n - число элементов выборки\n",
    "# Если n выборка репрезентативная и число элементов > 30, то se=0.5\n",
    "# Интервал для поиска М генеральной совокупности(доверительный интервал): для 95% выборок Хсред ± 1.96*se включат в себя М"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.842423Z",
     "start_time": "2021-02-15T09:13:57.839422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ненормальные распределения\n",
    "# Сгладить распределение, уменьшив шкалу на основании полезности данных, удалив аномалии\n",
    "# Логарифмировать переменные (не забываем про ноль в исходной переменной). Схлопывает экстремальные значения\n",
    "# Логарифмирование отлично работает с ассиметричными распределениями\n",
    "# Если логарифмы переменных зависимы линейно, то значит сами переменные зависят нелинейно\n",
    "# Преобразование Бокса-Кокса подбирает оптимальную степень для возведения в нее mathworks.com/help/finance/boxcox.html\n",
    "# Bootstrap и метод Монте-Карло. Сравнивать медиану, мин, макс, 13-процентиль, среднее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.847422Z",
     "start_time": "2021-02-15T09:13:57.843421Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Число наблюдений(N1), попавших в столбец. H = C*N1\n",
    "# H = N1/(N*длина интервала) - в таком случае гистограмма будет вероятностной, то есть в пределах единицы\n",
    "# Плотность распределения f(x) позволяет рассчитать вероятность P(A) попаданий в определенный интервал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.893453Z",
     "start_time": "2021-02-15T09:13:57.848421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ядерная оценка плотности Скотта-Сильвермана - обобщение гистограммы F(t) = (1/n*h)*сумма всех наблюдений K(t-Xi/h)\n",
    "# Распределение Японечникова определяет плотность К - симметричная, неотрицательная, с интегралом=1\n",
    "# Метод определяет меру сглаживания\n",
    "my_density = gaussian_kde(df['y'], bw_method=1)\n",
    "x = linspace(min(df['y']), max(df['y']), 1000)\n",
    "plot(x, my_density(x), 'g')  # распределение функции\n",
    "hist(df['y'], density=True, alpha=.3)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.894447Z",
     "start_time": "2021-02-15T09:13:57.800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Для определения \"типичного\" объекта совокупности можно использовать среднее(если нет выбросов) или медиану(если есть)\n",
    "# При неравномерном распределении можно убрать выбросы\n",
    "df_new = df.iloc[2:1004]\n",
    "# Или логарифмировать переменную (для лог-нормального распределения)\n",
    "x = np.log10(df[u'y'])\n",
    "pd.Series(x).hist(bins=45)\n",
    "# Усеченное среднее. Выбрасывается 2,5% самых малых и 2,5% наибольших значений переменной. Для новой БД считается среднее\n",
    "exclude = int(len(df)/100*2.5)\n",
    "redacted_town = df[exclude:len(df)-exclude]\n",
    "# Если рассевание нельзя разделить линейно, то меняем точку начала координат и выбираем новые параметры для разделения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.895455Z",
     "start_time": "2021-02-15T09:13:57.801Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Для получения комплексного сравнения объектов по нескольким переменным\n",
    "# Диагональ показывает ядерную оценку плотности\n",
    "# Матрица состоит из диаграмм рассеивания\n",
    "colors = {'genuine': 'green', 'counterfeit': 'red'}\n",
    "scatter_matrix(df,\n",
    "               figsize=(6, 6),\n",
    "               diagonal='kde',  # плотность вместо гистограммы на диагонали\n",
    "               c=df['data'].replace(colors),\n",
    "               alpha=0.2,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.896422Z",
     "start_time": "2021-02-15T09:13:57.802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Иерархический кластерный анализ разделяет объекты на группы (стратификация). Число групп заранее неизвестно\n",
    "# Кластерный анализ позволяет сократить число наблюдений и проинтерпретировать их\n",
    "# Схожесть внутри кластера отображается как расстоянием между близкими объектами на диаграмме кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.897421Z",
     "start_time": "2021-02-15T09:13:57.803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Расстояние можно рассчитать методами: Евклида(или квадрата Евклида), Блок(Манхеттен), Хэмминга(для слов) и тд.\n",
    "# Манхеттен предпочтительнее, когда нет больших различий в рандомных переменных, потому что вес аномалий тогда меньше\n",
    "# Расстояние между кластерами рассчитывается:\n",
    "# Метод Варда (WARD) - позволяет работать с шаровыми скоплениями\n",
    "# Метод ближайших соседей (позволяет определять ленточные кластеры)\n",
    "# Средневзвешенное расстояние: среднее для суммы всех расстояний (также для ленточных)\n",
    "# Центроид: расстояние между кластерами равно расстоянию между их центрами тяжести\n",
    "# Методы дальнего и ближайшего соседа: расстояние между самыми дальними\\близкими объектами есть межкластер\n",
    "# Метод расстояния Sorencen-Dice Q = 2*|A^B|/|A|+|B|. Не работает если множества слабо пересекаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.898452Z",
     "start_time": "2021-02-15T09:13:57.804Z"
    }
   },
   "outputs": [],
   "source": [
    "# kNN основана на гипотезе компактности, если метрика удачна, то схожие примеры чаще лежат в одном классе, а не в разных\n",
    "# kNN часто используется для построения мета-признаков (прогноз kNN подается на вход прочим моделям) и рекомендаций\n",
    "# Качество классификации/регрессии зависит от числа соседей и метрики расстояния между объектами\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "# weights: \"uniform\" (все веса равны), \"distance\" (вес обратно пропорционален расстоянию)\n",
    "# algorithm: \"brute\", \"ball_tree\", \"KD_tree\", или \"auto\" \n",
    "# leaf_size: порог переключения на полный перебор в случае выбора BallTree или KDTree для нахождения соседей\n",
    "# metric: \"minkowski\", \"manhattan\", \"euclidean\", \"chebyshev\"\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "knn_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, knn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.899424Z",
     "start_time": "2021-02-15T09:13:57.806Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Алгоритмы кластерного анализа. Объекты группируются с ближайшими, пока нет скачка в расстояниях для следующего слияния\n",
    "# Момент для прекращения слияния определяется дендрограммой (для умеренного числа объектов)\n",
    "# Каменистая осыпь/локоть показывают скачок (резкий взлет графика) шагов объединений, когда кластеризуются тысячи объектов\n",
    "# Задача аналитика: отобрать переменные, выбрать метод стандартизации, установить расстояние между кластерами и объектами\n",
    "# обратная матрица существует только для несингулярных матриц, у которых нет линейной зависимости колонок или строк.\n",
    "\n",
    "# Объект, в котором будет хранится информация о последовательном слиянии кластеров\n",
    "# Для функции нужен фрейм, метод межкластера и метод межобъектов\n",
    "link = linkage(df, 'ward', 'euclidean')\n",
    "\n",
    "# link - матрица (n-1) x 4, где n - число наблюдений.\n",
    "# Каждая строка - результат слияния очередной пары кластеров с номерами link[i, 0] и link[i, 1].\n",
    "# Новому кластеру присваивается номер n + i\n",
    "# link[i, 2] означает расстояние между слитыми кластерами, а link[i, 3] - размер нового кластера.\n",
    "\n",
    "# Построение дендрограммы\n",
    "dn = dendrogram(link, orientation='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:57.899424Z",
     "start_time": "2021-02-15T09:13:57.808Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ключевые характеристики кластеров\n",
    "# В колонку cluster запишем номер кластера объекта с помощью функции fcluster.\n",
    "# Аргументы: linkage, пороговое значение для межкластера (либо число кластеров), criterion: distance для остановки разбиения\n",
    "# Останавливаем объединение, если расстояние между кластерами превышает 3\n",
    "df['cluster'] = fcluster(link, 3, criterion='distance')\n",
    "# Доля объектов в кластере, которые имеют соответствующие характеристики\n",
    "df.groupby(\"cluster\").mean()\n",
    "\n",
    "# Кластерный анализ методом К-средних\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Три случайные центроиды \n",
    "np.random.seed(seed=42)\n",
    "centroids = np.random.normal(loc=0.0, scale=1., size=6)\n",
    "centroids = centroids.reshape((3, 2))\n",
    "\n",
    "cent_history = []\n",
    "cent_history.append(centroids)\n",
    "\n",
    "for i in range(3):\n",
    "    # Считаем расстояния от наблюдений до центроид\n",
    "    distances = cdist(X, centroids)\n",
    "    # Смотрим, до какой центроиде каждой точке ближе всего\n",
    "    labels = distances.argmin(axis=1)\n",
    "\n",
    "    # Положим в каждую новую центроиду геометрический центр её точек\n",
    "    centroids = centroids.copy()\n",
    "    centroids[0, :] = np.mean(X[labels == 0, :], axis=0)\n",
    "    centroids[1, :] = np.mean(X[labels == 1, :], axis=0)\n",
    "    centroids[2, :] = np.mean(X[labels == 2, :], axis=0)\n",
    "\n",
    "    cent_history.append(centroids)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(4):\n",
    "    distances = cdist(X, cent_history[i])\n",
    "    labels = distances.argmin(axis=1)\n",
    "\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(X[labels == 0, 0], X[labels == 0, 1], 'bo', label='cluster #1')\n",
    "    plt.plot(X[labels == 1, 0], X[labels == 1, 1], 'co', label='cluster #2')\n",
    "    plt.plot(X[labels == 2, 0], X[labels == 2, 1], 'mo', label='cluster #3')\n",
    "    plt.plot(cent_history[i][:, 0], cent_history[i][:, 1], 'rX')\n",
    "    plt.legend(loc=0)\n",
    "    plt.title('Step {:}'.format(i + 1));\n",
    "\n",
    "# Для задач кластеризации можно экспериментировать с количеством шагов, критерием сходимости, с метрикой расстояний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В kMeans критерий качества – сумма квадратов расстояний от точек до центроидов их кластеров: $\\ J(C) = \\sum_{k=1}^K\\sum_{i~\\in~C_k} ||x_i - \\mu_k||^{2} \\rightarrow \\min\\limits_C,$ <br>  $\\ С$– множество кластеров мощности $\\ K$ ,   $\\ mu_k$ - центроид кластера $\\ C_k$ . Чтобы ограничить число кластеров, используют правило локтя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:58.162423Z",
     "start_time": "2021-02-15T09:13:58.146424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Как правило локтя выглядит на графике\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 8):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)\n",
    "    inertia.append(np.sqrt(kmeans.inertia_))\n",
    "\n",
    "plt.plot(range(1, 8), inertia, marker='s');\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('$J(C_k)$');\n",
    "# MiniBatch K-means ускоряет процесс, для обучения используя не весь фрейм, а небольшие выборки из него "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:58.164422Z",
     "start_time": "2021-02-15T09:13:58.147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Создание модели\n",
    "model = KMeans(n_clusters=2,  # Число кластеров\n",
    "               # random_state - зерно датчика случайных чисел. Для воспроизводимости результата\n",
    "               random_state=42\n",
    "               )\n",
    "\n",
    "# подгонка модели по данным из БД\n",
    "model.fit(df)\n",
    "\n",
    "# Результат кластеризации на данных из БД\n",
    "model.labels_\n",
    "\n",
    "# координаты центров кластеров\n",
    "model.cluster_centers_\n",
    "\n",
    "# Добавление в кластер данных. Предсказание для новых наблюдений. Метод predict\n",
    "new_items = [\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "model.predict(new_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:13:58.165423Z",
     "start_time": "2021-02-15T09:13:58.150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Число кластеров можно определить через график локтя для для разного числа кластеров\n",
    "# Метод inertia_ вернёт сумму расстояний от каждой точки данных до центра ближайшего у ней кластера\n",
    "# Кластеризацию можно считать условно хорошей, когда инерция перестаёт сильно уменьшаться при увеличении числа кластеров\n",
    "K = range(1, 11)\n",
    "models = [KMeans(n_clusters=k, random_state=42).fit(df) for k in K]\n",
    "dist = [model.inertia_ for model in models]\n",
    "\n",
    "# График локтя\n",
    "plt.plot(K, dist, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of distances')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n",
    "\n",
    "# В колонке NR находится номер объекта, его нужно исключить из данных для кластеризации\n",
    "del df['NR']\n",
    "\n",
    "# Оптимизируем модель, меняя число задаваемых кластеров на основании графика локтя\n",
    "model = KMeans(n_clusters=4, random_state=42)\n",
    "model.fit(df)\n",
    "df['cluster'] = model.labels_\n",
    "df.groupby('cluster').mean()\n",
    "\n",
    "# Смотрим к какому кластеру какие объекты относятся\n",
    "df['cluster'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affinity Propagation - алгоритм авто-кластеризации на основе того, как они общаются, насколько похожи друг еа друга.\n",
    "\n",
    "Спектральная кластеризация  - сумма подходов. построение матрицы похожести наблюдений: $\\ A_{i, j} = - ||x_i - x_j||^{2}$ <br> Матрица описывает полный граф с вершинами в наблюдениях и рёбрами между каждой парой наблюдений с весом, соответствующим степени похожести вершин. Две точки более похожи, если ребро между ними короче. Normalized cuts problem - разделение графа на части, в которых наблюдения больше схожи друг с другом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Агломеративная кластеризация - простой и понятный алгоритм кластеризации без фиксированного числа кластеров. Начало - индивидуальные кластеры, затем сортировка по попарным расстояниям между кластерами и слияние ближайших. Цикл продолжается пока не образуется единый кластер. Методы - Single linkage, Complete linkage, Average linkage, Centroid linkage. Пример: $\\ d(C_i, C_j) = min_{x_i \\in C_i, x_j \\in C_j} ||x_i - x_j||$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:05.937446Z",
     "start_time": "2021-02-15T09:13:58.840422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Дендограмма Агломеративной кластеризации\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "\n",
    "distance_mat = pdist(X) # pdist посчитает нам верхний треугольник матрицы попарных расстояний\n",
    "\n",
    "Z = hierarchy.linkage(distance_mat, 'single') # linkage — реализация агломеративного алгоритма\n",
    "plt.figure(figsize=(10, 5))\n",
    "dn = hierarchy.dendrogram(Z, color_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики качества кластеризации\n",
    "\n",
    "<b>Adjusted Rand Index (ARI)</b>  симметрична, не зависит от значений и перестановок меток. Индекс является мерой расстояния между различными разбиениями выборки. ARI принимает значения в диапазоне (-1,1) . Отрицательные значения соответствуют \"независимым\" разбиениям на кластеры, близкие к нулю, — случайным разбиениям, положительные говорят о том, что два разбиения схожи (совпадают при =1). Для a - числа пар объектов из одного кластера и b - числа пар объектов с разными метками и из разных кластеров Rand Index:  $\\text{RI} = \\frac{2(a + b)}{n(n-1)};$   $\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}$\n",
    "\n",
    "<b>Adjusted Mutual Information (AMI)</b> почти такая же. Определяется с использованием функции энтропии, интерпретируя разбиения выборки, как дискретные распределения (вероятность отнесения к кластеру равна доле объектов в нём). Индекс  определяется как взаимная информация для двух распределений, соответствующих разбиениям выборки на кластеры. Взаимная информация измеряет долю информации, общей для обоих разбиений: насколько информация об одном из них уменьшает неопределенность относительно другого.\n",
    "\n",
    "<b>Гомогенность, полнота, V-мера</b>\n",
    "\n",
    "$\\ h = 1 - \\frac{H(C\\mid K)}{H(C)}, c = 1 - \\frac{H(K\\mid C)}{H(K)}$ h  измеряет, насколько каждый кластер состоит из объектов одного класса, c  — насколько объекты одного класса относятся к одному кластеру. Обе величины принимают значения в диапазоне (0,1), большие значения соответствуют более точной кластеризации. Эти меры не являются нормализованными, как ARI и поэтому зависят от числа кластеров. Для учёта обеих величин h и c одновременно вводится V-мера, как их среднее гармоническое: $\\ v = 2\\frac{hc}{h+c}.$ Она является симметричной и показывает, насколько две кластеризации схожи между собой.\n",
    "\n",
    "<b>Силуэт</b> не предполагает знания истинных меток объектов, и позволяет оценить качество кластеризации, используя только саму (неразмеченную) выборку и результат кластеризации. Сначала силуэт определяется отдельно для каждого объекта. a- среднее расстояние от данного объекта до объектов из того же кластера, b  — среднее расстояние от данного объекта до объектов из ближайшего кластера. $\\ s = \\frac{b - a}{\\max(a, b)}$ В диапазоне (-1,1) чем больше силуэт, тем более четко выделены кластеры, и чем они компактнее.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.513421Z",
     "start_time": "2021-02-15T09:14:05.938421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Пример построения метрик качества кластеризации\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering\n",
    "\n",
    "data = datasets.load_digits()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "algorithms = []\n",
    "algorithms.append(KMeans(n_clusters=10, random_state=1))\n",
    "algorithms.append(AffinityPropagation())\n",
    "algorithms.append(SpectralClustering(n_clusters=10, random_state=1,\n",
    "                                     affinity='nearest_neighbors'))\n",
    "algorithms.append(AgglomerativeClustering(n_clusters=10))\n",
    "\n",
    "data = []\n",
    "for algo in algorithms:\n",
    "    algo.fit(X)\n",
    "    data.append(({\n",
    "        'ARI': metrics.adjusted_rand_score(y, algo.labels_),\n",
    "        'AMI': metrics.adjusted_mutual_info_score(y, algo.labels_),\n",
    "        'Homogenity': metrics.homogeneity_score(y, algo.labels_),\n",
    "        'Completeness': metrics.completeness_score(y, algo.labels_),\n",
    "        'V-measure': metrics.v_measure_score(y, algo.labels_),\n",
    "        'Silhouette': metrics.silhouette_score(X, algo.labels_)}))\n",
    "\n",
    "results = pd.DataFrame(data=data, columns=['ARI', 'AMI', 'Homogenity',\n",
    "                                           'Completeness', 'V-measure',\n",
    "                                           'Silhouette'],\n",
    "                       index=['K-means', 'Affinity',\n",
    "                              'Spectral', 'Agglomerative'])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.517421Z",
     "start_time": "2021-02-15T09:14:10.514421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Проверка статистических гипотез:\n",
    "# Гипотеза согласия. Совпадает рандомная функция распределения с нормальным распределением? Самый дешевый и простой вариант\n",
    "# Гипотеза согласия2. Гипотеза об экспоненциальности распределения. Нужна, когда есть переменная времени ожидания\n",
    "# Гипотеза однородности. Совпадают две рандомные функции распредления? Например, чтобы сравнить данные до и после события\n",
    "# Гипотеза независимости. Нулевая гипотеза для рандомных объектов. Проверяется через коэффициент корреляции (скаляры)\n",
    "# Гипотеза о параметре распределения. Определение ключевых параметров. Например одинаковые средние или медианы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.521423Z",
     "start_time": "2021-02-15T09:14:10.518422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Альфа-это уровень значимости(0.05, 0.01. 0.005). Определеяет ошибки первого рода. На второго рода влияет размер выборки\n",
    "# Т- это статистика критерия. Если T<Cальфа, то верна нулевая гипотеза\n",
    "# Cальфа- это критическое значение. Вероятность отвергнуть правильную гипотезу(T>C) не должна превышать А(альфа)\n",
    "# p-value показывает насколько часто статистика критерия в верной гипотезе будет превышать реальные значения p=P{T>Tэксп}\n",
    "# Если p<A, гипотезу отвергаем. Если p>A, гипотезу не отвергаем. Проверяются все условия, при которых критерий будет работать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.527421Z",
     "start_time": "2021-02-15T09:14:10.522422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Тесты Колмогорова-Смирнова и Shapiro-Wilk позволяют проверить выборку на принадлежность к ГС и нормальность распредеелния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.564421Z",
     "start_time": "2021-02-15T09:14:10.528421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Q-Q plot - график рассеяния, где значения набора данных сравниваются со квантилями нормального распределения\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = df.data[(df.data <= 20000) & (df.data > 500)]\n",
    "\n",
    "data_log = np.log(data)\n",
    "data_mm = MinMaxScaler().fit_transform(data.values.reshape(-1, 1).astype(np.float64)).flatten()\n",
    "data_z = StandardScaler().fit_transform(data.values.reshape(-1, 1).astype(np.float64)).flatten()\n",
    "\n",
    "# Q-Q графики для логарифма и стандартизаций.\n",
    "sm.qqplot(data_log, loc=data_log.mean(), scale=data_log.std()).savefig('qq_data_log.png')\n",
    "sm.qqplot(data_mm, loc=data_mm.mean(), scale=data_mm.std()).savefig('qq_data_mm.png')\n",
    "sm.qqplot(data_z, loc=data_z.mean(), scale=data_z.std()).savefig('qq_data_z.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.572422Z",
     "start_time": "2021-02-15T09:13:59.169Z"
    }
   },
   "outputs": [],
   "source": [
    "# логарифмирование – это частный случай трансформации Бокса-Кокса. Преобразование Йео-Джонсона (на отрицательные числа)\n",
    "# Если данные описываются логнормальным распределением, их можно легко привести к честному нормальному распределению\n",
    "from scipy.stats import lognorm\n",
    "data = lognorm(s=1).rvs(1000) #Дата с тяжелым правым хвостом\n",
    "shapiro(np.log(data))\n",
    "\n",
    "# Применяем критерий Шапиро-Вилка после логарифмирования.\n",
    "df = df.set_index(u'y')\n",
    "plt.hist(np.log10(df[u'data']), bins=50)\n",
    "res = stats.shapiro(np.log10(df[u'data']))\n",
    "print('p-value: ', res[1])\n",
    "# P очень маленькое, поэтому гипотезу о нормальности отвергаем.\n",
    "# Отклонения от нормальности будут несущественны, если убрать выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.573422Z",
     "start_time": "2021-02-15T09:13:59.170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Тест на гипотезу однородности\n",
    "# За один вариант дизайна выказалось 28 из 100 опрошенных, за второй 20 из 100 опрошенных.\n",
    "# Проверяем, является ли эта разница статистически значимой с помощью критерия хи-квадрат.\n",
    "\n",
    "# Cтроим таблицу сопряжённости.\n",
    "contingency_table = pd.DataFrame([[28, 72], [20, 80]],\n",
    "                                 index=['first', 'second'],\n",
    "                                 columns=['for', 'against']\n",
    "                                 )\n",
    "\n",
    "# AB-тест. Проверка разных вариантах на схожих выборках\n",
    "res = stats.chi2_contingency(contingency_table)\n",
    "print('p-value: {0}'.format(res[1]))\n",
    "\n",
    "# p-value получился достаточно большим, поэтому оснований отвергнуть гипотезу о равенстве долей нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cлучайная величина  имеет распределение Бернулли, если она принимает всего два значения (0 и 1 с вероятностями $\\theta$ и $\\ 1-\\theta$ соответственно) и имеет следующую функцию распределения вероятности:\n",
    "\\begin{equation}\n",
    "\\ p\\left(\\theta, x\\right) = \\theta^{x} \\left(1 - \\theta\\right)^\\left(1 - x\\right), x \\in \\left\\{0, 1\\right\\}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.574421Z",
     "start_time": "2021-02-15T09:13:59.489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Z-метка (организация выборок так, чтобы они мало отличались от нормального распределения)\n",
    "\n",
    "s1 = 135       # успех в выборке А\n",
    "n1 = 1781      # выборка А\n",
    "s2 = 47        # успех в выборке Б\n",
    "n2 = 1443      # выборка Б\n",
    "p1 = s1/n1  # оценка вероятности успеха выборка А\n",
    "p2 = s2/n2  # оценка вероятности успеха выборка Б\n",
    "p = (s1 + s2)/(n1+n2)  # оценка вероятности успеха выборки А+Б\n",
    "z = (p2-p1) / ((p*(1-p)*((1/n1)+(1/n2)))**0.5)  # Z-метка\n",
    "\n",
    "p_value = norm.cdf(z)  # Функция распределения нормального распределения\n",
    "\n",
    "#  z-метка и p-значение\n",
    "print(['{:.12f}'.format(a) for a in (abs(z), p_value * 2)])\n",
    "# Нулевая гипотеза отвергнута, статистические доли отличаются\n",
    "\n",
    "# То же самое, но со встроенным методом библиотеки statsmodels\n",
    "z1, p_value1 = sm.stats.proportions_ztest([s1, s2], [n1, n2])\n",
    "print(['{:.12f}'.format(b) for b in (z1, p_value1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.574421Z",
     "start_time": "2021-02-15T09:13:59.491Z"
    }
   },
   "outputs": [],
   "source": [
    "# Тест Стьюдента на независимость переменных\n",
    "x = df[df['data'] == '1']['y']\n",
    "y = df[df['data'] == '0']['y']\n",
    "x.name, y.name = '1', '0'\n",
    "two_histograms(x, y)  # Данные условно нормальны.\n",
    "\n",
    "# Проверим c помощью критерия Флигнера-Килина, равны ли дисперсии.\n",
    "res = stats.fligner(x, y)\n",
    "# p-value низкое, гипотезу о равенстве дисперсий отвергаем, наблюдаемые объекты несвязные\n",
    "print('p-value: ', res[1])\n",
    "\n",
    "# Гипотезу о равенстве средних значений будем проверять с помощью теста Стьюдента при неравных дисперсиях\n",
    "# Опция equal_var=False говорит, что равенство дисперсии не предполагать\n",
    "res = stats.ttest_ind(x, y, equal_var=False)\n",
    "# P-значение значительно меньше альфы, гипотеза о равенстве отвергается\n",
    "print('p-value: ', res[1])\n",
    "\n",
    "# Ищем зависимость отклика от предиктора. Чтобы применить Стьюдента, проверим нормальность данных и равенство дисперсий\n",
    "# Заменяем -9999 (здесь=пустое) на корректное пустое значение.\n",
    "df = df.replace(-9999, np.nan)\n",
    "# Сохраним в отдельные переменные выборки, которые собираемся сравнивать.\n",
    "x = df[df['data'] == '1']['y']\n",
    "y = df[df['data'] == '0']['y']\n",
    "x.name, y.name = '1', '0'\n",
    "\n",
    "# Видно, что выбросы не дают применить Стюдента и нужно пробовать Манна-Витни\n",
    "two_histograms(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.575422Z",
     "start_time": "2021-02-15T09:13:59.493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Распределение Стьюдента (t-distribution) для n<30 - более высокие хвосты распределений.Число степеней свободы df=n-1\n",
    "# t заменяет Z в распределении Стьюдента. t=(Xинд-M)/(sd/n**0.5)\n",
    "# Помимо средних, нужно сравнить дисперсии D (тест Флигнера-Килина) и медианы (много n - тест Муда, мало n - Манн-Витни)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.576421Z",
     "start_time": "2021-02-15T09:13:59.495Z"
    }
   },
   "outputs": [],
   "source": [
    "# Парный t-критерий Стьюдента.  X1сред - Х2сред = А , se=((sd1**2/n1)+(sd2**2/n2))**0.5 , df=n1+n2-2\n",
    "# При t = A/se и df можно рассчитать p при котором M1-M2=0. То есть разницы между выборками почти нет\n",
    "# Q-Q Plot показывает насколько выборочные значения соответствуют предсказанным(из нормального распределеня)\n",
    "x = df['data']\n",
    "y = df['data1']\n",
    "x.name, y.name = 'data', 'data1'\n",
    "two_histograms(x, y)\n",
    "\n",
    "# Распределения условно нормальны. Поскольку в наблюдениях содержатся одни и те же люди, выборки связные (парные)\n",
    "res = stats.ttest_rel(x, y)  # Метод для парных выборок\n",
    "print('p-value: ', res[1])\n",
    "p-value: 0.0162416779538\n",
    "# p-value низкий, гипотеза на уровне значимости 0.05 будет отвергнута, но на уровне 0.01 уже нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.576421Z",
     "start_time": "2021-02-15T09:13:59.497Z"
    }
   },
   "outputs": [],
   "source": [
    "# U-критерий Манна-Витни переводит значения в ранговую шкалу и проверяет НЕ равенство медиан. P{X>Y}=P{X<Y}\n",
    "\n",
    "res = stats.mannwhitneyu(x, y)\n",
    "print('p-value:', res[1])\n",
    "# p-value большое, поэтому у нас нет оснований отвергнуть нулевую гипотезу. Разница медиан в выборках случайна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.577421Z",
     "start_time": "2021-02-15T09:13:59.498Z"
    }
   },
   "outputs": [],
   "source": [
    "# Корреляция. Scatter-plot или диаграмма рассеивания\n",
    "# Сила и направление взаимосвязи определяется ковариацией. cov=Сумма((Xi-Xсред)*(Yi-Yсред))/N-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для двух признаков $\\ X_i$ и $\\ X_j$ их ковариация будет $\\ cov(X_i, X_j) = E[(X_i - \\mu_i) (X_j - \\mu_j)] = E[X_i X_j] - \\mu_i \\mu_j$, где $\\mu_i$  матожидание i-ого признака.\n",
    "Ковариация симметрична и ковариация вектора с самим собой будет равна его дисперсии.<BR> Матрица ковариации - симметричная матрица, где на диагонали лежат дисперсии соответствующих признаков, а вне - ковариации соответствующих пар признаков. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.578421Z",
     "start_time": "2021-02-15T09:13:59.813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Коэффициент корреляции Пирсона находится в промежутке [-1; 1] и считается как Rxy=cov/SDx*SDy\n",
    "# Коэффициент детерминации r**2 показывает влияние дисперсии одной переменной на другую в промежутке [0; 1]\n",
    "# Коэффициент Спирмена позволяет блокировать выбросы через ранги. d=X-Y. Rs=1-6*сумма d**2/N(N**2-1)\n",
    "# Часто корреляция обусловлена скрытой переменной\n",
    "\n",
    "# Корреляция двух предикторов\n",
    "plt.scatter(df['data'], df['data1'])\n",
    "\n",
    "# Допускаем что коэфффициент корреляции=0, но гипотеза отвергнута\n",
    "res = stats.pearsonr(df['data'], df['data1'])\n",
    "\n",
    "print('Pearson rho: ', res[0])\n",
    "print('p-value: ', res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.578421Z",
     "start_time": "2021-02-15T09:13:59.814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Стандартизация позволяет сделать вес важных переменных соизмеримым. Min=0(-1), max=1. ИЛИ Z\n",
    "# Z-Стандартизация: преобразование в тип, где М=0, sd = 1. Правило одной, двух и трех \"сигм\"\n",
    "# Z=(Xинд-М)/sd Пример: по таблице Z, где Хсред=150, sd=8, превышать Xинд будет 0.5z или 30%\n",
    "# Z=(Xсред-M)/se =(18,5-20)/0.5 = -3. Вероятность получить такой результат p = 0.0027\n",
    "\n",
    "# Если в БД нет единой метрики, то стандартизируем данные\n",
    "norm = preprocessing.StandardScaler()\n",
    "norm.fit(df)\n",
    "X = norm.transform(df)\n",
    "\n",
    "# Вариант стратифицированной разбивки фрейма для кросс-валидации на модели случайного леса классификации\n",
    "X, y = df[cols].copy(), np.asarray(df[\"Y\"],dtype='int8')\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "results = cross_val_score(model, X, y, cv=skf)\n",
    "print(\"CV accuracy score: {:.2f}%\".format(results.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.579422Z",
     "start_time": "2021-02-15T09:13:59.816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Дисперсионный Анализ. Если межгрупповой показатель изменчивости сильно превышает внутригрупповой, то средние разнятся\n",
    "# SST - общая сумма квадратов показывает общую изменчивость данных. Сумма(Xинд-Xсред)**2  SST = SSW+SSB\n",
    "# SSW - сумма квадратов внутригрупповая. Сумма(X1инд-Х1сред)**2 + ...(XNинд-ХNсред)**2\n",
    "# SSB - сумма квадратов межгрупповая. SSB= n(X1сред - Хсред)**2 + ...n(XNсред-Хсред)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.580421Z",
     "start_time": "2021-02-15T09:13:59.818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Распределение Фишера, F-значение. F=(ssb/n-1)/(ssw/N-n). При верности нулевой гипотезы значения F очень маленькие\n",
    "# Поправка Бонферрони на множественную проверку гипотез. a = ai/n  НО: мешает получить значимые уровни различия\n",
    "# FDR или критерий Тьюки считает p-уровень для сравниваемых пар Xтэ=Xa-Xб\n",
    "# Двухакторный дисперсионный анализ SStotal=SSW+SSBa +SSBb + SSBa*SSBb\n",
    "# Взаимодействие факторов в ANOVA\n",
    "# Дисперсионный анализ требует нормальности распределения зависимой переменной и гомогенности дисперсии(тест Левена)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.581422Z",
     "start_time": "2021-02-15T09:13:59.820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Регрессионнный Анализ позволяет исследовать взаимосвязи переменных и делать линию тренда\n",
    "# Простая Линейная Регрессия. Взаимосвязь 2-х переменных. Y-зависимая(отклик) Х-независимая(предиктор)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим модель линейной регрессии следующим образом:\n",
    "\\begin{equation}\n",
    "\\ \\vec y = X \\vec w + \\epsilon,\n",
    "\\end{equation}\n",
    " * $\\ \\vec y \\in \\mathbb{R}^n $ объясняемая переменная (отклик);\n",
    " * $\\ w $ - вектор параметров модели (вес);\n",
    " * $\\ X $ - матрица наблюдений и признаков размерности  строк на столбцах (включая фиктивную единичную колонку слева) с полным рангом по столбцам: $\\ \\text{rank}\\left(X\\right) = m + 1 $;\n",
    " * $\\epsilon$ - случайная переменная, соответствующая непрогнозируемой ошибке модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выражение для каждого конкретного наблюдения [линейной регрессии](https://habr.com/ru/company/ods/blog/322076/):\n",
    "\\begin{equation}\n",
    "\\ y_i = \\sum_{j=0}^m w_j X_{ij} + \\epsilon_i\n",
    "\\end{equation}\n",
    "При ограничениях:\n",
    "* матожидание случайных ошибок равно нулю: $\\ \\forall i: \\mathbb{E}\\left[\\epsilon_i\\right] = 0$;\n",
    "* дисперсия случайных ошибок одинакова и конечна (гомоскедастична): $\\forall i: \\text{Var}\\left(\\epsilon_i\\right) = \\sigma^2 < \\infty$\n",
    "* случайные ошибки не скоррелированы: $\\forall i \\neq j: \\text{Cov}\\left(\\epsilon_i, \\epsilon_j\\right) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка распределена нормально с центром в нуле и некоторым разбросом: $\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка $\\hat{w}_i$ весов $\\ w_i$ называется линейной, если\n",
    "$ \\ \\hat{w}_i = \\omega_{1i}y_1 + \\omega_{2i}y_2 + \\cdots + \\omega_{ni}y_n,$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка $\\hat{w}_i$ называется несмещенной, когда матожидание оценки равно реальному, но неизвестному значению оцениваемого параметра: $\\ \\mathbb{E}\\left[\\hat{w}_i\\right] = w_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из способов вычислить значения параметров модели является метод наименьших квадратов (МНК), который минимизирует среднеквадратичную ошибку между реальным значением отклика и прогнозом, выданным моделью:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ \\begin{array}{rcl}\\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) &=& \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\\\ &=& \\frac{1}{2n} \\left\\| \\vec{y} - X \\vec{w} \\right\\|_2^2 \\\\ &=& \\frac{1}{2n} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right) \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "Для решения данной оптимизационной задачи необходимо вычислить производные по параметрам модели, приравнять их к нулю и решить полученные уравнения относительно $ \\vec w$. <br>\n",
    "Оценка МНК является лучшей оценкой параметров модели, среди всех линейных и несмещенных оценок, то есть обладающей наименьшей дисперсией. И она же является максимизацией правдоподобия данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.581422Z",
     "start_time": "2021-02-15T09:14:01.633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Y=B0(intercept)+B1(slope). Зачение Y, где линия пересекает ось, угол наклона линии к оси X\n",
    "# Метод наименьших квадратов(МНК) находит оптимальные параметры B0 и B1, чтобы сумма квадратов (SE) была минимальна MSE\n",
    "# Уравнение регрессии Y=B0+B1*X1\n",
    "# B1 = SDy/SDx*Rxy, B0 = (Yсред-B1*Xсред), t = B1/se, df=N-2 Если B1 близка к нулю, то взаимосвязи почти нет\n",
    "# Коэффтцтент Детерминации (выборочная дисперсия) R указывает какой процент вариации отклика определяется влиянием предиктора\n",
    "# R**2 = 1-(SSres/SStotal) доля дисперсии Y, объясняемая регрессионной моделью. Чем больше R , тем лучше\n",
    "# Требования: линейная вхаимосвязь X Y, нормальное распределение остатков, гомоскедатичность(изменчивость) остатков\n",
    "# Избежать ошибок спецификации при линейной регрессии помогает Анализ Остатков. Выявлять колинеарность\n",
    "# Число обусловленности Y по отношению к X измеряет изменения Y при колебании Х, т.е. чувствительность функции\n",
    "# Регуляризация — способ уменьшить сложность модели чтобы предотвратить переобучение или исправить некорректную задачу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.582421Z",
     "start_time": "2021-02-15T09:14:01.635Z"
    }
   },
   "outputs": [],
   "source": [
    "# Алгоритм обучения - выбор из большого семейства гипотез по критерию меры качества\n",
    "# Модель обладает обобщающей способностью, когда ошибка на тестовом наборе данных мала или предсказуема \n",
    "# Эмпирический риск (функция стоимости) принимает форму среднеквадратичной ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.583422Z",
     "start_time": "2021-02-15T09:14:01.639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Расщепление на обучающую и тестовые выборки\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.584422Z",
     "start_time": "2021-02-15T09:14:01.641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Линейная регрессия\n",
    "# Плюсы: быстрые, изученные, всеконкуренции когда множество предикторов,коэффициенты хорошо интерпретируются\n",
    "# Минусы: плохо работают при сложной и нелинейной зависимости отклика. В жизни теорема Маркова-Гаусса почти не выполняется\n",
    "\n",
    "# Строим модель\n",
    "model = LinearRegression()\n",
    "\n",
    "# обучаем модель\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Строим предсказание модели на тестовом множестве\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Промежуточные Регрессионные Коэффициенты от метода model.coef_ и свободный член от метода model.intercept_\n",
    "coef = pd.DataFrame(zip(['intercept'] + X.columns.tolist(), [model.intercept_] + model.coef_.tolist()),\n",
    "                    columns=['predictor', 'coef'])\n",
    "\n",
    "# Матрица показывает базовую цену и вес коэффициентов: 83.17 + 0.29*площадь SQFT + 12.17*удобства и т.д.\n",
    "# Логика показывает, что что-то не то. Проверяем на колинеарность\n",
    "df.corr()\n",
    "\n",
    "# Видим, что колинеарен TAX. Убираем и снова считаем, в этот раз с p-значением\n",
    "regression_coef(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка прогноза любой модели вида $\\ y = f\\left(\\vec{x}\\right) + \\epsilon$ складывается из:\n",
    "* квадрата смещения: $\\ \\text{Bias}\\left(\\hat{f}\\right)$ – средняя ошибка по всевозможным наборам данных;\n",
    "* дисперсии: $\\ \\text{Var}\\left(\\hat{f}\\right)$  – вариативность ошибки, то, на сколько ошибка будет отличаться, если обучать модель на разных наборах данных;\n",
    "* неустранимой ошибки: $\\ \\sigma^2$ . <br>\n",
    "\n",
    "При увеличении сложности модели (например, при увеличении количества свободных параметров) увеличивается дисперсия (разброс) оценки, но уменьшается смещение. Теорема Маркова-Гаусса как раз утверждает, что МНК-оценка параметров является лучшей в классе несмещенных линейных оценок, с наименьшей дисперсией. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В линейной регрессии пространство гипотез ограничено только линейными функциями от признаков. В полиномиальной регрессии пространство гипотез расширено до всех полиномов степени . Тогда в нашем случае, когда количество признаков равно одному , пространство гипотез будет выглядеть следующим образом:\n",
    "\n",
    "$\\ \\begin{array}{rcl} \\forall h \\in \\mathcal{H}, h\\left(x\\right) &=& w_0 + w_1 x + w_1 x^2 + \\cdots + w_n x^p \\\\ &=& \\sum_{i=0}^p w_i x^i \\end{array} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.585422Z",
     "start_time": "2021-02-15T09:14:02.216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Полиномиальная регрессия\n",
    "# Множественная регрессия  Y= B0+B1*X1 + ... + BN*XN   Многомерный scatter-plot\n",
    "# Дополнительно требует: мультиколлинеарность(без сильной корреляции или идентичности), нормальное распределение переменных.\n",
    "# t-критерий показывает оказываемое влияние каждого предиктора. Если 0, то влияния нет\n",
    "# Для множественной регрессии используется \"Исправленный\" R**2\n",
    "# Предсказать результат не только с помощью переменной (1я модель), но и её квадрата(2я модель) и их обеих (3я модель)\n",
    "# Класс PolynomialFeatures, метод fit_transform сгенерирует из множества фич множество одночленов заданной степени\n",
    "# Например, для степени 2 и фич a, b будут сгенерированы фичи [a, b, a**2, b**2, ab]\n",
    "# при указанном параметре include_bias=True ещё и вектор-свободный член из единиц.\n",
    "\n",
    "import statsmodels.api as sm\n",
    "poly = PolynomialFeatures(\n",
    "    # Максимальная степень\n",
    "    degree=2,\n",
    "    # Не генерировать свободный член\n",
    "    include_bias=False)\n",
    "y = df['price']\n",
    "X0 = poly.fit_transform(df[['weight']])\n",
    "X0 = pd.DataFrame(X0, columns=['weight', 'weight**2'])\n",
    "\n",
    "X0 = [\n",
    "    # Одна оригинальная переменная weight\n",
    "    X0[['weight']],\n",
    "    # Одна переменная weight**2\n",
    "    X0[['weight**2']],\n",
    "    # Две переменных weight и weight**2\n",
    "    X0.copy()]\n",
    "models = [LinearRegression() for _ in X0]\n",
    "\n",
    "for X, model in zip(X0, models):\n",
    "    model.fit(X, y)\n",
    "    print(model.score(X, y))\n",
    "\n",
    "# 𝑅**2  во всех моделях очень большой и примерно одинаков. Но на самом деле модели различны. Проверим их более тщательно\n",
    "\n",
    "regression_coef(models[0], X0[0], y)\n",
    "regression_coef(models[1], X0[1], y)\n",
    "regression_coef(models[2], X0[2], y)\n",
    "\n",
    "# Коэффициенты показывают спорные моменты в 1 и 3 моделях. 3-я ошибается из-за колинеарности (ложной)\n",
    "\n",
    "X2 = sm.add_constant(X0[2])\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основе байесовой статистики находится формула Байеса, нацеленная на поиски апостериорного распределениея:\n",
    "\n",
    "$ \\color{green}{p\\left(y \\mid x\\right)} = \\dfrac{\\color{orange}{p\\left(x \\mid y\\right)} \\color{blue}{p\\left(y\\right)}}{\\color{red}{p\\left(x\\right)}} $\n",
    "\n",
    " * $\\color{blue}{p\\left(y\\right)}$ априорные ожидания (prior): насколько правдоподобна гипотеза перед наблюдением данных;\n",
    " * $\\color{orange}{p\\left(x \\mid y\\right)}$ правдоподобие (likelihood): насколько правдоподобны данные при условии того, что гипотеза верна;\n",
    " * $\\color{red}{p\\left(x\\right) = \\sum_{z} p\\left(x \\mid z\\right) p\\left(z\\right)}$ маргинальная вероятность (marginal probability): вероятность данных, усредненная по всевозможным гипотезам;\n",
    " * $\\color{green}{p\\left(y \\mid x\\right)}$ апостериорное распределение (posterior): насколько правдоподобна гипотеза при наблюдаемых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.586423Z",
     "start_time": "2021-02-15T09:14:02.494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Прогнозирование временных рядов: данные упорядочены по моментам времени и могут содержать дополнительную информацию\n",
    "from tqdm import tqdm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "from scipy.optimize import minimize\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# График предиктора по временному ряду\n",
    "def plotly_df(df, title=''):\n",
    "    data = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        trace = go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df[column],\n",
    "            mode='lines',\n",
    "            name=column\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = dict(title=title)\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    iplot(fig, show_link=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv('hour_online.csv', index_col=['Time'], parse_dates=['Time'])\n",
    "plotly_df(df, title=\"title\")\n",
    "\n",
    "# Функция скользящего среднего\n",
    "def moving_average(series, n):\n",
    "    return np.average(series[-n:])\n",
    "\n",
    "\n",
    "moving_average(df.Data, 24)\n",
    "\n",
    "# Это не долгосрочный прогноз (предыдущее значение должно быть фактическим). Зато сглаживает исходный ряд (видим тренды)\n",
    "# DataFrame.rolling(window).mean() - готовое решение. Чем больше ширина интервала — тем более сглаженный тренд\n",
    "\n",
    "# Функция для добавления сглаживания во временной ряд\n",
    "def plotMovingAverage(series, n):\n",
    "    \"\"\"\n",
    "    series - dataframe with timeseries\n",
    "    n - rolling window size \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rolling_mean = series.rolling(window=n).mean()\n",
    "\n",
    "    # При желании, можно строить и доверительные интервалы для сглаженных значений\n",
    "    #rolling_std =  series.rolling(window=n).std()\n",
    "    #upper_bond = rolling_mean+1.96*rolling_std\n",
    "    #lower_bond = rolling_mean-1.96*rolling_std\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title(\"Moving average\\n window size = {}\".format(n))\n",
    "    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n",
    "\n",
    "    #plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n",
    "    #plt.plot(lower_bond, \"r--\")\n",
    "    plt.plot(dataset[n:], label=\"Actual values\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "plotMovingAverage(dataset, 24*7)  # сглаживаем по неделям\n",
    "\n",
    "# Функция для вычисления взвешенной средней\n",
    "def weighted_average(series, weights):\n",
    "    result = 0.0\n",
    "    weights.reverse()\n",
    "    for n in range(len(weights)):\n",
    "        result += series[-n-1] * weights[n]\n",
    "    return result\n",
    "\n",
    "weighted_average(df.Data, [0.6, 0.2, 0.1, 0.07, 0.03])\n",
    "\n",
    "# Функция и график для экспоненциального сглаживания\n",
    "def exponential_smoothing(series, alpha):\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "with plt.style.context('seaborn-white'):    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for alpha in [0.3, 0.05]:\n",
    "        plt.plot(exponential_smoothing(df.Data, alpha), label=\"Alpha {}\".format(alpha))\n",
    "    plt.plot(df.Data.values, \"c\", label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Exponential Smoothing\")\n",
    "    plt.grid(True)\n",
    "\n",
    "# Функция и график для двойного экспоненциального сглаживания\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # прогнозируем\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "    return result\n",
    "\n",
    "with plt.style.context('seaborn-white'):    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for alpha in [0.9, 0.02]:\n",
    "        for beta in [0.9, 0.02]:\n",
    "            plt.plot(double_exponential_smoothing(df.Data, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "    plt.plot(df.Data.values, label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Double Exponential Smoothing\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция и график для тройного экспоненциального сглаживания Холта-Винтерса\n",
    "class HoltWinters:\n",
    "\n",
    "    \"\"\"\n",
    "    Модель Хольта-Винтерса с методом Брутлага для детектирования аномалий\n",
    "    https://fedcsis.org/proceedings/2012/pliks/118.pdf\n",
    "\n",
    "    # series - исходный временной ряд\n",
    "    # slen - длина сезона\n",
    "    # alpha, beta, gamma - коэффициенты модели Хольта-Винтерса\n",
    "    # n_preds - горизонт предсказаний\n",
    "    # scaling_factor - задаёт ширину доверительного интервала по Брутлагу (обычно принимает значения от 2 до 3)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "\n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # вычисляем сезонные средние\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n",
    "        # вычисляем начальные значения\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals   \n",
    "\n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "\n",
    "        seasonals = self.initial_seasonal_components()\n",
    "\n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # инициализируем значения компонент\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])\n",
    "\n",
    "                self.PredictedDeviation.append(0)\n",
    "\n",
    "                self.UpperBond.append(self.result[0] + \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "\n",
    "                self.LowerBond.append(self.result[0] - \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "\n",
    "                continue\n",
    "            if i >= len(self.series): # прогнозируем\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n",
    "\n",
    "                # во время прогноза с каждым шагом увеличиваем неопределенность\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n",
    "\n",
    "            else:\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])\n",
    "\n",
    "                # Отклонение рассчитывается в соответствии с алгоритмом Брутлага\n",
    "                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n",
    "                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n",
    "\n",
    "            self.UpperBond.append(self.result[-1] + \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.LowerBond.append(self.result[-1] - \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i % self.slen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Rolling window estimations</b> будем считать, что будущее значение переменной зависит от среднего её предыдущих значений -  воспользуемся скользящей средней: \n",
    "\n",
    "$\\ \\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k-1}_{n=0} y_{t-n}$\n",
    "\n",
    "Модификацией простой скользящей средней является <b>взвешенная средняя</b>, внутри которой наблюдениям придаются различные веса, в сумме дающие единицу, при этом обычно последним наблюдениям присваивается больший вес: \n",
    "\n",
    "$\\ \\hat{y}_{t} = \\displaystyle\\sum^{k}_{n=1} \\omega_n y_{t+1-n}$\n",
    "\n",
    "<b>Простое экспоненциальное сглаживание</b> взвешивание всех наблюдений, экспоненциально уменьшая веса назад во времени: \n",
    "\n",
    "$\\ \\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1}$  \n",
    "\n",
    "Если разбить ряд на две составляющих, то можно прогнозировать не только значение(уровень) но и тренд.<br>\n",
    "Тройное сглаживание ХолтВинтерса прогнозирует еще и сезонность(повторение колебаний): $\\ \\hat y_{max_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}+m⋅d_{t−T}\\\\ \\hat y_{min_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}-m⋅d_{t−T}\\\\ d_t=\\gamma∣y_t−\\hat y_t∣+(1−\\gamma)d_{t−T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on a rolling basis позволяет проводить кросс-валидацию именно на временных рядах\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "def timeseriesCVscore(x):\n",
    "    # вектор ошибок\n",
    "    errors = []\n",
    "    values = data.values\n",
    "    alpha, beta, gamma = x\n",
    "\n",
    "    # задаём число фолдов для кросс-валидации\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    # идем по фолдам, на каждом обучаем модель, строим прогноз на отложенной выборке и считаем ошибку\n",
    "    for train, test in tscv.split(values):\n",
    "\n",
    "        model = HoltWinters(\n",
    "            series=values[train], slen=24*7, alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n",
    "        model.triple_exponential_smoothing()\n",
    "\n",
    "        predictions = model.result[-len(test):]\n",
    "        actual = values[test]\n",
    "        error = mean_squared_error(predictions, actual)\n",
    "        errors.append(error)\n",
    "\n",
    "    # Возвращаем средний квадрат ошибки по вектору ошибок\n",
    "    return np.mean(np.array(errors))\n",
    "\n",
    "\n",
    "# Из-за Хольта-Винтерса для минимизации функции потерь нужен алгоритм, поддерживающий ограничения на параметры (0,1)\n",
    "# Truncated Newton conjugate gradient:\n",
    "\n",
    "data = dataset.Users[:-500]  # инициализируем значения параметров\n",
    "x = [0, 0, 0]  # инициализируем значения параметров\n",
    "\n",
    "# Минимизируем функцию потерь с ограничениями на параметры\n",
    "opt = minimize(timeseriesCVscore, x0=x, method=\"TNC\",\n",
    "               bounds=((0, 1), (0, 1), (0, 1)))\n",
    "\n",
    "# Из оптимизатора берем оптимальное значение параметров\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print(alpha_final, beta_final, gamma_final)\n",
    "\n",
    "# Передадим полученные оптимальные значения коэффициентов альфа, бета, гаммаи построим прогноз на 5 дней вперёд (128 часов)\n",
    "data = dataset.Users\n",
    "model = HoltWinters(data[:-128], slen=24*7, alpha=alpha_final,\n",
    "                    beta=beta_final, gamma=gamma_final, n_preds=128, scaling_factor=2.56)\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стационарные временные ряды не меняют статистические характеристики со временем (E, сигма, ковариации)\n",
    "# Тест Дики-Фуллера на стационарность ряда: первыми разностями убираем нестационарность в интегрированном первого порядка \n",
    "# Также убрать можно разностями, выделением тренда и сезонности, сглаживаниями, преобразованиями Бокса-Кокса или log\n",
    "# Со стационарным рядом можно использовать модель ARIMA \n",
    "# Проще бывает выделить несколько признаков из имеющегося временного ряда и построить линейную регрессию или решаюший лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.586423Z",
     "start_time": "2021-02-15T09:14:02.494Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Линейная регрессия плохой метод, но безальтернативен при коротких временных рядах или двух или более факторах сезонности\n",
    "\n",
    "# Преобразуем строчки с датами в объект datetime\n",
    "# format показывает что читаем: '%b %Y' трехбуквенный месяц, затем год\n",
    "df['date'] = pd.to_datetime(df['date'], format='%b %Y')\n",
    "\n",
    "# Построим график проверить тип тренда (линейный или нет), тип сезонности (аддитивный или мультипликативный), его длину, выбросы\n",
    "# Видим линейный тренд и мультипликативную сезонность. Это подтверждается после логирафмирование цикла\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "df['series_g'].plot(ax=ax1)\n",
    "ax1.set_title(u'Объём пассажироперевозок')\n",
    "ax1.set_ylabel(u'Тысяч человек')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "pd.Series(np.log10(df['series_g'])).plot(ax=ax2)\n",
    "ax2.set_title(u'log10 от объёма пассажироперевозок')\n",
    "ax2.set_ylabel(u'log10 от тысяч человек')\n",
    "\n",
    "# Вывод: будем строить модель линейной регрессии для приближения логарифма от объёма перевозок.\n",
    "# log𝑦𝑖=𝛽𝑥𝑖+𝑐(𝑥𝑖)+𝜀𝑖, где  𝑦𝑖 объём перевозок,  𝑥𝑖 порядковый номер месяца,  𝑐(𝑥𝑖) сезонная составляющая,  𝜀𝑖  случайный шум\n",
    "# Создадим новый объект класса DateTimeIndex для 12 новых дат (месяцев) с помощью функции pd.date_range.\n",
    "# Создаём последовательсть месяцев. freq='MS' означает первое число каждого месяца из указанного диапазона\n",
    "new_dates = pd.date_range('1961-01-01', '1961-12-01', freq='MS')\n",
    "\n",
    "# Приводим df['date'] к типу Index, объединяем с 12 месяцами, полученными на предыдущем шаге\n",
    "new_dates = pd.Index(df['date']) | new_dates\n",
    "\n",
    "# Создаём датафрейм из одной колонки с расширенным набором дат\n",
    "df2 = pd.DataFrame({'date': new_dates})\n",
    "# Объединяем два датафрейма по колонке 'date'.\n",
    "# Склеиваем по указанной колонке (on) и правилу склейки (how)\n",
    "df = pd.merge(df, df2, on='date', how='right')\n",
    "\n",
    "# Регрессионная переменная month_num - порядковый номер пары (месяц, год). Логарифмируем таргет\n",
    "df['month_num'] = range(1, len(df) + 1)\n",
    "df['log_y'] = np.log10(df['series_g'])\n",
    "\n",
    "# Создадем 12 колонок season_1.., season_12, в которые поместим индикаторы соответствующего месяца\n",
    "# Чтобы избежать колинеарности, исключаем один из месяцев(январь) и делаем его эталоном, с которым сравниваем все остальные\n",
    "# Внутри цикла проверяем, равен ли очередной месяц текущему значению из цикла\n",
    "for x in range(1, 13):\n",
    "    df['season_' + str(x)] = df['date'].dt.month == x\n",
    "\n",
    "# xrange(2, 13) соответствует всем месяцам с февраля по декабрь\n",
    "season_columns = ['season_' + str(x) for x in range(2, 13)]\n",
    "\n",
    "# Создадим матрицу X и вектор y для обучения модели\n",
    "X = df[['month_num'] + season_columns]\n",
    "y = df['log_y']\n",
    "\n",
    "# Оставим только те строчки, у которых известны значения y (с номером < 144)\n",
    "X1 = X[X.index < 144]\n",
    "y1 = y[y.index < 144]\n",
    "\n",
    "# Настроим регрессионную модель. \"Подгонка\" через .fit\n",
    "model = LinearRegression()\n",
    "model.fit(X1, y1)\n",
    "\n",
    "pred = pd.DataFrame({\n",
    "    'pred': model.predict(X1),\n",
    "    'real': y1})\n",
    "pred.plot()\n",
    "\n",
    "# строим предсказание для всей матрицы X, включая неизвестные 12 месяцев\n",
    "pred = pd.DataFrame({\n",
    "    'pred': model.predict(X),\n",
    "    'real': y})\n",
    "pred.plot()\n",
    "\n",
    "# Экспонируем прогноз, чтобы получить реальные числа\n",
    "pred['number'] = 10**pred['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.587423Z",
     "start_time": "2021-02-15T09:14:02.495Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Логистическая регрессия позволяет исседовать взаимосвязи для отклика с двумя значениями (0,1)\n",
    "# Линейный классификатор делит признаковое пространство гиперплоскостью на 2 части, в которых прогнозируется бинарный отклик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $\\ p_+ $  отнесения примера $\\vec{x_i}$ к классу \"+\": $\\ p_+ = P\\left(y_i = 1 \\mid \\vec{x_i}, \\vec{w}\\right)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогнозировать вероятность, преобразуя линейный прогноз с помощью МНК можно с помощью функции: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$ \n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (когда знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$\\ p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\vec{w}^T\\vec{x}}}{1 + \\exp^{\\vec{w}^T\\vec{x}}} = \\frac{1}{1 + \\exp^{-\\vec{w}^T\\vec{x}}} = \\sigma(\\vec{w}^T\\vec{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача минимизации логистической функции потерь решается через рассчет отступа: $\\ M(\\vec{x_i}) = y_i\\vec{w}^T\\vec{x_i}$\n",
    "Если он неотрицателен, модель не ошибается на объекте, если же отрицателен – значит, класс для $\\vec{x_i}$ спрогнозирован неправильно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.588422Z",
     "start_time": "2021-02-15T09:14:03.342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Влияние регуляризации на качество классификации логистической регрессии с полиномиальными признаками\n",
    "from __future__ import division, print_function\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Сохраним предикторы и отклик в отдельных массивах NumPy\n",
    "X = df.ix[:, :2].values\n",
    "y = df.ix[:, 2].values\n",
    "\n",
    "# Определяем функцию для отображения разделяющей кривой классификатора\n",
    "def plot_boundary(clf, X, y, grid_step=.01, poly_featurizer=None):\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),\n",
    "                         np.arange(y_min, y_max, grid_step))\n",
    "\n",
    "\n",
    "# каждой точке в сетке [x_min, m_max]x[y_min, y_max] ставим в соответствие свой цвет\n",
    "Z = clf.predict(poly_featurizer.transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Создадим объект sklearn, добавим в матрицу полиномиальные признаки до седьмой степени\n",
    "# обучим логистическую регрессию с параметром регуляризации С=10**(-2)\n",
    "poly = PolynomialFeatures(degree=7)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "C = 1e-2\n",
    "logit = LogisticRegression(C=C, n_jobs=-1, random_state=17)\n",
    "logit.fit(X_poly, y)\n",
    "\n",
    "plot_boundary(logit, X, y, grid_step=.01, poly_featurizer=poly)\n",
    "\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='green', label='Да')\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', label='Нет')\n",
    "plt.xlabel(\"Тест 1\")\n",
    "plt.ylabel(\"Тест 2\")\n",
    "plt.title('2 теста. Логит с C=0.01')\n",
    "plt.legend()\n",
    "\n",
    "print(\"Доля правильных ответов классификатора на обучающей выборке:\",\n",
    "      round(logit.score(X_poly, y), 3))  # Модель недообучена, т.к. гиперпараметр модели - \"С\" подобран неверно\n",
    "# Ослабив регуляризацию(С=1 и т.д.), значения весов окажутся больше(по модулю). Слишком слабое С ведет к переобучению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Формула для функционала, который оптимизируется в логистической регрессии: $\\ J(X,y,w) = \\mathcal{L} + \\frac{1}{C}||w||^2$\n",
    " \n",
    " где $\\mathcal{L}$ – логистическая функция потерь, а С - обратный коэффициент регуляризации.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.589422Z",
     "start_time": "2021-02-15T09:14:03.615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Найдем оптимальное значение параметра регуляризации С через кросс-валидацию\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n",
    "\n",
    "c_values = np.logspace(-2, 3, 500)\n",
    "\n",
    "logit_searcher = LogisticRegressionCV(\n",
    "    Cs=c_values, cv=skf, verbose=1, n_jobs=-1)\n",
    "logit_searcher.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.590422Z",
     "start_time": "2021-02-15T09:14:03.617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Кросс-валидация оценивает значение функции потерь при данных параметрах модели \n",
    "# Это позволяет искать градиент, менять в соответствии с ним параметры и спускаться в сторону глобального минимума ошибки\n",
    "# Для проверки качества модели можно менять размер обучающей выборки и оценить смену качества по \"кривым обучения\"\n",
    "# Отображаем ошибку как функцию от количества примеров, используемых для обучения. Параметры модели фиксируются заранее\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(degree=2, alpha=0.01):\n",
    "    train_sizes = np.linspace(0.05, 1, 20)\n",
    "    #коэффициент регуляризации альфа можно уменьшить, но после 0.05 тренд к переобучению\n",
    "    logit_pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=degree)),\n",
    "                           ('sgd_logit', SGDClassifier(n_jobs=-1, random_state=42, alpha=alpha))])\n",
    "    N_train, val_train, val_test = learning_curve(logit_pipe,\n",
    "                                                  X, y, train_sizes=train_sizes, cv=5,\n",
    "                                                  scoring='roc_auc')\n",
    "    plot_with_err(N_train, val_train, label='training scores')\n",
    "    plot_with_err(N_train, val_test, label='validation scores')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "plot_learning_curve(degree=2, alpha=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.591422Z",
     "start_time": "2021-02-15T09:14:03.619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ошибка на обучающей выборке сама по себе ничего не говорит о качестве модели\n",
    "# Кросс-валидационная ошибка показывает, насколько хорошо модель подстраивается под данные (сохраняя способность к обобщению)\n",
    "# График результатов на тренировочной и валидационной выборке в зависимости от сложности модели:\n",
    "    # если две кривые распологаются близко, и обе ошибки велики, — это признак недообучения\n",
    "    # если две кривые далеко друг от друга, — это показатель переобучения\n",
    "# Если кривые сошлись близко, добавление новых данных не поможет, надо менять сложность модели. Если нет - можно пробовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.592422Z",
     "start_time": "2021-02-15T09:14:03.621Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Результирующая переменная в порядковой шкале\n",
    "# При невозможности распознать классы их следует отбросить (укрупняя группировку)\n",
    "# Критерии качества: #Accuracy (доля правильных ответов). Precision (точночть). Recall (полнота) ИЛИ\n",
    "# AUC area under curve - общая площадь под кривой(один классификатор и множество пороговых значений)\n",
    "# ROC reciever operator curve. FPR=FP/(FP+TP). Чем ближе точка в РОК кривой в классификаторе к TPR, тем лучше\n",
    "# Пороговое значение управляет качеством классификатора. Например, можно увеличивать охват, но снижать долю\n",
    "\n",
    "# Если несколько классов, но хочется сделать классификацию строго бинарной, то разбиваем на группы ДА и НЕТ\n",
    "#df['Desired1(3)'] = df['Desired1(3)'].replace(0, 1)\n",
    "\n",
    "x = np.array(np.arange(-10, 10, 0.5))\n",
    "y = 1. / (1 + np.exp(-x))\n",
    "plt.plot(x, y)\n",
    "plt.title(u'Логистическая функция')\n",
    "\n",
    "# Категориальные колонки не имеют естественного порядка, поэтому преобразуем их с помощью one-hot encoding\n",
    "# One Hot Encoding подразумевает создание 10 признаков, все из которых равны нулю за исключением одного\n",
    "cat_features = ['data', 'data1', 'data2']\n",
    "df = pd.get_dummies(df, columns=cat_features)\n",
    "\n",
    "# Разделяем на предикторы и отклик\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "\n",
    "# Создаем модель\n",
    "model = LogisticRegression(\n",
    "    # метод для поиска решения. Для больших - sag и saga. Варианты: newton-cg, lbfgs\n",
    "    solver='liblinear',\n",
    "    penalty='l2',  # норма для регуляризации. Варианты: l2, l1\n",
    "    C=1,  # параметр регуляризации. Чем меньше, тем сильнее регуляризация. Можно искать greedsearch\n",
    "    tol=1e-4,  # параметр для остановки поиска решения.\n",
    "    multi_class='ovr'  # Уточняем, что всего 2 класса\n",
    ")\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "# Для анализа предсказания результатов строим матрицу ошибок\n",
    "preds = model.predict(X)\n",
    "conf_mat = metrics.confusion_matrix(y, preds)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "# Смотрим результат рассчета вероятности\n",
    "pred_prob = model.predict_proba(X)\n",
    "\n",
    "# Рассчитываем РОК-кривую, указывая FPR, TPR\n",
    "preds = pred_prob[:, 1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Рисуем РОК-кривую\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.593422Z",
     "start_time": "2021-02-15T09:14:03.622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Задача распознавания наименований или порядков через деревья классификации. И чисел через регрессию\n",
    "# Помимо внутренних параметров (заданных изначально), есть еще внешние (задаваемые аналитиком)\n",
    "# Выбор модели с помощью обучающей/тестовой выборок через наименьшую среднюю квадратичную ошибку\n",
    "# Критерий качества Q - сумма модулей ошибок или сумма квадратов ошибок или процент ошибок и т.д.\n",
    "# Валидация - метод проверки выбранной модели на ее адекватность\n",
    "# Регуляризация - инструмент проверки моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.594422Z",
     "start_time": "2021-02-15T09:14:03.624Z"
    }
   },
   "outputs": [],
   "source": [
    "# CART - Classification and regression trees\n",
    "# деление матрицы прямыми\\гиперплоскостями, чтобы в ограниченных областях доминировали схожие объекты\n",
    "# Узел(node) - множество, которое расщепляется. Родительский, потомок, конечный.\n",
    "# Пороговое значение - эталон для сравнения\n",
    "# Ограничения задаются оператором. На кол-во слоев, на свойство потомков, на родителя, на правила остановки\n",
    "# Чистота - порядок разделения выборки на части, в каждой из которых \"загрязнение\" данных меньше\n",
    "# Критерий загразненности(вероятность принадлежать к классу P) измеряется Энтропией, Индексом Джини или Ошибкой Классификации\n",
    "# Улучшение критерия как показателя важности, связанной с переменной разделения, накапливается отдельно для каждой из них\n",
    "# Энтропия H1 = -СуммаP*log2P. Индекс Джини H2 = 1-СуммаP**2 = СуммаP*(1-P). Ошибка Классификации H3 = 1-maxP\n",
    "# Дельта H - вклад переменной в очищение. Считаем суммы для каждой и получаем информативность переменной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энтропия Шеннона соответствуете степени хаоса в системе. Чем выше энтропия, тем менее упорядочена система.\n",
    "\\begin{equation}\n",
    "\\ S = -\\sum_{i=1}^{N}p_i \\log_2{p_i},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В деревьях классификации прирост информации IG при разбиении выборки по признаку Q определяется как\n",
    "\n",
    "\\begin{equation}\n",
    "\\ IG(Q) = S_O - \\sum_{i=1}^{q}\\frac{N_i}{N}S_i,\n",
    "\\end{equation}\n",
    "\n",
    "где q число групп после разбиения, Ni число элементов выборки, у которых признак Q имеет i-ое значение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неопределенность Джини: максимизация числа пар объектов одного класса, оказавшихся в одном поддереве.\n",
    "\\begin{equation}\n",
    "\\ G = 1 - \\sum\\limits_k (p_k)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка классификации: \n",
    "\\begin{equation}\n",
    "\\ E = 1 - \\max\\limits_k p_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.596424Z",
     "start_time": "2021-02-15T09:14:04.758Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# В основе алгоритмов построения деревmев решений лежит принцип максимизации прироста информации\n",
    "# Метод случайного леса схож с методом kNN, строя предсказания для объектов на основе меток похожих объектов из обучения\n",
    "import pydot\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Инициализируем и обучаем модель\n",
    "model = DecisionTreeClassifier(random_state=42,                               \n",
    "                               criterion='gini', #Критерий качества (gini или entropy)\n",
    "                               max_depth=5,                               \n",
    "                               min_samples_split=5, # минимальное число элементов в узле для разбиения (может быть долей)\n",
    "                               min_samples_leaf=5, # минимальное число элементов в листе (может быть долей)\n",
    "                               # min_impurity_decrease=0, # минимальное значение дельты impurity\n",
    "                               class_weight=None  # веса для классов\n",
    "                               )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Для интерпретации получившейся модели изображаем её в виде дерева предикатов (решающих правил)\n",
    "export_graphviz(clf_tree, feature_names=['x1', 'x2'],\n",
    "                out_file='small_tree.dot', filled=True)\n",
    "\n",
    "# Расширенные настройки\n",
    "export_graphviz(model,\n",
    "                out_file='tree.dot',\n",
    "                feature_names=X.columns,  # задать названия фич\n",
    "                class_names=None,\n",
    "                label='all',  # показывать названия полей у численных значений внутри узла\n",
    "                filled=True,  # раскрашивать узлы в цвет преобладающего класса\n",
    "                impurity=True,  # показывать значение impurity для каждого узла\n",
    "                node_ids=True,  # показывать номера узлов                \n",
    "                proportion=True, # Показывать доли каждого класса в узлах (а не количество)                \n",
    "                rotate=True, # Повернуть дерево на 90 градусов (вертикальная ориентация)\n",
    "                precision=3  # Число точек после запятой для отображаемых дробей\n",
    "                )\n",
    "\n",
    "# Преобразуем файл .dot в .png\n",
    "# node - номер узла, X[1]<=1.5 правило расщепления, gini, samples-доля наблюдений попавших в узел, p-value (p0, pX)\n",
    "!dot - Tpng 'small_tree.dot' -o 'small_tree.png'\n",
    "# ИЛИ\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree.png')\n",
    "Image(\"tree.png\")\n",
    "\n",
    "# Модель позволяет оценить ценность (importance) и эффективность каждой фичи, считая для каждой из сумму дельты H\n",
    "pd.DataFrame({'feature': X.columns,\n",
    "              'importance': model.feature_importances_}).sort_values('importance',\n",
    "                                                                     ascending=False\n",
    "                                                                     )\n",
    "\n",
    "# Метод predict позволяет получить предсказания классов для входного списка элементов (подаём на вход матрицу)\n",
    "# Предсказание класса для новых элементов\n",
    "new_item = [1, 1, 1, 1]\n",
    "model.predict([new_item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При прогнозировании количественного признака критерий качества меняется на дисперсию вокруг среднего:\n",
    "\\begin{equation}\n",
    "\\ D = \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} (y_i - \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} y_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "$\\ell$ - число объектов в листе,  $\\ y_i$ – значения целевого признака. Минимизируя дисперсию, значения отклика в каждом листе будут примерно равны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.597423Z",
     "start_time": "2021-02-15T09:14:05.019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Самый простой вариант обучения дерева и просмотра результатов\n",
    "model = DecisionTreeClassifier(random_state=17)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "export_graphviz(age_tree, feature_names=['data', 'data1'],\n",
    "                out_file='tree.dot', filled=True)\n",
    "\n",
    "!dot - Tpng 'tree.dot' - o 'tree.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.598422Z",
     "start_time": "2021-02-15T09:14:05.021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Оценка качества классификатора: доля совпавших ответов в y_pred и y_test, или считаем точность и полноту\n",
    "# Если доля в обучающем выше тестового, означает переобученность модели. Нужно упрощать модель\n",
    "# Матрица ошибок  𝐶=(𝑐𝑖,𝑗) , где  𝑐𝑖,𝑗 количество элементов класса 𝑖 , которым классификатор присвоил класс 𝑗\n",
    "# Точность(precision) - доля правильно классифицированных объектов в найденных классификатором.\n",
    "# Полнота(recall) - доля этих объектов НА САМОМ ДЕЛЕ\n",
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "# Гармоническое среднее F1 = 2*точность*полнота/(точность+полнота). Считается с помощью classification_report\n",
    "print(metrics.classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.599422Z",
     "start_time": "2021-02-15T09:14:05.023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Деревья решений для задач регрессии (отклик не дискретный, а непрерывный). Методы схожы с деревом классификации\n",
    "# Предпочтительнии линейной регрессии, когда зависимость не линейная :)\n",
    "# В этом случа Дельта H = сумма квадратов ошибок\n",
    "# Prune (обрезание) - очистка от узлов, которые не нужны, через добавление третьей выборки (валидации)\n",
    "\n",
    "# Случайный лес. Ключевые параметры:\n",
    "# ntree - число деревьев(в начале макс, потом сокращать), mtry - число переменных в выборке (M**0.5)\n",
    "# sampsize - число наблюдений в подвыборке(0.632*N для декорреляции), nodesize - мин. число наблюдений в узле (10)\n",
    "# replace - подвыборка с  возвращением или без\n",
    "# Out-of-Bag - неиспользуемая при обучении часть выборки, используется в качестве предварительного теста модели(37%)\n",
    "# Out-of-Bag оценка — это усредненная оценка базовых алгоритмов на тех ~37% данных, на которых они не обучались"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее снижение точности, вызываемое переменной, определяется во время фазы вычисления out-of-bag ошибки. Чем больше уменьшается точность предсказаний из-за исключения (или перестановки) одной переменной, тем важнее эта переменная, и поэтому переменные с бо́льшим средним уменьшением точности более важны для классификации данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.600422Z",
     "start_time": "2021-02-15T09:14:05.288Z"
    }
   },
   "outputs": [],
   "source": [
    "#Оценка важности признаков\n",
    "from __future__ import division, print_function\n",
    "\n",
    "features = {\"f1\":u\"data1\", \"f2\":u\"data2\", \"f3\":u\"data3\"}\n",
    "forest = RandomForestRegressor(n_estimators=1000, max_features=10,\n",
    "                                random_state=0)\n",
    "\n",
    "forest.fit(df.drop(['data', 'y'], axis=1), df['y'])\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# График важности фич в лесу\n",
    "num_to_plot = 10\n",
    "feature_indices = [ind+1 for ind in indices[:num_to_plot]]\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(num_to_plot):\n",
    "    print(\"%d. %s %f \" % (f + 1, \n",
    "            features[\"f\"+str(feature_indices[f])], \n",
    "            importances[indices[f]]))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(u\"Важность конструктов\")\n",
    "bars = plt.bar(range(num_to_plot), \n",
    "               importances[indices[:num_to_plot]],\n",
    "       color=([str(i/float(num_to_plot+1)) \n",
    "               for i in range(num_to_plot)]),\n",
    "               align=\"center\")\n",
    "ticks = plt.xticks(range(num_to_plot), \n",
    "                   feature_indices)\n",
    "plt.xlim([-1, num_to_plot])\n",
    "plt.legend(bars, [u''.join(features[\"f\"+str(i)]) \n",
    "                  for i in feature_indices]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решающее дерево хорошо в классификации, достаточно сложно и может достигать нулевой ошибки на любой выборке. Метод случайных подпространств позволяет снизить корреляцию между деревьями и избежать переобучения.\n",
    "Построить решающее дерево $\\ b_n$  по выборке $\\ X_n$:\n",
    "* по заданному критерию мы выбираем лучший признак, делаем разбиение в дереве по нему и так до исчерпания выборки\n",
    "* дерево строится, пока в каждом листе не более $\\ n_\\text{min}$ объектов или пока не достигнем определенной высоты дерева\n",
    "* при каждом разбиении сначала выбирается m случайных признаков из n исходных,\n",
    "и оптимальное разделение выборки ищется среди них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговый классификатор: $\\ a(x) = \\frac{1}{N}\\sum_{i = 1}^N b_i(x)$ \n",
    "\n",
    "Для задачи кассификации ввыбирается решение голосованием по большинству, для регрессии — среднее.\n",
    "Рекомендуется в задачах классификации брать $\\ m = \\sqrt{n}$, а в задачах регрессии $\\ m = \\frac{n}{3}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.601421Z",
     "start_time": "2021-02-15T09:14:05.813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Случайный лес классификации\n",
    "# Плюсы:\n",
    "# высокая точность предсказания, низкая чувствительность к выбросам и масштабированию значений признаков\n",
    "# способен обрабатывать данные с большим числом классов и признаков, с пропущенными данными, редко переобучается\n",
    "# есть методы оценивания значимости отдельных признаков, приятно для кластеризации и общего понимания данных\n",
    "# Минусы:\n",
    "# результаты сложнее интерпретировать, нет формальных выводов (p-values), доступных для оценки важности переменных\n",
    "# работает хуже линейных методов, когда много разреженных признаков (тексты, Bag of words)\n",
    "# не умеет экстраполировать, требуется  много памяти для хранения модели \n",
    "# при категориальных предикторах предвзяты в пользу признаков с большим количеством уровней, якобы прирост информации \n",
    "# если есть группы коррелированных признаков, то предпочтение отдается небольшим группам перед большими\n",
    "\n",
    "model = RandomForestClassifier(random_state=42,\n",
    "                               n_estimators=30,                               \n",
    "                               criterion='gini',                                \n",
    "                               max_depth=5,                              \n",
    "                               oob_score=True,                               \n",
    "                               warm_start=False, # использовать результаты предыдущего вызова и нарастить предыдущий лес\n",
    "                               class_weight=None # веса классов для балансировки выборки для обучения\n",
    "                              )\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_pred, y_test))\n",
    "\n",
    "print('Out-of-bag score: {0}'.format(model.oob_score_))\n",
    "\n",
    "pd.DataFrame({'feature': X.columns,\n",
    "              'importance': model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "\n",
    "# В случае постояннного переобучения модели, можно использовать ExtraTreesClassifier и ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.602422Z",
     "start_time": "2021-02-15T09:14:05.814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Параметры случайного леса для регрессии:\n",
    "# n_estimators — число деревьев в \"лесу\" (по дефолту – 10)\n",
    "# criterion — функция, которая измеряет качество разбиения ветки дерева (дефолт—\"mse\", можно выбрать \"mae\")\n",
    "# max_features — число признаков, по которым ищется разбиение. Число или процент признаков(деф-\"auto\", \"sqrt\", \"log2\")\n",
    "# max_depth — максимальная глубина дерева  (по дефолту не ограничена)\n",
    "# min_samples_split — минимальное количество объектов, необходимое для разделения внутреннего узла (деф — 2)\n",
    "# min_samples_leaf — минимальное число объектов в листе. (деф — 1)\n",
    "# min_weight_fraction_leaf — минимальная взвешенная доля от общей суммы весов (всех входных объектов) (деф-равная)\n",
    "# max_leaf_nodes — максимальное количество листьев (деф нет ограничения)\n",
    "# min_impurity_split — порог для остановки наращивания дерева (по дефолту 1е-7)\n",
    "# bootstrap — применять ли бустрэп для построения дерева (по дефолту True)\n",
    "# oob_score — использовать ли out-of-bag объекты для оценки R^2 (по дефолту False)\n",
    "# n_jobs — количество ядер для построения модели и предсказаний (по дефолту 1, -1 - будут использоваться все ядра)\n",
    "# random_state — начальное значение для генерации случайных чисел \n",
    "# verbose — вывод логов по построению деревьев (по дефолту 0)\n",
    "# warm_start — использует уже натренированую модель и добавляет деревьев в ансамбль (по дефолту False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.602422Z",
     "start_time": "2021-02-15T09:14:05.816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Параметры случайного леса для классификации:\n",
    "# criterion — критерий классификации, по дефолту \"gini\" (можно выбрать \"entropy\")\n",
    "# class_weight — вес каждого класса (деф 1). Можно передать словарь с весами, либо \"balanced\", \"balanced_subsample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.603422Z",
     "start_time": "2021-02-15T09:14:05.818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Приемы улучшения классификаторов: stacking, bagging, boosting\n",
    "# Stacking(предсказание на базе предсказаний)\n",
    "# Bagging(усредненное мнение всех моделей), он же случайный лес. Чтобы избежать колинеарности, выборки собираются рандомно\n",
    "# Boosting - обучение на основе ошибок предыдущего классификатора (улучшением слабого классификатора)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дисперсия для случайного леса, с учетом корреляции, вызванной выборочным распределением между двумя любыми деревьями:\n",
    "$\\ Varf(x) = \\rho(x)\\sigma^2(x)$ <br>\n",
    "Поскольку бутстреп и отбор признаков независимы и одинаково распределены, по факту она равна 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.604422Z",
     "start_time": "2021-02-15T09:14:06.078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Бутстреп: Bagging - формирование М подвыборок X, которые состоят из N числа элементов из N, отобранных с вероятностью 1/N\n",
    "fig = sns.kdeplot(df[df['y'] == False]['data'], label = 'Yes')\n",
    "fig = sns.kdeplot(df[df['y'] == True]['data'], label = 'No')        \n",
    "fig.set(xlabel='Количество', ylabel='Плотность')    \n",
    "plt.show()\n",
    "\n",
    "def get_bootstrap_samples(data, n_samples):\n",
    "    # функция для генерации подвыборок с помощью бутстрэпа\n",
    "    indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    samples = data[indices]\n",
    "    return samples\n",
    "def stat_intervals(stat, alpha):\n",
    "    # функция для интервальной оценки\n",
    "    boundaries = np.percentile(stat, [100 * alpha / 2., 100 * (1 - alpha / 2.)])\n",
    "    return boundaries\n",
    "\n",
    "# сохранение в отдельные numpy массивы данных по бинарному Y\n",
    "loyal = df[df['y'] == False]['data'].values\n",
    "unloyal = df[df['y'] == True]['data'].values\n",
    "\n",
    "# ставим seed для воспроизводимости результатов\n",
    "np.random.seed(0)\n",
    "\n",
    "# генерируем выборки с помощью бутстрэра и сразу считаем по каждой из них среднее\n",
    "loyal_mean_scores = [np.mean(sample) \n",
    "                       for sample in get_bootstrap_samples(loyal_calls, 1000)]\n",
    "unloyal_mean_scores = [np.mean(sample) \n",
    "                       for sample in get_bootstrap_samples(unloyal_calls, 1000)]\n",
    "\n",
    "#  выводим интервальную оценку среднего\n",
    "print(\"loyal:  mean interval\",  stat_intervals(loyal_mean_scores, 0.05))\n",
    "print(\"unloyal:  mean interval\",  stat_intervals(churn_mean_scores, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бэггинг классификатор будет усреднять ответы всех алгоритмов $\\ a_i(x)$ (в случае классификации это соответствует голосованию):\n",
    "$ \\ a(x) = \\frac{1}{M}\\sum_{i = 1}^M a_i(x)$\n",
    "\n",
    "Для регрессии с базовыми алгоритмами b, истинной ф-цией ответа y(x) и распределении на объектах p(x) ошибкf каждой функции регрессии:\n",
    "$\\ \\varepsilon_i(x) = b_i(x) − y(x), i = 1, \\dots, n$\n",
    "\n",
    "Матожидание среднеквадратичной ошибки:\n",
    "$\\ E_x(b_i(x) − y(x))^{2} = E_x \\varepsilon_i^2 (x)$\n",
    "\n",
    "Средняя ошибка построенных функций регрессии имеет вид\n",
    "$\\ E_1 = \\frac{1}{n}E_x \\sum_{i=1}^n \\varepsilon_i^{2}(x)$\n",
    "\n",
    "Усреднение ответов позволило уменьшить средний квадрат ошибки в n раз: $\\ E_n = \\frac{1}{n}E_1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общая ошибка складывается из смещения, вариативности (дисперсии) и неустранимой ошибки:\n",
    "\n",
    "$\\text{Err}\\left(\\vec{x}\\right) = \\text{Bias}\\left(\\hat{f}\\right)^2 + \\text{Var}\\left(\\hat{f}\\right) + \\sigma^2 $ \n",
    "\n",
    "Бэггинг за счет взаимных ошибок снижает дисперсию обучаемого классификатора для разных наборов данных, предотвращая переобучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.605423Z",
     "start_time": "2021-02-15T09:14:06.609Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GBM - Gradient Boosting Machine. Остановка бустинга, когда очередные циклы перстают улучшать модель\n",
    "# Сумма квадратов ошибок Zi = -2*(Yi - f(Xi))\n",
    "# Метод максимального правдоподобия. Предполагаем наиболее вероятное событие. Критерий качества = P**A*(1-P)**(n-A)\n",
    "# Критерии качества Гаусса и Лапласа универсальны. Для двух классов - биномиальное, для большего - мультиноминальное\n",
    "# При временном промежутке - распределение Пуассона\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn import metrics\n",
    "\n",
    "model = GradientBoostingClassifier(random_state=42,                                   \n",
    "                                   n_estimators=500,\n",
    "                                   # загрязнение измеряем “mse”, “mae” или “friedman_mse” (mse с улучшениями)\n",
    "                                   criterion='friedman_mse',\n",
    "                                   max_depth=5, # Максимальная глубина каждого дерева \n",
    "                                   loss='deviance', # критерий качества ‘deviance’ (кросс-энтропия) или ‘exponential’\n",
    "                                   min_impurity_decrease=0.0, # минимальное уменьшение загрязнения\n",
    "                                   min_samples_leaf=5, # минимальное число наблюдений в потомке\n",
    "                                   min_samples_split=10, # минимальное число наблюдений в родителе\n",
    "                                   # Параметр, уменьшающий переобучение, являющемся весом отдельного дерева (меньше лучше)\n",
    "                                   learning_rate=0.01\n",
    "                                   )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "# Смотрим точность(ошибки 1 и 2го рода)\n",
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "# Cмотрим важность признаков\n",
    "fi = pd.DataFrame({'features': X_train.columns,\n",
    "                   'importance': model.feature_importances_})\n",
    "fi.sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "# Калибровка (интерпретация вероятности)\n",
    "model_sigmoid = CalibratedClassifierCV(model, cv=2, method='sigmoid')\n",
    "model_sigmoid.fit(X_train, y_train)\n",
    "model_sigmoid.predict_proba(X_test)[0:11, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.606423Z",
     "start_time": "2021-02-15T09:14:06.611Z"
    }
   },
   "outputs": [],
   "source": [
    "# Метод градиентного спуска - минимизация функции, делая небольшие шаги в сторону наискорейшего убывания функции "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор $\\ f = (\\frac{\\partial f}{\\partial x_1}, \\ldots \\frac{\\partial f}{\\partial x_n})^T$ частных производных функции $\\ f(x) = f(x_1, \\ldots x_n)$ задает направление наискорейшего возрастания этой функции. Уменьшать значения можно двигаясь в сторону антиградиента. \n",
    "\n",
    "Обычный метод минимизации квадратичной ошибки при поиске весов парной регрессии $\\ SE(w_0, w_1) = \\frac{1}{2}\\sum_{i=1}^\\ell(y_i - (w_0 + w_1x_{i}))^2 \\rightarrow min_{w_0,w_1}$ работает для небольших баз. Но такой градиентный спуск включает пересчет сумм по всем значениям в выборке - это затратно. \n",
    "\n",
    "Суть стохастического градиентного спуска - неформально выкинуть знак суммы из формул пересчета весов и обновлять их по одному объекту $\\ w_0^{(t+1)} = w_0^{(t)} + \\eta (y_i - w_0^{(t)} - w_1^{(t)}x_i) \\\\ w_1^{(t+1)} = w_1^{(t)} + \\eta (y_i - w_0^{(t)} - w_1^{(t)}x_i)x_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Классический GBM алгоритм Friedman-а</b>\n",
    "\n",
    "* набор данных $\\ \\left\\{ (x_i, y_i) \\right\\}_{i=1, \\ldots,n}$;\n",
    "* число итераций $\\ M$;\n",
    "* выбор функции потерь $\\  L(y, f)$ с выписанным градиентом;\n",
    "* выбор семейства функций базовых алгоритмов $\\ h(x, \\theta)$ , с процедурой их обучения;\n",
    "* дополнительные гиперпараметры $\\ h(x, \\theta)$, например, глубина дерева у деревьев решений;\n",
    "* начальное приближение $\\ f_0(x)$ -для инициализации константное значение $\\ \\gamma$. \n",
    "* оптимальный коэффициент $\\ \\rho$ находят бинарным или другим line search алгоритмом относительно исходной функции потерь\n",
    "\n",
    "1. Инициализировать GBM константным значением $\\  \\hat{f}(x) = \\hat{f}_0, \\hat{f}_0 = \\gamma, \\gamma \\in \\mathbb{R}$ <br> $\\ \\hat{f}_0 = \\underset{\\gamma}{\\arg\\min} \\ \\sum_{i = 1}^{n} L(y_i, \\gamma)$\n",
    "\n",
    "2. Для каждой итерации $\\ t = 1, \\dots, M$ повторять:\n",
    " * Посчитать псевдо-остатки $\\ r_t$ : $\\ r_{it} = -\\left[\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right]_{f(x)=\\hat{f}(x)}, \\quad \\mbox{for } i=1,\\ldots,n$\n",
    " * Построить новый базовый алгоритм $' h_t(x)$ как регрессию на псевдо-остатках: $\\ \\left\\{ (x_i, r_{it}) \\right\\}_{i=1, \\ldots,n}$ \n",
    " * Найти оптимальный коэффициент $\\ \\rho_t$  при $' h_t(x)$ относительно исходной функции потерь: $\\ \\rho_t = \\underset{\\rho}{\\arg\\min} \\ \\sum_{i = 1}^{n} L(y_i, \\hat{f}(x_i) + \\rho \\cdot h(x_i, \\theta))$\n",
    " * Сохранить $\\ \\hat{f_t}(x) = \\rho_t \\cdot h_t(x)$\n",
    " * Обновить текущее приближение $\\ \\hat{f}(x)$\n",
    "\n",
    "\n",
    "3. Скомпоновать итоговую GBM модель $\\ \\hat{f}(x)$: $\\ \\hat{f}(x) = \\sum_{i = 0}^M \\hat{f_i}(x)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.607422Z",
     "start_time": "2021-02-15T09:14:06.865Z"
    }
   },
   "outputs": [],
   "source": [
    "# применение LabelEncoding для преобразования категорий в числа переводит их в евклидово представление\n",
    "# для построения моделей kNN или линейных это не подходит, лучше использовать One-Hot Encoding\n",
    "# если данные динамичны, то категории можно векторизировать хэшированием (hashing trick)\n",
    "\n",
    "hash_space = 25 #Ограничиваем значение хэша\n",
    "hashing_example = pd.DataFrame([{i: 0.0 for i in range(hash_space)}])\n",
    "for s in ('job=student', 'marital=single', 'day_of_week=mon'):\n",
    "    print(s, '->', hash(s) % hash_space)\n",
    "    hashing_example.loc[0, hash(s) % hash_space] = 1\n",
    "hashing_example\n",
    "# Здесь хэшировались пары название + значение признака, чтобы разделить одинаковые значения разных признаков между собой\n",
    "assert hash('no') == hash('no')\n",
    "assert hash('data=no') != hash('data1=no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант кодирования категориальных признаков – кодирование средним значением отклика\n",
    "def code_mean(df, cat_feat, y):\n",
    "    \"\"\"\n",
    "    Возвращает словарь, где ключами являются уникальные категории признака cat_feature, \n",
    "    а значениями - средние по real_feature\n",
    "    \"\"\"\n",
    "    return dict(df.groupby(cat_feat)[y].mean())\n",
    "\n",
    "\n",
    "# добавим в фрейм час, день недели и выходной в качестве категориальных переменных\n",
    "df.index = df.index.to_datetime()\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"weekday\"] = df.index.weekday\n",
    "df['is_weekend'] = df.weekday.isin([5,6])*1\n",
    "df.head()\n",
    "\n",
    "# Посмотрим на средние по дням недели\n",
    "code_mean(data, 'weekday', \"y\")\n",
    "\n",
    "# Для увеличения числа признаков используют множество метрик (см.  библиотека tsfresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания переменных\n",
    "def prepareData(data, lag_start=5, lag_end=20, test_size=0.15):\n",
    "\n",
    "    data = pd.DataFrame(data.copy())\n",
    "    data.columns = [\"y\"]\n",
    "\n",
    "    # считаем индекс в датафрейме, после которого начинается тестовыый отрезок\n",
    "    test_index = int(len(data)*(1-test_size))\n",
    "\n",
    "    # добавляем лаги исходного ряда в качестве признаков\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "\n",
    "    data.index = data.index.to_datetime()\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "\n",
    "    # считаем средние только по тренировочной части, чтобы избежать лика\n",
    "    data['weekday_average'] = map(code_mean(data[:test_index], 'weekday', \"y\").get, data.weekday)\n",
    "    data[\"hour_average\"] = map(code_mean(data[:test_index], 'hour', \"y\").get, data.hour)\n",
    "\n",
    "    # выкидываем закодированные средними признаки \n",
    "    data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    "\n",
    "    data = data.dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # разбиваем весь датасет на тренировочную и тестовую выборку\n",
    "    X_train = data.loc[:test_index].drop([\"y\"], axis=1)\n",
    "    y_train = data.loc[:test_index][\"y\"]\n",
    "    X_test = data.loc[test_index:].drop([\"y\"], axis=1)\n",
    "    y_test = data.loc[test_index:][\"y\"]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Построение линейной регрессии\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepareData(dataset.Users, test_size=0.3, lag_start=12, lag_end=48)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "prediction = lr.predict(X_test)\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(prediction, \"r\", label=\"prediction\")\n",
    "plt.plot(y_test.values, label=\"actual\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Linear regression\\n Mean absolute error {} users\".format(round(mean_absolute_error(prediction, y_test))))\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.607422Z",
     "start_time": "2021-02-15T09:14:06.866Z"
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost - множество деревьев регрессии. q - структура дерева\n",
    "# Метод обратного распределения E = сумма(Yi-Vi)**2 позволяет через MSE находить ошибку и на ее основе исправлять веса\n",
    "# W - вес узла, набор правил попадения наблюдений в конечный узел.\n",
    "# T - количество конечных узлов, для избежание переобучения. Автоматом определяется гаммой\n",
    "# L - критерий качества XGBoost состоит из традиционного(Q)+регуляризация(1/2*гамма*T). Влияет на изменения критерия чистоты\n",
    "\n",
    "# Создаем модель. В выборе параметров помогают grid_search и валидация\n",
    "model = XGBClassifier(seed=42,\n",
    "                      booster=gbtree,  # выбор деревьев gbtree или линейных gblinear\n",
    "                      silent=True,  # НЕ вывод промежуточных результатов\n",
    "                      n_estimators=100,  # Предельное число деревьев\n",
    "                      learning_rate=0.02,  # скорость обучения\n",
    "                      min_child_weight=5,  # минимальные веса в узле-потомке\n",
    "                      max_leaf_nodes=6,  # Макс. значение конечных узлов в дереве\n",
    "                      max_depth=5,  # Максимальное число слоев дерева\n",
    "                      gamma=0.05,  # Запрещает расщепление, если загрязнение уменьшилось меньше чем на гамму\n",
    "                      subsample=0.7,  # Доля наблюдений, попадающих в случайную подвыборку\n",
    "                      reg_lambda=0,  # Критерий качества, сумма квадратов, mse\n",
    "                      reg_alpha=1  # Критерий качества, сумма модулей, mae\n",
    "                      )\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "xgb.plot_importance(model)\n",
    "xgb.plot_importance(model, max_num_features=30)\n",
    "\n",
    "# В выборе параметров помогают grid_search и валидация\n",
    "# Оптимально перебирать параметры последовательно по одному (но наилучшее решение можно и не найти)\n",
    "\n",
    "smart_xgboost = GridSearchCV(cv=5,  # Параметр разделения для кросс-валидации (фолды)\n",
    "                             error_score='raise',\n",
    "                             estimator=XGBClassifier(  # Задаем оценку = XGBClassifier\n",
    "                                 base_score=0.5,  # Задаем параметры, которые не собираемся оптимизировать\n",
    "                                 colsample_bylevel=1,\n",
    "                                 colsample_bytree=0.8,\n",
    "                                 gamma=0,\n",
    "                                 max_delta_step=0,\n",
    "                                 missing=None,\n",
    "                                 nthread=-1,\n",
    "                                 # Критерий кач-ва при обучении. здесь 2, больше-softmax, sofprob\n",
    "                                 objective='binary:logistic',\n",
    "                                 num_class=3,  # Дополнительно задаем число классов в задаче\n",
    "                                 reg_alpha=0,\n",
    "                                 reg_lambda=1,\n",
    "                                 scale_pos_weight=1,\n",
    "                                 seed=1234,\n",
    "                                 silent=True,\n",
    "                                 subsample=0.8\n",
    "                             ),\n",
    "                             fit_params={},\n",
    "                             iid=True,\n",
    "                             n_jobs=-1,\n",
    "                             param_grid={  # Изменяемые параметры\n",
    "                                 'min_child_weight': [1, 3, 5],\n",
    "                                 'max_depth': [3, 5, 7]\n",
    "                                 'n_estimators': [100, 300, 500, 800, 1000],\n",
    "                                 'learning_rate': [0.05, 0.1, 0.3]\n",
    "                             },\n",
    "                             pre_dispatch='2*n_jobs',\n",
    "                             refit=True,\n",
    "                             scoring='accuracy',\n",
    "                             verbose=0\n",
    "                             )\n",
    "\n",
    "smart_xgboost.fit(X_train, y_train)\n",
    "\n",
    "sorted(smart_xgboost.cv_results_.keys())\n",
    "\n",
    "smart_xgboost.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.608422Z",
     "start_time": "2021-02-15T09:14:06.868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Нейронные сети. Deep Learning\n",
    "# Активационная функция = сумма(Wi*Xi) от числа входов нейрона.\n",
    "# Логистическая функция f(x)=e**x/(1+e**x) или гиперболический тангенс\n",
    "# ReLU функция f(x) = max(0, X) проще, но чуть менее точная и сложнее в добавлении параметров\n",
    "# Подбор архитектуры НС позволяет оптимизировать число нейронов, настроив входной, скрытые, выходной слои.\n",
    "# Сети прямого распространения: в пределах слоя нейроны не связаны, передают только в след. слой, перепрыгивать нельзя\n",
    "# Обучение НС = определение значений ВЕСОВ (внутренних параметров)  каждого соединения. Остальное задается аналитиком заранее\n",
    "# Keras модули: архитектура, входные значения, условия обучения, оценка качества\n",
    "\n",
    "# Преобразование в np.array для Keras\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Поскольку больше двух классов и они не упорядочены, то разбиваем колонку \"y\" на три, с бинарными значениями\n",
    "y_train_bin = np_utils.to_categorical(y_train)\n",
    "y_test_bin = np_utils.to_categorical(y_test)\n",
    "y_train_bin[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.609421Z",
     "start_time": "2021-02-15T09:14:06.870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Метод скорейшего(градиентного) спуска SGD. Улучшение метода: Momentum, Nesterov momentum, Adam и др.\n",
    "# Argmin, правило остановки: число итераций или малое уменьшение функции\n",
    "# Начальная точка(инициализация). Начальные значение д.б. минимальным, ближе к нулю (кроме свободных слагаемых)\n",
    "# График зависимости критерия каач-ва Q от номера итерации. Малая скорость обучения(0.001 и т.д.) дает качество, но идет дольше\n",
    "# Входные значения рекомендуется стандартизовать(снижает риск насыщения) (Xi-Xmin)/(Xmax-Xmin)\n",
    "# Batch - коррекция весов после каждой эпохи. Насыщение - отсутствие коррекций. Gradient clipping - блок от больших поправок\n",
    "\n",
    "# Создаем модели. Для небольших данных можно всего пару слоев в 5-7 нейронов\n",
    "# Инициализация. Присвоение стартовых весов\n",
    "# Зерно не указано, контроля над обучением меньше\n",
    "init = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "# Усеченное нормальное распределение. Инициация весов\n",
    "init_2 = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=12345)\n",
    "init_3 = initializers.Constant(value=1e-3)  # Инициация свободных членов\n",
    "\n",
    "model = Sequential()  # Указываем на тип модели (сеть прямого распространения)\n",
    "# Первый слой, 9 нейронов, входные значения (13 предикторов)\n",
    "model.add(Dense(9, input_dim=13, activation='relu'))\n",
    "model.add(Dense(10, activation='relu', ))  # Втоой слой, 10 нейронов\n",
    "# Третий слой, ранжировка софтмаксом для стандартизации и активации. Три выхода\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(9, input_dim=13, activation='relu'))\n",
    "model2.add(Dense(10, activation='relu'))\n",
    "# Софтмакс используется для распознавания n-классов\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(9, input_dim=13, activation='relu',\n",
    "                 kernel_initializer=init_2, bias_initializer=init_3))\n",
    "model3.add(Dense(10, activation='relu',\n",
    "                 kernel_initializer=init_2, bias_initializer=init_3))\n",
    "model3.add(Dense(3, activation='softmax',\n",
    "                 kernel_initializer=init_2, bias_initializer=init_3))\n",
    "\n",
    "# Categorical crossentropy (CC) используется для определения вероятности принадлежности объекта к классу (упорядочному)\n",
    "\n",
    "# Компилируем: optimizer(rmsprop или adam), loss function(categorical_crossentropy(классификация) или mse(регрессия)). Точность\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "sgd2 = optimizers.SGD(lr=0.02, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "               optimizer=sgd2, metrics=['accuracy'])\n",
    "\n",
    "sgd3 = optimizers.SGD(lr=0.02, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "               optimizer=sgd3, metrics=['accuracy'])\n",
    "\n",
    "# Обучаем модель: 300 эпох, пропуск 10 элементов до смены весов\n",
    "model.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "model2.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "model3.fit(X_train, y_train_bin, epochs=300, batch_size=10)\n",
    "\n",
    "# Проверяем на тестовом множестве\n",
    "scores = model.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "scores2 = model2.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy2: %.2f%%\" % (scores2[1]*100))\n",
    "\n",
    "scores3 = model3.evaluate(X_test, y_test_bin)\n",
    "print(\"\\nAccuracy3: %.2f%%\" % (scores3[1]*100))\n",
    "\n",
    "\n",
    "# Рассчитываем предикторы\n",
    "predictions = model.predict(X_test)\n",
    "predictions2 = model2.predict(X_test)\n",
    "predictions3 = model3.predict(X_test)\n",
    "# round predictions\n",
    "#rounded = [round(x[0]) for x in predictions]\n",
    "# print(rounded)\n",
    "print(predictions[0:5])\n",
    "print(predictions2[0:5])\n",
    "print(predictions3[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.610422Z",
     "start_time": "2021-02-15T09:14:06.871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Нейронные сети: Прогнозирование (в надежде, что поведение ряда сохраняется). Аддитивная сезонность, логарифм не нужен\n",
    "# Уменьшение накапливаемости ошибки м.б. только с параллельным обучением нескольких сетей (и со снижением качества в целом)\n",
    "# Наиболее важны самые недавние (или самые высокоранжированные) наблюдения. Их оптимально ставить в тестовую\n",
    "# MSE, MAE или Mean absolute percentage error (MAPE) - показывают относительную ошибку в регрессии\n",
    "# MAPE = 1/n *сумма(Yi-Yпрогноз)/Yi*100%\n",
    "\n",
    "sales = pd.read_csv('monthly-car-sales-in-quebec-1960.csv',\n",
    "                    sep=';', header=0, parse_dates=[0])\n",
    "\n",
    "# Преобразуем данные\n",
    "sales_2 = pd.DataFrame()\n",
    "\n",
    "for i in range(12, 0, -1):  # Убрано накопление ошибок, поскольку нет предсказаний на основе предыдущих предсказаний\n",
    "    # Новые колонки, где значения сдвинуты с обратным временным шагом(помесячно)\n",
    "    sales_2['t-'+str(i)] = sales.iloc[:, 1].shift(i)\n",
    "\n",
    "sales_2['t'] = sales.iloc[:, 1].values  # Дублируем первоначальную колонку\n",
    "sales_4 = sales_2[12:]  # Отрезаем первые 12 строк\n",
    "\n",
    "# Задаем предикторы и отклик\n",
    "y = sales_4['t']\n",
    "X = sales_4.drop('t', axis=1)\n",
    "\n",
    "# Разделяем на обучающую и тестовую. Тестовая - последние наблюдения\n",
    "X_train = X[:91]\n",
    "y_train = y[:91]\n",
    "X_test = X[91:]\n",
    "y_test = y[91:]\n",
    "\n",
    "# Преобразование в np.array\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Создаем, компилируем и обучаем модель\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "# Линейная выходная ф-ция, чтобы сохранить линейную комбинацию\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',\n",
    "              metrics=['mean_absolute_percentage_error'])\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=None)\n",
    "\n",
    "# оценка качества модели на тестовом множестве\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nMAPE: %.2f%%\" % (scores[1]))\n",
    "\n",
    "# Вычисляем прогноз\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Вычисляем подгонку\n",
    "predictions_train = model.predict(X_train)\n",
    "\n",
    "# График с результатами numpy.arange(start, stop, step, dtype=None)\n",
    "x2 = np.arange(0, 91, 1)\n",
    "x3 = np.arange(91, 96, 1)\n",
    "\n",
    "plt.plot(x2, y_train, color='blue')\n",
    "plt.plot(x2, predictions_train, color='green')\n",
    "plt.plot(x3, y_test, color='blue')\n",
    "plt.plot(x3, predictions, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.611421Z",
     "start_time": "2021-02-15T09:14:06.873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Нейронные сети: прогнозирование логарифмов\n",
    "# Мультипликативная сезонность. Потому добавляем логарифм, который превращает произведение в сумму\n",
    "df['log_y'] = np.log10(df['series_g'])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "df['data'].plot(ax=ax1)\n",
    "ax1.set_title(u'y')\n",
    "ax1.set_ylabel(u'data')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "pd.Series(df['log_y']).plot(ax=ax2)\n",
    "ax2.set_title(u'log10 от y')\n",
    "ax2.set_ylabel(u'log10 от data')\n",
    "\n",
    "# Данные лучше разбивать на два ряда: сезонность и все остальное (сглаженный ряд скользящего среднего)\n",
    "# Преобразуем данные: 12 предикторов(помесяцам), 1 отклик\n",
    "df_new = pd.DataFrame()\n",
    "for i in range(12, 0, -1):\n",
    "    df_new['y-'+str(i)] = df.iloc[:, 2].shift(i)\n",
    "df_new['y'] = df.iloc[:, 2].values\n",
    "\n",
    "# Отрезаем первые 12 строк\n",
    "df4 = df_new[12:]\n",
    "\n",
    "# Задаем предикторы и отклик\n",
    "y = df4['y']\n",
    "X = df4.drop('y', axis=1)\n",
    "\n",
    "X_train = X[:120]\n",
    "y_train = y[:120]\n",
    "X_test = X[120:]\n",
    "y_test = y[120:]\n",
    "\n",
    "# numpy array\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Обучаем\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',\n",
    "              metrics=['mean_absolute_percentage_error'])\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=None)\n",
    "\n",
    "# оценка качества модели на тестовом\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nMAPE: %.2f%%\" % (scores[1]))\n",
    "\n",
    "# прогноз\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# подгонка\n",
    "predictions_train = model.predict(X_train)\n",
    "\n",
    "# Результаты\n",
    "x2 = np.arange(0, 120, 1)\n",
    "x3 = np.arange(120, 132, 1)\n",
    "\n",
    "# График позволяет увидеть, насколько модель уловила закономерности\n",
    "plt.plot(x2, y_train, color='blue')\n",
    "plt.plot(x2, predictions_train, color='green')\n",
    "plt.plot(x3, y_test, color='blue')\n",
    "plt.plot(x3, predictions, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.612422Z",
     "start_time": "2021-02-15T09:14:06.874Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deep Learning. Набор для тестирования MNIST(28x28 px, 256 grey).\n",
    "# Каскад Хаара(15 признаков, которые ищутся на частях картинки)\n",
    "# Сверточные сети создают аналогичные наборы признаков для решения целевых задач. Фиши пошагово усложняются\n",
    "# Stride - параметр сдвига по картинке\n",
    "# Pooling - уменьшение числа нейронов за счет повышения контрастности и сужения. Веса модифицируются\n",
    "# FC - преображение матрицы в вектор. Можно использовать уже готовую сеть и дообучить ее на своих данных\n",
    "# Dropout - борьба с переподгонкой и декорреляцией\n",
    "\n",
    "# Параметры сети\n",
    "batch_size = 128  # Объем данных для итерации\n",
    "nr_classes = 10  # Число классов (10 цифр)\n",
    "nr_iterations = 20  # Число эпох\n",
    "\n",
    "# Читаем данные (качаем MNIST)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Для сверточной сети картинку вытягиваем в столбец\n",
    "# 784 - это значения для каждого пикселя в картинке 28х28\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "# Уточняем тип данных\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Нормируем (станартизируем) входные значения (256 оттенков)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Делаем 10 бинарных столбцов (так как 10 цифр)\n",
    "Y_train = np_utils.to_categorical(y_train, nr_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nr_classes)\n",
    "\n",
    "#  Описываем сеть. Один внутренний слой\n",
    "model = Sequential()\n",
    "model.add(Dense(196, input_shape=(784,)))  # Число нейронов, число входов\n",
    "model.add(Activation('relu'))\n",
    "# Какая доля будет обучаться (половина нейронов будет спать)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))  # Выходной слой\n",
    "model.add(Activation('softmax'))  # Сумма всех чисел в итоге будет равна 1\n",
    "\n",
    "# Проверяем себя\n",
    "model.summary()\n",
    "\n",
    "# Определяем параметры обучения\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# для воспроизводимости сети. Зерно датчика уже настроено на оптимальный вариант\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Обучаем модель\n",
    "net_res_1 = model.fit(X_train, Y_train,\n",
    "                      batch_size=batch_size, epochs=nr_iterations,\n",
    "                      verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.613422Z",
     "start_time": "2021-02-15T09:14:06.876Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Deep Learning - 2. Набор для тестирования MNIST(28x28 px, 256 grey)\n",
    "# Чтобы определить оптимальное число слоев и нейронов нужно эксперементировать\n",
    "# Теорема существования - всегда можно создать аналог рабочей НС с одним слоем (но большим числом нейронов)\n",
    "# Augmentation - улучшение точности через преобразование(вращение, зеркальность и т.д.) картинок в обучающей выборке\n",
    "# Регуляризация по Тихонову - предотвратить подгонку, найти колинеарность, запретить слишком большие значения\n",
    "\n",
    "train = pd.read_csv(r\"C:\\Users\\Mr Alex\\Files\\FlightPreparens\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\Mr Alex\\Files\\FlightPreparens\\test.csv\")\n",
    "\n",
    "\n",
    "# Разделяем предикторы и отклик\n",
    "Y = train['label']  # Столбец с кодами(10 цифр)\n",
    "X = train.drop(['label'], axis=1)  # Цветовая гамма\n",
    "\n",
    "# Разделяем на обучающую выборку и выборку валидации (чтобы не обучаться на тестовой выборке)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    X.values, Y.values, test_size=0.10, random_state=42)\n",
    "\n",
    "# параметры сети, чтобы их было удобно менять. Batch Normalization\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "# размерность картинки\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# преобразование обучающей выборки\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "# преобразование выборки валидации\n",
    "x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
    "x_val = x_val.astype('float32')\n",
    "x_val /= 255\n",
    "\n",
    "# преобразование тестовой выборки\n",
    "Xtest = test.values\n",
    "Xtest = Xtest.reshape(Xtest.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# преобразование отклика в 10 бинарных перменных\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "# Обучение модели (4 слоя в два этапа)\n",
    "model = Sequential()\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n",
    "                 input_shape=input_shape))  # Первый сверточный слой\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # Второй сверточный слой\n",
    "# Pooling. Сокращаем размерность и контраст\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout. Четверь нейронов выбрасываем из обучения\n",
    "\n",
    "# Переходим на сеть прямого распространения\n",
    "model.add(Flatten())  # Преобразуем матрицу в вектор\n",
    "model.add(Dense(128, activation='relu'))  # Первый слой анализа\n",
    "model.add(Dropout(0.5))  # Dropout\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Второй слой анализа\n",
    "\n",
    "# Определяемся с обучением\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.613422Z",
     "start_time": "2021-02-15T09:14:06.877Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(x_val, y_val))\n",
    "accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test accuracy:', accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.614421Z",
     "start_time": "2021-02-15T09:14:06.879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Метод главных компонент (PCA) для снижения размерности данных и проекции их на ортогональное подпространство признаков\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Заведём красивую трёхмерную картинку\n",
    "fig = plt.figure(1, figsize=(6, 5))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "plt.cla()\n",
    "\n",
    "for name, label in [('data', 0), ('data1', 1), ('data2', 2)]:\n",
    "    ax.text3D(X[y == label, 0].mean(),\n",
    "              X[y == label, 1].mean() + 1.5,\n",
    "              X[y == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "# Поменяем порядок цветов меток, чтобы они соответствовали правильному\n",
    "y_clr = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_clr, cmap=plt.cm.spectral)\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_centered = X - X.mean(axis=0)\n",
    "pca.fit(X_centered)\n",
    "X_pca = pca.transform(X_centered)\n",
    "\n",
    "# И нарисуем получившиеся точки в нашем новом пространстве\n",
    "plt.plot(X_pca[y == 0, 0], X_pca[y == 0, 1], 'bo', label='data')\n",
    "plt.plot(X_pca[y == 1, 0], X_pca[y == 1, 1], 'go', label='data1')\n",
    "plt.plot(X_pca[y == 2, 0], X_pca[y == 2, 1], 'ro', label='data2')\n",
    "plt.legend(loc=0);\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict_proba(X_test)\n",
    "print('Accuracy: {:.5f}'.format(accuracy_score(y_test, \n",
    "                                                preds.argmax(axis=1))))\n",
    "# 2 главные компоненты в PCA-представлении данных и на тот процент исходной дисперсии в даных, который они \"объясняют\".\n",
    "for i, component in enumerate(pca.components_):\n",
    "    print(\"{} component: {}% of initial variance\".format(i + 1, \n",
    "          round(100 * pca.explained_variance_ratio_[i], 2)))\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            iris.feature_names)))\n",
    "    \n",
    "# На практике выбирают столько главных компонент, чтобы оставить 90% дисперсии исходных данных \n",
    "pca = decomposition.PCA().fit(X)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, 63)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.axvline(21, c='b')\n",
    "plt.axhline(0.9, c='r')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.615422Z",
     "start_time": "2021-02-15T09:14:06.880Z"
    }
   },
   "outputs": [],
   "source": [
    "# Факторный анализ\n",
    "# Сокращение переменных через введение новых, искусственных переменных (факторов), которые их заменяют\n",
    "# Способы поиска наилучших проекций: projection persuit, многомерное шкалирование,карты Sommer\n",
    "# Выявление структур взаимозависимости данных, матриц корреляции\n",
    "# Преодоление мультиколинеарности переменных в регрессионом анализе\n",
    "# R - матрица корелляции=k*k, где k - число столбцов исходной матрицы. Она же дисперсия вектора\n",
    "# U - (уникальности) особенности в данных, которые не удается объеснить факторами. Определяет качество модели\n",
    "\n",
    "# Смотрим коэффициенты корелляций. Мало больших значений - плохо для факторного анализа\n",
    "df.corr()\n",
    "\n",
    "# Строим R матрицу корелляций. Много выбросов, есть бимодальности. Но сильной корелляции увы нет\n",
    "scatter_matrix(df)  # Добавление \";\" позволяет показать только график, без цифр\n",
    "\n",
    "# Cтандартизируем переменные\n",
    "df_scaled = preprocessing.scale(df)\n",
    "\n",
    "# Методом поиска главных компонентов проецируем данные на двумерную плоскость и получаем ранжирование компонентов по важности\n",
    "# Уточняем число компонент и источник данных\n",
    "pca = PCA(n_components=5).fit(df_scaled)\n",
    "\n",
    "# Доля разброса в данных, объясняемая главными компонентами\n",
    "print('Влияние компонентов на общий разброс данных: ',\n",
    "      pca.explained_variance_ratio_)\n",
    "# Чистые значения главных компонент\n",
    "meanings = pca.singular_values_\n",
    "\n",
    "# Массив, в котором посчитаны значения факторов, заменяющие исходный набор данных\n",
    "pca_factor = pca.transform(df_scaled)\n",
    "\n",
    "# Запускаем факторный анализ. Сравним влияние факторов\n",
    "# Факторов задаем много, сокращаем пока не получим адекватную группировку\n",
    "fa = FactorAnalysis(n_components=2).fit(df_scaled)\n",
    "\n",
    "# Таблица коэффициентов корреляции - что именно измерили факторы. Семь параметров превращаются в два новых фактора\n",
    "# Обновление данных, где те же колонки, но переменные в них - факториалы\n",
    "pd.DataFrame(fa.components_, columns=df.columns)\n",
    "\n",
    "# Дисперсия остатков U. Видно, что хотя новые факторы очевидны, остатки огромные, то есть переменные объяснены плохо\n",
    "# Если некоторые переменные объясняются очень плохо, то они уникальны и преображать их в факторы не надо\n",
    "pd.Series(fa.noise_variance_, df.columns)\n",
    "\n",
    "# Значения факторов можно применить к конкретным объектам\n",
    "scores = pd.DataFrame(fa.transform(df_scaled), columns=['factor1', 'factor2'])\n",
    "scores['factor1'].sort_values()  # Сортируем по значению фактора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.615422Z",
     "start_time": "2021-02-15T09:14:06.882Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVD-разложение Simon Funk'a для рекомендательных систем. Позволяет работать с данными, где много пропусков\n",
    "# Двойной факторный анализ, проведенный одновременно. Матрица триплетов получается из: номер стр, номер стлб., элемент.\n",
    "# Работает с субъективными оценками, поэтому проверяет ее с помощью средних по матрице, по строке, по столбцу\n",
    "# Не стоит минимизировать MSE - потому что нужны рекомендации только с большими значениями\n",
    "\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Создаем модель\n",
    "algo = SVD(n_factors=160,\n",
    "           n_epochs=100,\n",
    "           lr_all=0.005,  # скорость обучения, шаг модификации\n",
    "           reg_all=0.1,  # гамма регулирующая\n",
    "           biased=True,\n",
    "           random_state=42\n",
    "           )\n",
    "# Обучаем модель\n",
    "cross_validate(algo,\n",
    "               data,\n",
    "               measures=['RMSE', 'MAE'],\n",
    "               cv=5,\n",
    "               verbose=True\n",
    "               )\n",
    "\n",
    "# проверяем модель на случайном пользователе\n",
    "userid = str(196)\n",
    "itemid = str(302)\n",
    "actual_rating = 4\n",
    "print(algo.predict(userid, 302, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.616422Z",
     "start_time": "2021-02-15T09:14:06.884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Калибровка классификаторов. Нужна для рабочих моментов, чтобы лучше понять машину\n",
    "# Уточнение выданных машиной значений, с учетом \"вероятности\" попасть в класс\n",
    "# Можно задать пороговые значения, которые определят класс. В промежутке между ними машина скажет \"не знаю\"\n",
    "# Калибровка - это пересчет выходных значений, чтобы про них м.б. сказать - это Вероятность\n",
    "# Изотоническая регрессия - это линия, у которой вектор не убывает\n",
    "# Логистическая кривая, метод Платта - неубывающая линия, которая приблизит Pi к P\n",
    "\n",
    "\n",
    "# Строим предсказание модели с возможностью посмотреть на вероятности принадлежать к классу\n",
    "y_pred_train2 = model.predict_proba(X_train)\n",
    "y_pred_test2 = model.predict_proba(X_test)\n",
    "\n",
    "# Завершаем построенную машину калибровкой\n",
    "model_sigmoid = CalibratedClassifierCV(model,\n",
    "                                       cv=2,\n",
    "                                       method='sigmoid'  # Или \"isotonic\"\n",
    "                                       )\n",
    "\n",
    "\n",
    "# Обучаем калибровку на выборке валидации\n",
    "model_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "# Смотрим на вероятности после калибровки\n",
    "model_sigmoid.predict_proba(X_test)\n",
    "\n",
    "# Предсказание класса для новых элементов\n",
    "new_item = [1, 1, 1, 1]\n",
    "model.predict([new_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.617422Z",
     "start_time": "2021-02-15T09:14:06.886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Вычисление статистической погрешности для случайной выборки\n",
    "\n",
    "# Расчет объема выборки\n",
    "N = 40000  # Генеральная совокупность\n",
    "P = 0.95  # Доверительный уровень в 95%\n",
    "# коэффициент доверительного уровня (p = 95%, Z=1,96)(p=99%,   Z=2,58)\n",
    "Z = 1.96\n",
    "p = 0.5  # доля респондентов с  наличием исследуемого признака,\n",
    "q = (1 - p)  # доля респондентов, у которых исследуемый признак отсутствует,\n",
    "delta = 0.05  # Задаваемая предельная ошибка выборки.\n",
    "n = (Z**2)*p*q/delta**2  # объем выборки\n",
    "\n",
    "print(\"Рекомендуемый объем выборки для данной аудитории:\", int(n), \"человек\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.617422Z",
     "start_time": "2021-02-15T09:14:06.889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Расчет ошибки выбоки для доли признака\n",
    "# Случай 1. Генеральная совокупность значительно больше выборки\n",
    "n = 384  # Объем выборки\n",
    "m = 276  # Число объектов выборки с нужными параметрами (True)\n",
    "p = m/n  # Вероятность на основе практических данных\n",
    "sigma = n/2*((p*(1-p)/n*(1-n/N)))**0.5\n",
    "print('Результат выборки один составит: ',\n",
    "      float(\"{0:.1f}\".format(p*100)), \"±\", float(\"{0:.1f}\".format(sigma)), \"%\")\n",
    "\n",
    "# Случай 2. Генеральная совокупность сопоставима с объемом выборки\n",
    "N = 2500\n",
    "delta = Z*((p*q/n)*((N-n)/(N-1)))**0.5\n",
    "print(\"Точность результатов выборки два составит: \",\n",
    "      \"±\", float(\"{0:.1f}\".format(delta*100)), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T09:14:10.618422Z",
     "start_time": "2021-02-15T09:14:06.891Z"
    }
   },
   "outputs": [],
   "source": [
    "# Рассчет доверительного интервала\n",
    "P = 0.99  # Доверительный уровень в 99%\n",
    "Z = 2.58  # коэффициент доверительного уровня\n",
    "p = 0.2  # доля респондентов с наличием исследуемого признака,\n",
    "q = (1 - p)  # доля респондентов, у которых исследуемый признак отсутствует,\n",
    "n = 1000  # Объем выборки\n",
    "\n",
    "sigma = Z*(p*q/n)**0.5  # Погрешность оценки\n",
    "\n",
    "print('Точность результатов конкретной выборки составит: ±',\n",
    "      float(\"{0:.2f}\".format(sigma*100)), \"%\")\n",
    "print('Доверительный интервал составит:', float(\"{0:.2f}\".format((p - sigma)*100)), \"% ;\",\n",
    "      float(\"{0:.2f}\".format((p + sigma)*100)), \"%\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "628px",
    "left": "19px",
    "right": "20px",
    "top": "283px",
    "width": "356px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
